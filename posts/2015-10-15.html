<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Tomcat memory debugging</title><meta name="next-head-count" content="3"/><link rel="shortcut icon" href="favicon.ico"/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><meta name="description" content="Blogging for the future"/><link rel="preload" href="/_next/static/css/97dc5fe527f5d592.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97dc5fe527f5d592.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8c0202d61bc61a66.js" defer=""></script><script src="/_next/static/chunks/framework-5ac2b14b431a77fa.js" defer=""></script><script src="/_next/static/chunks/main-008d0e41da4e3c22.js" defer=""></script><script src="/_next/static/chunks/pages/_app-577aa7b9aa74ad1d.js" defer=""></script><script src="/_next/static/chunks/996-5423f94aa1dfaf82.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-2f2d5b8e0ced3566.js" defer=""></script><script src="/_next/static/zef3Je-IQtnhB8W40Cqa4/_buildManifest.js" defer=""></script><script src="/_next/static/zef3Je-IQtnhB8W40Cqa4/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div><main><div><div style="margin-bottom:100px"><a href="/">Misc scribbles</a></div><article><div><h1>Tomcat memory debugging</h1><h4>2015-10-15</h4></div><div><p>In my previous posts, I speculated about the issues that were causing server lag
and CPU usage spiking with tomcat:
<a href="https://cmdcolin.github.io/posts/2015-09-16">https://cmdcolin.github.io/posts/2015-09-16</a></p>
<p>Unfortunately, I was completely wrong in my speculations, but we increased
tomcat memory limits so that the entire Lucene search index could fit in memory,
which was able to fix the spiky CPU problems.</p>
<p>Luckily, fixing the memory issues had very good implications for our webapp:</p>
<p>I have a cron job uses a simple curl command to grab different pages on the
website, and then it logs the time taken to a output file. I charted these
output times, before and after we increased the memory limits of tomcat, and it
turned out that the response time of the webapp was dramatically improved by
this change.</p>
<p><img src="/media/131229569383_0.png" alt=""></p>
<p>Figure 1. The webapp response time was extremely variable before the redeploy on
Oct 2nd where we increased tomcat's memory allocation, which thereafter
dramatically improved the response time.</p>
<p>Clearly, the webapp response time was being severely compromised by the memory
issues.</p>
<p>In response to all of these issues, I also added GC logging to the tomcat
configuration so that I can see if the GC is correlated with these webapp
response time. Figure 2 shows how high GC activity is correlated with longer
webapp response times, but note that this figure was made after the other memory
allocation problems were fixed, so it is still much better than the problems we
had in the past.</p>
<p><img src="/media/131229569383_1.png" alt=""></p>
<p>Figure 2. After increasing the memory, you can see webapp response time is much
better, except if the GC activity becomes very high, and then this increases the
response time.</p>
<p>Edit: Bonus screenshot, seemingly each friday we get a majoy activity burst that
triggers GC activity!</p>
<p><img src="/media/131229569383_2.png" alt=""></p>
<p>Figure 3. Crazy Java GC activity on a friday night, but the app seems to recover
from it</p>
<h2 id="conclusion"><a aria-hidden="true" tabindex="-1" href="#conclusion"><a href="#conclusion" style="margin-right: 10px">#</a></a>Conclusion</h2>
<p>Increasing the memory allocation to java and tomcat allows the entire system to
perform much better. If you can afford to get more memory to allocate to tomcat,
then it's probably a good idea.</p>
<p>Also, tracking your webapp response times will help you see if your changes are
having a good effect. I made this a script for graphing log outputs here
<a href="https://github.com/cmdcolin/loggraph">https://github.com/cmdcolin/loggraph</a></p>
<p>PS:</p>
<p>If your tomcat is running as the tomcat user, then it can be difficult to debug
the memory problems simply with the "get heap dump" from jvisualvm, because the
permissions will be wrong. To fix this, try using a privileged user to run the
jmap command:</p>
<pre><code>runuser -l tomcat -c "/usr/java/latest/bin/jmap-dump:format=b,file=/db/tomcat/tomcat.dump 25543"
</code></pre></div><div style="margin-top:200px"></div></article></div></main></div><footer style="margin-top:100px"><a href="/">Home</a> <a href="/archive">Blog archive</a> <a href="https://github.com/cmdcolin/">Github</a> <a href="https://twitter.com/cmdcolin">Twitter</a> <a href="/projects">Projects</a><a href="/photos">Photos</a> <a href="/rss.xml">RSS</a><a href="/about">About</a></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Tomcat memory debugging","date":"2015-10-15","slug":"2015-10-15","html":"\u003cp\u003eIn my previous posts, I speculated about the issues that were causing server lag\nand CPU usage spiking with tomcat:\n\u003ca href=\"https://cmdcolin.github.io/posts/2015-09-16\"\u003ehttps://cmdcolin.github.io/posts/2015-09-16\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUnfortunately, I was completely wrong in my speculations, but we increased\ntomcat memory limits so that the entire Lucene search index could fit in memory,\nwhich was able to fix the spiky CPU problems.\u003c/p\u003e\n\u003cp\u003eLuckily, fixing the memory issues had very good implications for our webapp:\u003c/p\u003e\n\u003cp\u003eI have a cron job uses a simple curl command to grab different pages on the\nwebsite, and then it logs the time taken to a output file. I charted these\noutput times, before and after we increased the memory limits of tomcat, and it\nturned out that the response time of the webapp was dramatically improved by\nthis change.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/media/131229569383_0.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 1. The webapp response time was extremely variable before the redeploy on\nOct 2nd where we increased tomcat's memory allocation, which thereafter\ndramatically improved the response time.\u003c/p\u003e\n\u003cp\u003eClearly, the webapp response time was being severely compromised by the memory\nissues.\u003c/p\u003e\n\u003cp\u003eIn response to all of these issues, I also added GC logging to the tomcat\nconfiguration so that I can see if the GC is correlated with these webapp\nresponse time. Figure 2 shows how high GC activity is correlated with longer\nwebapp response times, but note that this figure was made after the other memory\nallocation problems were fixed, so it is still much better than the problems we\nhad in the past.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/media/131229569383_1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 2. After increasing the memory, you can see webapp response time is much\nbetter, except if the GC activity becomes very high, and then this increases the\nresponse time.\u003c/p\u003e\n\u003cp\u003eEdit: Bonus screenshot, seemingly each friday we get a majoy activity burst that\ntriggers GC activity!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/media/131229569383_2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 3. Crazy Java GC activity on a friday night, but the app seems to recover\nfrom it\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#conclusion\"\u003e\u003ca href=\"#conclusion\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIncreasing the memory allocation to java and tomcat allows the entire system to\nperform much better. If you can afford to get more memory to allocate to tomcat,\nthen it's probably a good idea.\u003c/p\u003e\n\u003cp\u003eAlso, tracking your webapp response times will help you see if your changes are\nhaving a good effect. I made this a script for graphing log outputs here\n\u003ca href=\"https://github.com/cmdcolin/loggraph\"\u003ehttps://github.com/cmdcolin/loggraph\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePS:\u003c/p\u003e\n\u003cp\u003eIf your tomcat is running as the tomcat user, then it can be difficult to debug\nthe memory problems simply with the \"get heap dump\" from jvisualvm, because the\npermissions will be wrong. To fix this, try using a privileged user to run the\njmap command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erunuser -l tomcat -c \"/usr/java/latest/bin/jmap-dump:format=b,file=/db/tomcat/tomcat.dump 25543\"\n\u003c/code\u003e\u003c/pre\u003e"}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"2015-10-15"},"buildId":"zef3Je-IQtnhB8W40Cqa4","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>