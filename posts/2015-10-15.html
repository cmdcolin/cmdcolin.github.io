<!DOCTYPE html><html lang="en"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="shortcut icon" href="favicon.ico"/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><meta name="description" content="Blogging for the future"/><title>Tomcat memory debugging</title><meta name="next-head-count" content="6"/><link rel="preload" href="/_next/static/css/ddb86c7b4e1045c6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ddb86c7b4e1045c6.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e1f6961df2961232.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e1f6961df2961232.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-514908bffb652963.js" defer=""></script><script src="/_next/static/chunks/framework-91d7f78b5b4003c8.js" defer=""></script><script src="/_next/static/chunks/main-7c3bf82eed00c281.js" defer=""></script><script src="/_next/static/chunks/pages/_app-76cd57bf65f05d70.js" defer=""></script><script src="/_next/static/chunks/162-1be62df9fbed0df5.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-2c663bbcac870718.js" defer=""></script><script src="/_next/static/mmNUHf9TZ6yuVOYTKcAeB/_buildManifest.js" defer=""></script><script src="/_next/static/mmNUHf9TZ6yuVOYTKcAeB/_ssgManifest.js" defer=""></script><script src="/_next/static/mmNUHf9TZ6yuVOYTKcAeB/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="min-h-screen"><main><div class="container mx-auto px-5"><div class="text-2xl md:text-4xl font-bold py-14 border-b border-accent-2 flex flex-col lg:flex-row items-center"><a class="header" href="/">Misc scribbles</a></div><article class="mb-32"><h1 class="text-2xl md:text-2xl lg:text-2xl font-bold tracking-tighter leading-tight md:leading-none mb-12 text-center md:text-left">Tomcat memory debugging<!-- --> - <!-- -->2015-10-15</h1><div class="max-w-1xl mx-auto"><div class="markdown-styles_markdown__h_8de"><p>In my previous posts, I speculated about the issues that were causing
CPU usage spiking with
tomcat: <a href="http://searchvoidstar.tumblr.com/post/129241954103/fixing-spiky-cpu-issues-with-tomcat">http://searchvoidstar.tumblr.com/post/129241954103/fixing-spiky-cpu-issues-with-tomcat</a></p>
<p>Unfortunately, I was completely wrong in my speculations, but we
increased tomcat memory limits so that the entire Lucene search index
could fit in memory, which was able to fix the spiky CPU problems.</p>
<p>Luckily, fixing the memory issues had very good implications for our
webapp:</p>
<p>I have a cron job uses a simple curl command to grab different pages on
the website, and then it logs the time taken to a output file. I charted
these output times, before and after we increased the memory limits of
tomcat, and it turned out that the response time of the webapp was
dramatically improved by this change.</p>
<p><a href="/media/131229569383_0.png"></a></p>
<p><img src="/media/131229569383_0.png" alt=""></p>
<p>Figure 1. The webapp response time was extremely variable before the
redeploy on Oct 2nd where we increased tomcat's memory allocation, which
thereafter dramatically improved the response time.</p>
<p>Clearly, the webapp response time was being severely compromised by the
memory issues.</p>
<p>In response to all of these issues, I also added GC logging to the
tomcat configuration so that I can see if the GC is correlated with
these webapp response time. Figure 2 shows how high GC activity is
correlated with longer webapp response times, but note that this figure
was made after the other memory allocation problems were fixed, so it is
still much better than the problems we had in the past.</p>
<p><a href="/media/131229569383_1.png"></a></p>
<p><img src="/media/131229569383_1.png" alt=""></p>
<p>Figure 2. After increasing the memory, you can see webapp response time
is much better, except if the GC activity becomes very high, and then
this increases the response time.</p>
<p>Edit: Bonus screenshot, seemingly each friday we get a majoy activity
burst that triggers GC activity!</p>
<p><img src="/media/131229569383_2.png" alt=""></p>
<p>Figure 3. Crazy Java GC activity on a friday night, but the app seems to
recover from it</p>
<p>Conclusion</p>
<p>Increasing the memory allocation to java and tomcat allows the entire
system to perform much better. If you can afford to get more memory to
allocate to tomcat, then it's probably a good idea.</p>
<p>Also, tracking your webapp response times will help you see if your
changes are having a good effect. I made this a script for graphing log
outputs here <a href="https://github.com/cmdcolin/loggraph">https://github.com/cmdcolin/loggraph</a></p>
<p>PS:</p>
<p>If your tomcat is running as the tomcat user, then it can be difficult
to debug the memory problems simply with the "get heap dump" from
jvisualvm, because the permissions will be wrong. To fix this, try using
a privileged user to run the jmap command:</p>
<p>runuser -l tomcat -c "/usr/java/latest/bin/jmap
-dump:format=b,file=/db/tomcat/tomcat.dump 25543"</p>
<p>::: {#footer}
[ October 15th, 2015 1:31pm ]{#timestamp}
:::</p>
<p>export default ({ children }) => {children}</p>
</div></div></article></div></main></div><footer class="bg-accent-1 border-t border-accent-2"><div class="container mx-auto px-5"><div class="py-14 flex flex-col lg:flex-row items-center"><div class="m-4"><a class="hover:underline" href="/">Home</a></div><div class="m-4"><a class="hover:underline" href="https://github.com/cmdcolin">Github</a></div><div class="m-4"><a class="hover:underline" href="https://twitter.com/cmdcolin">Twitter</a></div><div class="m-4"><a class="hover:underline" href="/projects">Projects</a></div><div class="m-4"><a class="hover:underline" href="/photos">Photos</a></div><div class="m-4"><a class="hover:underline" href="/rss.xml">RSS</a></div></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Tomcat memory debugging","date":"2015-10-15","slug":"2015-10-15","content":"\u003cp\u003eIn my previous posts, I speculated about the issues that were causing\nCPU usage spiking with\ntomcat: \u003ca href=\"http://searchvoidstar.tumblr.com/post/129241954103/fixing-spiky-cpu-issues-with-tomcat\"\u003ehttp://searchvoidstar.tumblr.com/post/129241954103/fixing-spiky-cpu-issues-with-tomcat\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUnfortunately, I was completely wrong in my speculations, but we\nincreased tomcat memory limits so that the entire Lucene search index\ncould fit in memory, which was able to fix the spiky CPU problems.\u003c/p\u003e\n\u003cp\u003eLuckily, fixing the memory issues had very good implications for our\nwebapp:\u003c/p\u003e\n\u003cp\u003eI have a cron job uses a simple curl command to grab different pages on\nthe website, and then it logs the time taken to a output file. I charted\nthese output times, before and after we increased the memory limits of\ntomcat, and it turned out that the response time of the webapp was\ndramatically improved by this change.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/media/131229569383_0.png\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/media/131229569383_0.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 1. The webapp response time was extremely variable before the\nredeploy on Oct 2nd where we increased tomcat's memory allocation, which\nthereafter dramatically improved the response time.\u003c/p\u003e\n\u003cp\u003eClearly, the webapp response time was being severely compromised by the\nmemory issues.\u003c/p\u003e\n\u003cp\u003eIn response to all of these issues, I also added GC logging to the\ntomcat configuration so that I can see if the GC is correlated with\nthese webapp response time. Figure 2 shows how high GC activity is\ncorrelated with longer webapp response times, but note that this figure\nwas made after the other memory allocation problems were fixed, so it is\nstill much better than the problems we had in the past.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/media/131229569383_1.png\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/media/131229569383_1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 2. After increasing the memory, you can see webapp response time\nis much better, except if the GC activity becomes very high, and then\nthis increases the response time.\u003c/p\u003e\n\u003cp\u003eEdit: Bonus screenshot, seemingly each friday we get a majoy activity\nburst that triggers GC activity!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/media/131229569383_2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 3. Crazy Java GC activity on a friday night, but the app seems to\nrecover from it\u003c/p\u003e\n\u003cp\u003eConclusion\u003c/p\u003e\n\u003cp\u003eIncreasing the memory allocation to java and tomcat allows the entire\nsystem to perform much better. If you can afford to get more memory to\nallocate to tomcat, then it's probably a good idea.\u003c/p\u003e\n\u003cp\u003eAlso, tracking your webapp response times will help you see if your\nchanges are having a good effect. I made this a script for graphing log\noutputs here \u003ca href=\"https://github.com/cmdcolin/loggraph\"\u003ehttps://github.com/cmdcolin/loggraph\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePS:\u003c/p\u003e\n\u003cp\u003eIf your tomcat is running as the tomcat user, then it can be difficult\nto debug the memory problems simply with the \"get heap dump\" from\njvisualvm, because the permissions will be wrong. To fix this, try using\na privileged user to run the jmap command:\u003c/p\u003e\n\u003cp\u003erunuser -l tomcat -c \"/usr/java/latest/bin/jmap\n-dump:format=b,file=/db/tomcat/tomcat.dump 25543\"\u003c/p\u003e\n\u003cp\u003e::: {#footer}\n[ October 15th, 2015 1:31pm ]{#timestamp}\n:::\u003c/p\u003e\n\u003cp\u003eexport default ({ children }) =\u003e {children}\u003c/p\u003e\n"}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"2015-10-15"},"buildId":"mmNUHf9TZ6yuVOYTKcAeB","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>