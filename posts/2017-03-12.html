<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>How I learned to hate ORM (especially for data import scripts)</title><meta name="next-head-count" content="3"/><link rel="shortcut icon" href="favicon.ico"/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><meta name="description" content="Blogging for the future"/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js" integrity="sha512-hpZ5pDCF2bRCweL5WoA0/N1elet1KYL5mx3LP555Eg/0ZguaHawxNvEjF6O3rufAChs16HVNhEc6blF/rZoowQ==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="preload" href="/_next/static/css/9d066bf523979bbf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/9d066bf523979bbf.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8b306f51a35d80a9.js" defer=""></script><script src="/_next/static/chunks/framework-c0d8f0fd2eea5ac1.js" defer=""></script><script src="/_next/static/chunks/main-4ce9603599af9a95.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d9139de5c3de3b6b.js" defer=""></script><script src="/_next/static/chunks/996-3d2a4318c0ac6f1a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-a48b13f8f13172f9.js" defer=""></script><script src="/_next/static/PZbT0rgyGNNpxhtrv3tQn/_buildManifest.js" defer=""></script><script src="/_next/static/PZbT0rgyGNNpxhtrv3tQn/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div><main><div><div style="margin-bottom:100px"><a href="/">Misc scribbles</a></div><article><div><h1>How I learned to hate ORM (especially for data import scripts)</h1><h4>2017-03-12</h4></div><p>When I was tasked with making a new application for our websites, I was
given several CSV files with some expectation that these files could
basically be just loaded into a database and jumped into production really
quickly. If you are using R and Shiny to make a data visualization dashboard,
especially if it is read only, this can actually be a reality for you: load
those CSVs and just pretend you&#x27;re a full featured database. I had to actually
create some read write functionality though. This was sort of experimental for
me and I&#x27;m not that well versed in databases, but I wanted to share my
experience</p>
<p>When I started, I chose grails/groovy/hibernate/GORM as a platform to
use. This quickly turned into pain when I tried to make a data importer
using grails also.</p>
<p>Each CSV row from the source file would have to be turned into many
different rows in the database because it represented multiple
relationships, example:</p>
<p><img src="/media/158300473458_0.png" alt=""/></p>
<p>Initially I made my data importer in grails, and was hardcoding column
names knowing full well this was really inflexible. At the same time I
was also trying to &quot;iterate&quot; on my database schema, and I&#x27;d want to
re-import my data to test it out, but it was really really slow. I tried
many different approaches to try to speed this up such as cleanUpGorm,
StatelessSessions, and other tricks, but it would take 10-20 minutes for
imports on a 100KB input file.</p>
<p>What I basically realised is that for bulk data import</p>
<p><strong>1) Using the ORM is really painful for bulk import.</strong></p>
<p><strong>2) If you can pre-process your data so that it is already in the
format the database expects, then you can use the CSV COPY command which
is very fast</strong></p>
<p><strong>3) If you can then abandon the ORM mentality and even ignore it as
a convenience factor, then you can embrace my database system itself</strong></p>
<p>Overall, after all this work, it just seemed like ORM treats the
database as a danger and something to be heavily abstracted over, but I
actually found joy in learning how to treat my database as a first class
citizen. Soon I started gaining appreciation of</p>
<ul>
<li>using plain SQL queries</li>
<li>learning about full text search in postgres with ts_query</li>
<li>learning about triggers to make a &quot;last updated&quot; field get updated
automatically</li>
</ul>
<p>I am pretty happy this way, and although I miss some things like
criteria queries which are very powerful, I am happy that I can interact
with my database as a friend</p>
<p>At the very least, due to the fact that I now pre-process the data
before database loading, I can now import large amounts of data super
fast with the CSV COPY command</p><div style="margin-top:200px"></div></article></div></main></div><footer style="margin-top:100px"><a href="/">Home</a> <a href="/archive">Blog archive</a> <a href="https://github.com/cmdcolin/">Github</a> <a href="https://twitter.com/cmdcolin">Twitter</a> <a href="/projects">Projects</a> <a href="/photos">Photos</a> <a href="/rss.xml">RSS</a><a href="/about">About</a> </footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"How I learned to hate ORM (especially for data import scripts)","date":"2017-03-12","slug":"2017-03-12","mdxSource":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    strong: \"strong\",\n    ul: \"ul\",\n    li: \"li\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"When I was tasked with making a new application for our websites, I was\\ngiven several CSV files with some expectation that these files could\\nbasically be just loaded into a database and jumped into production really\\nquickly. If you are using R and Shiny to make a data visualization dashboard,\\nespecially if it is read only, this can actually be a reality for you: load\\nthose CSVs and just pretend you're a full featured database. I had to actually\\ncreate some read write functionality though. This was sort of experimental for\\nme and I'm not that well versed in databases, but I wanted to share my\\nexperience\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When I started, I chose grails/groovy/hibernate/GORM as a platform to\\nuse. This quickly turned into pain when I tried to make a data importer\\nusing grails also.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Each CSV row from the source file would have to be turned into many\\ndifferent rows in the database because it represented multiple\\nrelationships, example:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/158300473458_0.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Initially I made my data importer in grails, and was hardcoding column\\nnames knowing full well this was really inflexible. At the same time I\\nwas also trying to \\\"iterate\\\" on my database schema, and I'd want to\\nre-import my data to test it out, but it was really really slow. I tried\\nmany different approaches to try to speed this up such as cleanUpGorm,\\nStatelessSessions, and other tricks, but it would take 10-20 minutes for\\nimports on a 100KB input file.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"What I basically realised is that for bulk data import\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.strong, {\n        children: \"1) Using the ORM is really painful for bulk import.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.strong, {\n        children: \"2) If you can pre-process your data so that it is already in the\\nformat the database expects, then you can use the CSV COPY command which\\nis very fast\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.strong, {\n        children: \"3) If you can then abandon the ORM mentality and even ignore it as\\na convenience factor, then you can embrace my database system itself\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Overall, after all this work, it just seemed like ORM treats the\\ndatabase as a danger and something to be heavily abstracted over, but I\\nactually found joy in learning how to treat my database as a first class\\ncitizen. Soon I started gaining appreciation of\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"using plain SQL queries\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"learning about full text search in postgres with ts_query\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"learning about triggers to make a \\\"last updated\\\" field get updated\\nautomatically\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I am pretty happy this way, and although I miss some things like\\ncriteria queries which are very powerful, I am happy that I can interact\\nwith my database as a friend\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"At the very least, due to the fact that I now pre-process the data\\nbefore database loading, I can now import large amounts of data super\\nfast with the CSV COPY command\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"2017-03-12"},"buildId":"PZbT0rgyGNNpxhtrv3tQn","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>