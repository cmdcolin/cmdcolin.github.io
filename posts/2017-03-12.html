<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>How I learned to hate ORM (especially for data import scripts)</title><meta name="next-head-count" content="3"/><link rel="shortcut icon" href="favicon.ico"/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><meta name="description" content="Blogging for the future"/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js" integrity="sha512-hpZ5pDCF2bRCweL5WoA0/N1elet1KYL5mx3LP555Eg/0ZguaHawxNvEjF6O3rufAChs16HVNhEc6blF/rZoowQ==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="preload" href="/_next/static/css/50853c3f2e0364c3.css" as="style"/><link rel="stylesheet" href="/_next/static/css/50853c3f2e0364c3.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-93aa87d657ed1852.js" defer=""></script><script src="/_next/static/chunks/framework-c0d8f0fd2eea5ac1.js" defer=""></script><script src="/_next/static/chunks/main-b12cd062888056b2.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d9139de5c3de3b6b.js" defer=""></script><script src="/_next/static/chunks/996-3d2a4318c0ac6f1a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-b6f09b4db8bfc341.js" defer=""></script><script src="/_next/static/qoQYN17Vxnef1-ULoOHHJ/_buildManifest.js" defer=""></script><script src="/_next/static/qoQYN17Vxnef1-ULoOHHJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div><main><div><div style="margin-bottom:100px"><a href="/">Misc scribbles</a></div><article><div><h1>How I learned to hate ORM (especially for data import scripts)</h1><h4>2017-03-12</h4></div><div><p>When I was tasked with making a new application for our websites, I was given
several CSV files with some expectation that these files could basically be
just loaded into a database and jumped into production really quickly. If you
are using R and Shiny to make a data visualization dashboard, especially if it
is read only, this can actually be a reality for you: load those CSVs and just
pretend you're a full featured database. I had to actually create some read
write functionality though. This was sort of experimental for me and I'm not
that well versed in databases, but I wanted to share my experience</p>
<p>When I started, I chose grails/groovy/hibernate/GORM as a platform to use. This
quickly turned into pain when I tried to make a data importer using grails
also.</p>
<p>Each CSV row from the source file would have to be turned into many different
rows in the database because it represented multiple relationships, example:</p>
<p><img src="/media/158300473458_0.png" alt=""></p>
<p>Initially I made my data importer in grails, and was hardcoding column names
knowing full well this was really inflexible. At the same time I was also
trying to "iterate" on my database schema, and I'd want to re-import my data to
test it out, but it was really really slow. I tried many different approaches
to try to speed this up such as cleanUpGorm, StatelessSessions, and other
tricks, but it would take 10-20 minutes for imports on a 100KB input file.</p>
<p>What I basically realised is that for bulk data import</p>
<ol>
<li>
<p>Using the ORM is really painful for bulk import.</p>
</li>
<li>
<p>If you can pre-process your data so that it is already in the format the
database expects, then you can use the CSV COPY command which is very fast</p>
</li>
<li>
<p>If you can then abandon the ORM mentality and even ignore it as a
convenience factor, then you can embrace my database system itself</p>
</li>
</ol>
<p>Overall, after all this work, it just seemed like ORM treats the database as a
danger and something to be heavily abstracted over, but I actually found joy in
learning how to treat my database as a first class citizen. Soon I started
gaining appreciation of</p>
<ul>
<li>using plain SQL queries</li>
<li>learning about full text search in postgres with ts_query</li>
<li>learning about triggers to make a "last updated" field get updated
automatically</li>
</ul>
<p>I am pretty happy this way, and although I miss some things like criteria
queries which are very powerful, I am happy that I can interact with my
database as a friend</p>
<p>At the very least, due to the fact that I now pre-process the data before
database loading, I can now import large amounts of data super fast with the
CSV COPY command</p></div><div style="margin-top:200px"></div></article></div></main></div><footer style="margin-top:100px"><a href="/">Home</a> <a href="/archive">Blog archive</a> <a href="https://github.com/cmdcolin/">Github</a> <a href="https://twitter.com/cmdcolin">Twitter</a> <a href="/projects">Projects</a> <a href="/photos">Photos</a> <a href="/rss.xml">RSS</a><a href="/about">About</a> </footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"How I learned to hate ORM (especially for data import scripts)","date":"2017-03-12","slug":"2017-03-12","html":"\u003cp\u003eWhen I was tasked with making a new application for our websites, I was given\nseveral CSV files with some expectation that these files could basically be\njust loaded into a database and jumped into production really quickly. If you\nare using R and Shiny to make a data visualization dashboard, especially if it\nis read only, this can actually be a reality for you: load those CSVs and just\npretend you're a full featured database. I had to actually create some read\nwrite functionality though. This was sort of experimental for me and I'm not\nthat well versed in databases, but I wanted to share my experience\u003c/p\u003e\n\u003cp\u003eWhen I started, I chose grails/groovy/hibernate/GORM as a platform to use. This\nquickly turned into pain when I tried to make a data importer using grails\nalso.\u003c/p\u003e\n\u003cp\u003eEach CSV row from the source file would have to be turned into many different\nrows in the database because it represented multiple relationships, example:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/media/158300473458_0.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eInitially I made my data importer in grails, and was hardcoding column names\nknowing full well this was really inflexible. At the same time I was also\ntrying to \"iterate\" on my database schema, and I'd want to re-import my data to\ntest it out, but it was really really slow. I tried many different approaches\nto try to speed this up such as cleanUpGorm, StatelessSessions, and other\ntricks, but it would take 10-20 minutes for imports on a 100KB input file.\u003c/p\u003e\n\u003cp\u003eWhat I basically realised is that for bulk data import\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eUsing the ORM is really painful for bulk import.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf you can pre-process your data so that it is already in the format the\ndatabase expects, then you can use the CSV COPY command which is very fast\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf you can then abandon the ORM mentality and even ignore it as a\nconvenience factor, then you can embrace my database system itself\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eOverall, after all this work, it just seemed like ORM treats the database as a\ndanger and something to be heavily abstracted over, but I actually found joy in\nlearning how to treat my database as a first class citizen. Soon I started\ngaining appreciation of\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusing plain SQL queries\u003c/li\u003e\n\u003cli\u003elearning about full text search in postgres with ts_query\u003c/li\u003e\n\u003cli\u003elearning about triggers to make a \"last updated\" field get updated\nautomatically\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI am pretty happy this way, and although I miss some things like criteria\nqueries which are very powerful, I am happy that I can interact with my\ndatabase as a friend\u003c/p\u003e\n\u003cp\u003eAt the very least, due to the fact that I now pre-process the data before\ndatabase loading, I can now import large amounts of data super fast with the\nCSV COPY command\u003c/p\u003e"}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"2017-03-12"},"buildId":"qoQYN17Vxnef1-ULoOHHJ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>