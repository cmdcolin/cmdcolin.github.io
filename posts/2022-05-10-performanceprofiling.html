<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/53ac0649e8acd1de.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-5c625475d344240d.js"/><script src="/_next/static/chunks/4bd1b696-7bed151428114f76.js" async=""></script><script src="/_next/static/chunks/517-b7d557e68f3808c7.js" async=""></script><script src="/_next/static/chunks/main-app-bba03a714545c59d.js" async=""></script><script src="/_next/static/chunks/173-c4204d8a61969f1d.js" async=""></script><script src="/_next/static/chunks/app/page-587629221be7b0d2.js" async=""></script><script src="/_next/static/chunks/app/posts/%5Bid%5D/page-cc4771b5e3a551f2.js" async=""></script><title>Notes on performance profiling JS applications</title><meta name="description" content="A blog"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div class="mb-8"><a href="/">Misc scribbles</a></div><div class="lg:w-1/2 m-auto"><div><h1>Notes on performance profiling JS applications</h1><h4>2022-05-10</h4></div><div><p>Keeping your program fast is important for</p>
<ul>
<li>user satisfaction in everyday apps</li>
<li>making certain things tractable</li>
</ul>
<p>In our application, we visualize some large-ish datasets using the browser and
javascript</p>
<h2 id="the-chrome-profiler"><a aria-hidden="true" tabindex="-1" href="#the-chrome-profiler"><a href="#the-chrome-profiler" style="margin-right: 10px">#</a></a>The Chrome profiler</h2>
<p>I use the Chrome DevTools "Performance" profiler, which is a
statistical/sampling profiler
<a href="https://en.wikipedia.org/wiki/Profiling_(computer_programming)#Statistical_profilers">https://en.wikipedia.org/wiki/Profiling_(computer_programming)#Statistical_profilers</a></p>
<p>This means it samples at some rate and see's where in the callstack the program
is executing.</p>
<ul>
<li>If you see large rectangles in the profiler, you may have a long running
function</li>
<li>If you see many small rectangles, your small function may be called many times</li>
</ul>
<p>Note: sometimes your function may be so fast, it is rarely or never encountered
by the sampling. It is a good thing (TM) to be this fast, but I mention it to
note that the sampling profiler does not give us a complete log of all function
calls.</p>
<h2 id="creating-a-flamegraph-from-the-chrome-profiler-results"><a aria-hidden="true" tabindex="-1" href="#creating-a-flamegraph-from-the-chrome-profiler-results"><a href="#creating-a-flamegraph-from-the-chrome-profiler-results" style="margin-right: 10px">#</a></a>Creating a flamegraph from the Chrome profiler results</h2>
<p>Note: sometimes, it is also useful to see the results as a "flamegraph" (see
<a href="https://www.brendangregg.com/flamegraphs.html">https://www.brendangregg.com/flamegraphs.html</a>)</p>
<p>The website <a href="https://www.speedscope.app/">https://www.speedscope.app/</a> can create "flamegraph" style figures
for Chrome profiling results</p>
<p>Update: Firefox actually has the concept of flamegraph built into their
profiler. In 2022, I switched to using Firefox as my daily driver, so enjoy this
built-in feature.</p>
<h2 id="stacking-up-many-small-optimizations"><a aria-hidden="true" tabindex="-1" href="#stacking-up-many-small-optimizations"><a href="#stacking-up-many-small-optimizations" style="margin-right: 10px">#</a></a>Stacking up many small optimizations</h2>
<p>Working with large datasets, sometimes your program will take a long time to
complete. Especially if you work with javascript in the browser, it is a
challenge to make things go fast. But you can use micro optimizations to help
improve performance over time.</p>
<p>For example, say a program takes 30 seconds to run on a certain dataset</p>
<p>If you do profiling and find a couple microoptimizations that give you a 15%,
10% and 5% performance improvement, then you program now takes 20 seconds to
run. That is still not instantaneous, but it is saving users a good 10 seconds.</p>
<h2 id="examples-of-micro-optimizations"><a aria-hidden="true" tabindex="-1" href="#examples-of-micro-optimizations"><a href="#examples-of-micro-optimizations" style="margin-right: 10px">#</a></a>Examples of micro optimizations</h2>
<ul>
<li>Using <code>Map</code> instead of <code>Object</code> can often get small performance boosts</li>
<li>Comparing value against <code>undefined</code> e.g. <code>if(val===undefined)</code> vs just
comparing against falsy e.g. <code>if(!val)</code></li>
<li>Using <code>TypedArray</code>/<code>Uint8Array</code> natively instead of <code>Buffer</code> polyfill. This
one is a kicker for me because we relied on <code>Buffer</code> polyfill, and webpack 5
stopped bundling polyfills by default which made us wake up to this</li>
<li>When converting <code>Uint8Array</code> to string, use <code>TextDecoder</code> for large strings,
and just small string concatenations of <code>String.fromCharCode</code> for small ones.
There is an inflection point for string size where one is faster</li>
<li>Use <code>for</code> loops instead of <code>Array.prototype.forEach</code>/<code>Array.prototype.map</code>. I
think similar to above, there is an inflection point (not where it gets faster
in the <code>forEach</code>/<code>map</code> case, but where you can choose to care whether the
small performance diff matters) based on number of elements in your array</li>
<li>Pre-allocate an array with <code>new Array(N)</code> instead of just <code>[]</code> if possible</li>
</ul>
<p>I have tried to keep track of more microoptimizations here, but they are pretty
specific to small examples and may not generalize across browsers or browser
versions <a href="https://gist.github.com/cmdcolin/ef57d2783e47b16aa07a03967fd870d8">https://gist.github.com/cmdcolin/ef57d2783e47b16aa07a03967fd870d8</a></p>
<h2 id="examples-of-macro-optimizations"><a aria-hidden="true" tabindex="-1" href="#examples-of-macro-optimizations"><a href="#examples-of-macro-optimizations" style="margin-right: 10px">#</a></a>Examples of macro optimizations</h2>
<p>Oftentimes, large scale re-workings of your code or "macro" optimizations are
the way to make progress.</p>
<p>A macro optimization may be revealed if you are looking at your performance
profiling result and you think: this entire section of the program could be
reworked to remove this overhead</p>
<p>In this case, it is hard to advise on because most of these will be very
specific to your particular app.</p>
<p>Just as a specific example of a macro optimization I undertook:</p>
<p>We use web workers, and had to serialize a lot of data from the web worker to
the main thread. I did a large re-working of the codebase to allow, in
particular examples, the main thread to request smaller snippets of data from
the web worker thread on-demand (the web worker is kept alive indefinitely)
instead of serializing all the web worker data and sending to the main thread.</p>
<p>This change especially pays off with large datasets, where all that
serialization/data duplication is computationally and memory expensive. Fun
fact: I remember sitting at a table at a conference in Jan 2020 talking with my
team at the Plant and Animal Genome conference, thinking that we should make
this change -- finally did it, just took 2 years. [1]</p>
<h2 id="end-to-end-optimization-testing"><a aria-hidden="true" tabindex="-1" href="#end-to-end-optimization-testing"><a href="#end-to-end-optimization-testing" style="margin-right: 10px">#</a></a>End-to-end optimization testing</h2>
<p>In order to comprehensively measure whether micro or macro optimizations are
actually improving your real world performance, it can be useful to create an
end-to-end test</p>
<p>For our app, I created a <code>puppeteer</code> based test where I loaded the website and
waited for a "DONE" condition. I created a variety of different tests which
allowed me to see e.g. some optimizations may only affect certain conditions.</p>
<p>Developing the end-to-end test suite tool awhile to develop (read: weeks to
mature, though some earlier result were available), but it let me compare the
current release vs experimental branches, and over time, the experimental
branches were merged and things got faster. [2]</p>
<h2 id="note-that-memory-usage-can-be-very-important-to-your-programs-performance"><a aria-hidden="true" tabindex="-1" href="#note-that-memory-usage-can-be-very-important-to-your-programs-performance"><a href="#note-that-memory-usage-can-be-very-important-to-your-programs-performance" style="margin-right: 10px">#</a></a>Note that memory usage can be very important to your programs performance.</h2>
<p>Excessive allocations will increase "GC pressure" (the garbage collector will
invoke more Minor and Major GC, which you will see in your performance profiling
reuslts as yellow boxes)</p>
<h2 id="conclusion"><a aria-hidden="true" tabindex="-1" href="#conclusion"><a href="#conclusion" style="margin-right: 10px">#</a></a>Conclusion</h2>
<p>It is really important to look at the profiling to see what your program
actually is spending time on. You can make hypothetical optimizations all day
and dream of rewriting in rust but you may just have a slow hot path in your JS
code that, if optimized, can get big speedups.</p>
<p>Let me know about your favorite optimizations in the comments!</p>
<h2 id="footnotes"><a aria-hidden="true" tabindex="-1" href="#footnotes"><a href="#footnotes" style="margin-right: 10px">#</a></a>Footnotes</h2>
<p>[1] Note that things like SharedArrayBuffer also offer a means to share data
between worker and main thread, but these come with many security limitations
from the browser (and was even removed for a time while these security
implications were sussed out, due to Spectre/Meltdown vulnerabilities)</p>
<p>[2] I still have not found a good way to get automated memory usage profiling
via puppeteer. You can access window.process.memory in puppeteer, but this
variable does not provide info about webworker memory usage
<a href="https://github.com/puppeteer/puppeteer/issues/8258">https://github.com/puppeteer/puppeteer/issues/8258</a></p></div><div class="mt-5"></div></div><footer class="mt-16"><a class="m-2" href="/">Home</a><a class="m-2" href="/archive">Blog archive</a><a class="m-2" href="https://github.com/cmdcolin/">Github</a><a class="m-2" href="/projects">Projects</a><a class="m-2" href="/photos">Photos</a><a class="m-2" href="/books">Books</a><a class="m-2" href="/about">About</a><a class="m-2" href="/rss.xml">RSS</a></footer><script src="/_next/static/chunks/webpack-5c625475d344240d.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[8173,[\"173\",\"static/chunks/173-c4204d8a61969f1d.js\",\"974\",\"static/chunks/app/page-587629221be7b0d2.js\"],\"\"]\n3:I[5244,[],\"\"]\n4:I[3866,[],\"\"]\n6:I[6213,[],\"OutletBoundary\"]\n8:I[6213,[],\"MetadataBoundary\"]\na:I[6213,[],\"ViewportBoundary\"]\nc:I[4835,[],\"\"]\n:HL[\"/_next/static/css/53ac0649e8acd1de.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"Q894bSyKnUIpeqoGZesK4\",\"p\":\"\",\"c\":[\"\",\"posts\",\"2022-05-10-performanceprofiling\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"id\",\"2022-05-10-performanceprofiling\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/53ac0649e8acd1de.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[\"$\",\"$L2\",null,{\"href\":\"/\",\"children\":\"Misc scribbles\"}]}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"className\":\"mt-16\",\"children\":[[\"$\",\"$L2\",null,{\"className\":\"m-2\",\"href\":\"/\",\"children\":\"Home\"}],[\"$\",\"$L2\",null,{\"className\":\"m-2\",\"href\":\"/archive\",\"children\":\"Blog archive\"}],[\"$\",\"$L2\",null,{\"className\":\"m-2\",\"href\":\"https://github.com/cmdcolin/\",\"children\":\"Github\"}],[\"$\",\"$L2\",null,{\"className\":\"m-2\",\"href\":\"/projects\",\"children\":\"Projects\"}],[\"$\",\"$L2\",null,{\"className\":\"m-2\",\"href\":\"/photos\",\"children\":\"Photos\"}],[\"$\",\"$L2\",null,{\"className\":\"m-2\",\"href\":\"/books\",\"children\":\"Books\"}],[\"$\",\"$L2\",null,{\"className\":\"m-2\",\"href\":\"/about\",\"children\":\"About\"}],[\"$\",\"$L2\",null,{\"className\":\"m-2\",\"href\":\"/rss.xml\",\"children\":\"RSS\"}]]}]]}]}]]}],{\"children\":[\"posts\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"id\",\"2022-05-10-performanceprofiling\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",null,[\"$\",\"$L6\",null,{\"children\":\"$L7\"}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"2szqLZBIsfUrjc_PpR4nS\",{\"children\":[[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],null]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"e:I[7928,[\"880\",\"static/chunks/app/posts/%5Bid%5D/page-cc4771b5e3a551f2.js\"],\"default\"]\nd:T236d,"])</script><script>self.__next_f.push([1,"\u003cp\u003eKeeping your program fast is important for\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003euser satisfaction in everyday apps\u003c/li\u003e\n\u003cli\u003emaking certain things tractable\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn our application, we visualize some large-ish datasets using the browser and\njavascript\u003c/p\u003e\n\u003ch2 id=\"the-chrome-profiler\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#the-chrome-profiler\"\u003e\u003ca href=\"#the-chrome-profiler\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eThe Chrome profiler\u003c/h2\u003e\n\u003cp\u003eI use the Chrome DevTools \"Performance\" profiler, which is a\nstatistical/sampling profiler\n\u003ca href=\"https://en.wikipedia.org/wiki/Profiling_(computer_programming)#Statistical_profilers\"\u003ehttps://en.wikipedia.org/wiki/Profiling_(computer_programming)#Statistical_profilers\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis means it samples at some rate and see's where in the callstack the program\nis executing.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf you see large rectangles in the profiler, you may have a long running\nfunction\u003c/li\u003e\n\u003cli\u003eIf you see many small rectangles, your small function may be called many times\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote: sometimes your function may be so fast, it is rarely or never encountered\nby the sampling. It is a good thing (TM) to be this fast, but I mention it to\nnote that the sampling profiler does not give us a complete log of all function\ncalls.\u003c/p\u003e\n\u003ch2 id=\"creating-a-flamegraph-from-the-chrome-profiler-results\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#creating-a-flamegraph-from-the-chrome-profiler-results\"\u003e\u003ca href=\"#creating-a-flamegraph-from-the-chrome-profiler-results\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eCreating a flamegraph from the Chrome profiler results\u003c/h2\u003e\n\u003cp\u003eNote: sometimes, it is also useful to see the results as a \"flamegraph\" (see\n\u003ca href=\"https://www.brendangregg.com/flamegraphs.html\"\u003ehttps://www.brendangregg.com/flamegraphs.html\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eThe website \u003ca href=\"https://www.speedscope.app/\"\u003ehttps://www.speedscope.app/\u003c/a\u003e can create \"flamegraph\" style figures\nfor Chrome profiling results\u003c/p\u003e\n\u003cp\u003eUpdate: Firefox actually has the concept of flamegraph built into their\nprofiler. In 2022, I switched to using Firefox as my daily driver, so enjoy this\nbuilt-in feature.\u003c/p\u003e\n\u003ch2 id=\"stacking-up-many-small-optimizations\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#stacking-up-many-small-optimizations\"\u003e\u003ca href=\"#stacking-up-many-small-optimizations\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eStacking up many small optimizations\u003c/h2\u003e\n\u003cp\u003eWorking with large datasets, sometimes your program will take a long time to\ncomplete. Especially if you work with javascript in the browser, it is a\nchallenge to make things go fast. But you can use micro optimizations to help\nimprove performance over time.\u003c/p\u003e\n\u003cp\u003eFor example, say a program takes 30 seconds to run on a certain dataset\u003c/p\u003e\n\u003cp\u003eIf you do profiling and find a couple microoptimizations that give you a 15%,\n10% and 5% performance improvement, then you program now takes 20 seconds to\nrun. That is still not instantaneous, but it is saving users a good 10 seconds.\u003c/p\u003e\n\u003ch2 id=\"examples-of-micro-optimizations\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#examples-of-micro-optimizations\"\u003e\u003ca href=\"#examples-of-micro-optimizations\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eExamples of micro optimizations\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUsing \u003ccode\u003eMap\u003c/code\u003e instead of \u003ccode\u003eObject\u003c/code\u003e can often get small performance boosts\u003c/li\u003e\n\u003cli\u003eComparing value against \u003ccode\u003eundefined\u003c/code\u003e e.g. \u003ccode\u003eif(val===undefined)\u003c/code\u003e vs just\ncomparing against falsy e.g. \u003ccode\u003eif(!val)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eUsing \u003ccode\u003eTypedArray\u003c/code\u003e/\u003ccode\u003eUint8Array\u003c/code\u003e natively instead of \u003ccode\u003eBuffer\u003c/code\u003e polyfill. This\none is a kicker for me because we relied on \u003ccode\u003eBuffer\u003c/code\u003e polyfill, and webpack 5\nstopped bundling polyfills by default which made us wake up to this\u003c/li\u003e\n\u003cli\u003eWhen converting \u003ccode\u003eUint8Array\u003c/code\u003e to string, use \u003ccode\u003eTextDecoder\u003c/code\u003e for large strings,\nand just small string concatenations of \u003ccode\u003eString.fromCharCode\u003c/code\u003e for small ones.\nThere is an inflection point for string size where one is faster\u003c/li\u003e\n\u003cli\u003eUse \u003ccode\u003efor\u003c/code\u003e loops instead of \u003ccode\u003eArray.prototype.forEach\u003c/code\u003e/\u003ccode\u003eArray.prototype.map\u003c/code\u003e. I\nthink similar to above, there is an inflection point (not where it gets faster\nin the \u003ccode\u003eforEach\u003c/code\u003e/\u003ccode\u003emap\u003c/code\u003e case, but where you can choose to care whether the\nsmall performance diff matters) based on number of elements in your array\u003c/li\u003e\n\u003cli\u003ePre-allocate an array with \u003ccode\u003enew Array(N)\u003c/code\u003e instead of just \u003ccode\u003e[]\u003c/code\u003e if possible\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI have tried to keep track of more microoptimizations here, but they are pretty\nspecific to small examples and may not generalize across browsers or browser\nversions \u003ca href=\"https://gist.github.com/cmdcolin/ef57d2783e47b16aa07a03967fd870d8\"\u003ehttps://gist.github.com/cmdcolin/ef57d2783e47b16aa07a03967fd870d8\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"examples-of-macro-optimizations\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#examples-of-macro-optimizations\"\u003e\u003ca href=\"#examples-of-macro-optimizations\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eExamples of macro optimizations\u003c/h2\u003e\n\u003cp\u003eOftentimes, large scale re-workings of your code or \"macro\" optimizations are\nthe way to make progress.\u003c/p\u003e\n\u003cp\u003eA macro optimization may be revealed if you are looking at your performance\nprofiling result and you think: this entire section of the program could be\nreworked to remove this overhead\u003c/p\u003e\n\u003cp\u003eIn this case, it is hard to advise on because most of these will be very\nspecific to your particular app.\u003c/p\u003e\n\u003cp\u003eJust as a specific example of a macro optimization I undertook:\u003c/p\u003e\n\u003cp\u003eWe use web workers, and had to serialize a lot of data from the web worker to\nthe main thread. I did a large re-working of the codebase to allow, in\nparticular examples, the main thread to request smaller snippets of data from\nthe web worker thread on-demand (the web worker is kept alive indefinitely)\ninstead of serializing all the web worker data and sending to the main thread.\u003c/p\u003e\n\u003cp\u003eThis change especially pays off with large datasets, where all that\nserialization/data duplication is computationally and memory expensive. Fun\nfact: I remember sitting at a table at a conference in Jan 2020 talking with my\nteam at the Plant and Animal Genome conference, thinking that we should make\nthis change -- finally did it, just took 2 years. [1]\u003c/p\u003e\n\u003ch2 id=\"end-to-end-optimization-testing\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#end-to-end-optimization-testing\"\u003e\u003ca href=\"#end-to-end-optimization-testing\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eEnd-to-end optimization testing\u003c/h2\u003e\n\u003cp\u003eIn order to comprehensively measure whether micro or macro optimizations are\nactually improving your real world performance, it can be useful to create an\nend-to-end test\u003c/p\u003e\n\u003cp\u003eFor our app, I created a \u003ccode\u003epuppeteer\u003c/code\u003e based test where I loaded the website and\nwaited for a \"DONE\" condition. I created a variety of different tests which\nallowed me to see e.g. some optimizations may only affect certain conditions.\u003c/p\u003e\n\u003cp\u003eDeveloping the end-to-end test suite tool awhile to develop (read: weeks to\nmature, though some earlier result were available), but it let me compare the\ncurrent release vs experimental branches, and over time, the experimental\nbranches were merged and things got faster. [2]\u003c/p\u003e\n\u003ch2 id=\"note-that-memory-usage-can-be-very-important-to-your-programs-performance\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#note-that-memory-usage-can-be-very-important-to-your-programs-performance\"\u003e\u003ca href=\"#note-that-memory-usage-can-be-very-important-to-your-programs-performance\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eNote that memory usage can be very important to your programs performance.\u003c/h2\u003e\n\u003cp\u003eExcessive allocations will increase \"GC pressure\" (the garbage collector will\ninvoke more Minor and Major GC, which you will see in your performance profiling\nreuslts as yellow boxes)\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#conclusion\"\u003e\u003ca href=\"#conclusion\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIt is really important to look at the profiling to see what your program\nactually is spending time on. You can make hypothetical optimizations all day\nand dream of rewriting in rust but you may just have a slow hot path in your JS\ncode that, if optimized, can get big speedups.\u003c/p\u003e\n\u003cp\u003eLet me know about your favorite optimizations in the comments!\u003c/p\u003e\n\u003ch2 id=\"footnotes\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#footnotes\"\u003e\u003ca href=\"#footnotes\" style=\"margin-right: 10px\"\u003e#\u003c/a\u003e\u003c/a\u003eFootnotes\u003c/h2\u003e\n\u003cp\u003e[1] Note that things like SharedArrayBuffer also offer a means to share data\nbetween worker and main thread, but these come with many security limitations\nfrom the browser (and was even removed for a time while these security\nimplications were sussed out, due to Spectre/Meltdown vulnerabilities)\u003c/p\u003e\n\u003cp\u003e[2] I still have not found a good way to get automated memory usage profiling\nvia puppeteer. You can access window.process.memory in puppeteer, but this\nvariable does not provide info about webworker memory usage\n\u003ca href=\"https://github.com/puppeteer/puppeteer/issues/8258\"\u003ehttps://github.com/puppeteer/puppeteer/issues/8258\u003c/a\u003e\u003c/p\u003e"])</script><script>self.__next_f.push([1,"5:[\"$\",\"div\",null,{\"className\":\"lg:w-1/2 m-auto\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h1\",null,{\"children\":\"Notes on performance profiling JS applications\"}],[\"$\",\"h4\",null,{\"children\":\"2022-05-10\"}]]}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$d\"}}],[\"$\",\"$Le\",null,{}]]}]\n9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Notes on performance profiling JS applications\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"A blog\"}]]\n7:null\n"])</script></body></html>