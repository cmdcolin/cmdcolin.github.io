<!DOCTYPE html><html lang="en"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>Notes on performance profiling JS applications</title><meta name="next-head-count" content="3"/><link rel="shortcut icon" href="favicon.ico"/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><meta name="description" content="Blogging for the future"/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js" integrity="sha512-hpZ5pDCF2bRCweL5WoA0/N1elet1KYL5mx3LP555Eg/0ZguaHawxNvEjF6O3rufAChs16HVNhEc6blF/rZoowQ==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="preload" href="/_next/static/css/0e13ae09fdc48bba.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0e13ae09fdc48bba.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-01f10f588aa1f712.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-896947e153b7d53e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d6e6a35eb3960502.js" defer=""></script><script src="/_next/static/chunks/996-9e3c12b77542c098.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-cbd064b69a90a980.js" defer=""></script><script src="/_next/static/UREeXyitazsNHfWnWfH36/_buildManifest.js" defer=""></script><script src="/_next/static/UREeXyitazsNHfWnWfH36/_ssgManifest.js" defer=""></script><script src="/_next/static/UREeXyitazsNHfWnWfH36/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div><main><div><div style="margin-bottom:100px"><a href="/">Misc scribbles</a></div><article><div><h1>Notes on performance profiling JS applications</h1><h4>2022-05-10</h4></div><p>Keeping your program fast is important for user satisfation in everyday apps,
and in other areas such as science, having fast code and algorithms can be the
difference for making certain problems tractable.</p>
<!-- --><p>I am not doing a lot of algorithmic analysis, but our javascript data
visualization app sometimes chugs through some large-ish datasets, so here are
some performance profiling notes</p>
<!-- --><h1>Look at the performance profiling</h1>
<!-- --><p>It is really critical to look at the profiling results when determining what to
speed up. You can make hypotheses about what is slow all day, but the profiler
will tell you what your program is spending time on.</p>
<!-- --><p>I use the Chrome DevTools &quot;Performance&quot; profiler, which is a
statistical/sampling profiler
https://en.wikipedia.org/wiki/Profiling_(computer_programming)#Statistical_profilers</p>
<!-- --><p>This means it samples at some rate and see&#x27;s where in the callstack the program
is executing.</p>
<!-- --><ul>
<!-- --><li>If you see large rectangles in the profiler, you may have a long running
function</li>
<!-- --><li>If you see many small rectangles, your small function may be called many
times</li>
<!-- --></ul>
<!-- --><p>Just because it is the rectangles are small (e.g. time taken by a function is
small) does not mean it can&#x27;t be sped up though. If you speed up a small
function, your function can become so fast the sampling profiler can miss it
and you will see it rarely in your profiling result.</p>
<!-- --><p>Note: sometimes, it is also useful to see the results as a &quot;flamegraph&quot; (see
https://www.brendangregg.com/flamegraphs.html)</p>
<!-- --><p>The website https://www.speedscope.app/ can create &quot;flamegraph&quot; style figures
for Chrome profiling results</p>
<!-- --><h2>Stacking up many small optimizations</h2>
<!-- --><p>Working with large datasets, sometimes your program will take a long time to
complete. Especially if you work with javascript in the browser, it is a
challenge to make things go fast. But you can use micro optimizations to help
improve performance over time.</p>
<!-- --><p>For example, say a program takes 30 seconds to run on a certain dataset</p>
<!-- --><p>If you do profiling and find a couple microoptimizations that give you a 15%,
10% and 5% performance improvement, then you program now takes 20 seconds to
run. That is still not instantaneous, but it is saving your a good 10 seconds.</p>
<!-- --><h2>Examples of micro optimizations</h2>
<!-- --><ul>
<!-- --><li>Using Map instead of Object, can often get small performance boosts</li>
<!-- --><li>Comparing value against <!-- --><code>undefined</code> e.g. <!-- --><code>if(val===undefined)</code> vs just
comparing against falsy e.g. <!-- --><code>if(!val)</code></li>
<!-- --><li>Using TypedArray/Uint8Array natively instead of Buffer polyfill on node.js.
This one is a kicker for me because we relied on Buffer polyfill for awhile,
and webpack 5 stopped bundling polyfills by default which made us wake up to
this</li>
<!-- --><li>When converting Uint8Array to String, use <!-- --><code>TextDecoder</code> for large strings, and
just small string concatenations of <!-- --><code>String.fromCharCode</code> for small ones.
There is an inflection point for string size where one is faster<!-- --></li>
<!-- --><li>Use <!-- --><code>for</code> loops instead of <!-- --><code>Array.prototype.forEach</code>/<!-- --><code>Array.prototype.map</code>. I
think similar to above, there is an inflection point (not where it gets
faster in the forEach/map case, but where you can choose to care whether the
small performance diff matters) based on number of elements in your array<!-- --></li>
<!-- --><li>Pre-allocate an array with &quot;new Array(N)&quot; instead of just &quot;[]&quot; if possible</li>
<!-- --></ul>
<!-- --><p>I have tried to keep track of more microoptimizations here, but they are pretty
specific to small examples and may not generalize across browsers or browser
versions https://gist.github.com/cmdcolin/ef57d2783e47b16aa07a03967fd870d8</p>
<!-- --><h2>Examples of macro optimizations</h2>
<!-- --><p>Oftentimes, large scale re-workings of your code or &quot;macro&quot; optimizations are
the way to make progress.</p>
<!-- --><p>A macro optimization may be revealed if you are looking at your performance
profiling result and you think: this entire section of the program could be
reworked to remove this overhead</p>
<!-- --><p>In this case, it is hard to advise on because most of these will be very
specific to your particular app.</p>
<!-- --><p>Just as a specific example of a macro optimization I undertook:</p>
<!-- --><p>We use web workers, and had to serialize a lot of data from the web worker to
the main thread. I did a large re-working of the codebase to allow, in
particular examples, the main thread to request smaller snippets of data from
the web worker thread on-demand (the web worker is kept alive indefinitely)
instead of serializing all the web worker data and sending to the main thread.</p>
<!-- --><p>This change especially pays off with large datasets, where all that
serialization/data duplication is computationally and memory expensive. Fun
fact: I remember sitting at a table at a conference in Jan 2020 talking with my
team at the Plant and Animal Genome conference, thinking that we should make
this change -- finally did it, just took 2 years. [1]</p>
<!-- --><h2>End-to-end optimization testing</h2>
<!-- --><p>In order to comprehensively measure whether micro or macro optimizations are
actually improving your real world performance, it can be useful to create an
end-to-end test</p>
<!-- --><p>For our app, I created a <!-- --><code>puppeteer</code> based test where I loaded the website and
waited for a &quot;DONE&quot; condition. I created a variety of different tests which
allowed me to see e.g. some optimizations may only affect certain conditions.<!-- --></p>
<!-- --><p>Developing the end-to-end test suite tool awhile to develop (read: weeks to
mature, though some earlier result were available), but it let me compare the
current release vs experimental branches, and over time, the experimental
branches were merged and things got faster.</p>
<!-- --><h2>Memory usage profiling</h2>
<!-- --><p>I have not found a great way to profile memory usage with puppeteer (you can
grab process.memory but this does not get webworker memory usage for example,
which was important for me. see
https://github.com/puppeteer/puppeteer/issues/8258 for possible xref) but using
the Chrome Profiler, I can look at memory usage. If the blue line is going up,
that means memory is being allocated! I have found that not all the things you
might expect to increase memory usage</p>
<!-- --><p>Note that memory usage can be very important to your programs performance.
Excessive allocations will increase &quot;GC pressure&quot; (the garbage collector will
invoke more Minor and Major GC, which you will see in your performance
profiling reuslts as yellow boxes)</p>
<!-- --><h2>Footnotes</h2>
<!-- --><p>[1] Note that things like SharedArrayBuffer also offer a means to share data
between worker and main thread, but these come with many security limitations
from the browser (and was even removed for a time while these security
implications were sussed out, due to Spectre/Meltdown vulnerabilities)</p><div style="margin-top:200px"><giscus-widget id="comments" repo="cmdcolin/cmdcolin.github.io" repoid="MDEwOlJlcG9zaXRvcnkyNjE0OTY3Nw==" category="General" categoryid="DIC_kwDOAY8DLc4CO-L9" mapping="pathname" term="Welcome to @giscus/react component!" reactionsenabled="1" emitmetadata="0" inputposition="top" lang="en" loading="lazy"></giscus-widget></div></article></div></main></div><footer style="margin-top:100px"><a href="/">Home</a> <!-- --><a href="/archive">Blog archive</a> <!-- --><a href="https://github.com/cmdcolin/">Github</a> <!-- --><a href="https://twitter.com/cmdcolin">Twitter</a> <!-- --><a href="/projects">Projects</a> <!-- --><a href="/photos">Photos</a> <!-- --><a href="/rss.xml">RSS</a><a href="/about">About</a> <!-- --></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Notes on performance profiling JS applications","date":"2022-05-10","slug":"2022-05-10-performanceprofiling","mdxSource":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      p: \"p\",\n      h1: \"h1\",\n      ul: \"ul\",\n      li: \"li\",\n      h2: \"h2\",\n      code: \"code\"\n    }, _provideComponents(), props.components);\n    return _jsxs(_Fragment, {\n      children: [_jsx(_components.p, {\n        children: \"Keeping your program fast is important for user satisfation in everyday apps,\\nand in other areas such as science, having fast code and algorithms can be the\\ndifference for making certain problems tractable.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"I am not doing a lot of algorithmic analysis, but our javascript data\\nvisualization app sometimes chugs through some large-ish datasets, so here are\\nsome performance profiling notes\"\n      }), \"\\n\", _jsx(_components.h1, {\n        children: \"Look at the performance profiling\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"It is really critical to look at the profiling results when determining what to\\nspeed up. You can make hypotheses about what is slow all day, but the profiler\\nwill tell you what your program is spending time on.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"I use the Chrome DevTools \\\"Performance\\\" profiler, which is a\\nstatistical/sampling profiler\\nhttps://en.wikipedia.org/wiki/Profiling_(computer_programming)#Statistical_profilers\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"This means it samples at some rate and see's where in the callstack the program\\nis executing.\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"If you see large rectangles in the profiler, you may have a long running\\nfunction\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"If you see many small rectangles, your small function may be called many\\ntimes\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Just because it is the rectangles are small (e.g. time taken by a function is\\nsmall) does not mean it can't be sped up though. If you speed up a small\\nfunction, your function can become so fast the sampling profiler can miss it\\nand you will see it rarely in your profiling result.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Note: sometimes, it is also useful to see the results as a \\\"flamegraph\\\" (see\\nhttps://www.brendangregg.com/flamegraphs.html)\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The website https://www.speedscope.app/ can create \\\"flamegraph\\\" style figures\\nfor Chrome profiling results\"\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Stacking up many small optimizations\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Working with large datasets, sometimes your program will take a long time to\\ncomplete. Especially if you work with javascript in the browser, it is a\\nchallenge to make things go fast. But you can use micro optimizations to help\\nimprove performance over time.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"For example, say a program takes 30 seconds to run on a certain dataset\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"If you do profiling and find a couple microoptimizations that give you a 15%,\\n10% and 5% performance improvement, then you program now takes 20 seconds to\\nrun. That is still not instantaneous, but it is saving your a good 10 seconds.\"\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Examples of micro optimizations\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Using Map instead of Object, can often get small performance boosts\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Comparing value against \", _jsx(_components.code, {\n            children: \"undefined\"\n          }), \" e.g. \", _jsx(_components.code, {\n            children: \"if(val===undefined)\"\n          }), \" vs just\\ncomparing against falsy e.g. \", _jsx(_components.code, {\n            children: \"if(!val)\"\n          })]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Using TypedArray/Uint8Array natively instead of Buffer polyfill on node.js.\\nThis one is a kicker for me because we relied on Buffer polyfill for awhile,\\nand webpack 5 stopped bundling polyfills by default which made us wake up to\\nthis\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"When converting Uint8Array to String, use \", _jsx(_components.code, {\n            children: \"TextDecoder\"\n          }), \" for large strings, and\\njust small string concatenations of \", _jsx(_components.code, {\n            children: \"String.fromCharCode\"\n          }), \" for small ones.\\nThere is an inflection point for string size where one is faster\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Use \", _jsx(_components.code, {\n            children: \"for\"\n          }), \" loops instead of \", _jsx(_components.code, {\n            children: \"Array.prototype.forEach\"\n          }), \"/\", _jsx(_components.code, {\n            children: \"Array.prototype.map\"\n          }), \". I\\nthink similar to above, there is an inflection point (not where it gets\\nfaster in the forEach/map case, but where you can choose to care whether the\\nsmall performance diff matters) based on number of elements in your array\"]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Pre-allocate an array with \\\"new Array(N)\\\" instead of just \\\"[]\\\" if possible\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"I have tried to keep track of more microoptimizations here, but they are pretty\\nspecific to small examples and may not generalize across browsers or browser\\nversions https://gist.github.com/cmdcolin/ef57d2783e47b16aa07a03967fd870d8\"\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Examples of macro optimizations\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Oftentimes, large scale re-workings of your code or \\\"macro\\\" optimizations are\\nthe way to make progress.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"A macro optimization may be revealed if you are looking at your performance\\nprofiling result and you think: this entire section of the program could be\\nreworked to remove this overhead\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"In this case, it is hard to advise on because most of these will be very\\nspecific to your particular app.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Just as a specific example of a macro optimization I undertook:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"We use web workers, and had to serialize a lot of data from the web worker to\\nthe main thread. I did a large re-working of the codebase to allow, in\\nparticular examples, the main thread to request smaller snippets of data from\\nthe web worker thread on-demand (the web worker is kept alive indefinitely)\\ninstead of serializing all the web worker data and sending to the main thread.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"This change especially pays off with large datasets, where all that\\nserialization/data duplication is computationally and memory expensive. Fun\\nfact: I remember sitting at a table at a conference in Jan 2020 talking with my\\nteam at the Plant and Animal Genome conference, thinking that we should make\\nthis change -- finally did it, just took 2 years. [1]\"\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"End-to-end optimization testing\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"In order to comprehensively measure whether micro or macro optimizations are\\nactually improving your real world performance, it can be useful to create an\\nend-to-end test\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"For our app, I created a \", _jsx(_components.code, {\n          children: \"puppeteer\"\n        }), \" based test where I loaded the website and\\nwaited for a \\\"DONE\\\" condition. I created a variety of different tests which\\nallowed me to see e.g. some optimizations may only affect certain conditions.\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Developing the end-to-end test suite tool awhile to develop (read: weeks to\\nmature, though some earlier result were available), but it let me compare the\\ncurrent release vs experimental branches, and over time, the experimental\\nbranches were merged and things got faster.\"\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Memory usage profiling\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"I have not found a great way to profile memory usage with puppeteer (you can\\ngrab process.memory but this does not get webworker memory usage for example,\\nwhich was important for me. see\\nhttps://github.com/puppeteer/puppeteer/issues/8258 for possible xref) but using\\nthe Chrome Profiler, I can look at memory usage. If the blue line is going up,\\nthat means memory is being allocated! I have found that not all the things you\\nmight expect to increase memory usage\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Note that memory usage can be very important to your programs performance.\\nExcessive allocations will increase \\\"GC pressure\\\" (the garbage collector will\\ninvoke more Minor and Major GC, which you will see in your performance\\nprofiling reuslts as yellow boxes)\"\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Footnotes\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"[1] Note that things like SharedArrayBuffer also offer a means to share data\\nbetween worker and main thread, but these come with many security limitations\\nfrom the browser (and was even removed for a time while these security\\nimplications were sussed out, due to Spectre/Meltdown vulnerabilities)\"\n      })]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"2022-05-10-performanceprofiling"},"buildId":"UREeXyitazsNHfWnWfH36","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>