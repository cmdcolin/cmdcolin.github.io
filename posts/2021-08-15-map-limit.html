<!DOCTYPE html><html lang="en"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>An amazing error message if you put more than 2^24 items in a JS Map object</title><meta name="next-head-count" content="3"/><link rel="shortcut icon" href="favicon.ico"/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><meta name="description" content="Blogging for the future"/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js" integrity="sha512-hpZ5pDCF2bRCweL5WoA0/N1elet1KYL5mx3LP555Eg/0ZguaHawxNvEjF6O3rufAChs16HVNhEc6blF/rZoowQ==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="preload" href="/_next/static/css/0e13ae09fdc48bba.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0e13ae09fdc48bba.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-fd1bc4a65a80e5c8.js" defer=""></script><script src="/_next/static/chunks/framework-5f4595e5518b5600.js" defer=""></script><script src="/_next/static/chunks/main-a6b7581332edf7c2.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1e17a6754614af77.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-12d1f4c4d63ac780.js" defer=""></script><script src="/_next/static/Vt7j04wIfYAWhKVSMgEoq/_buildManifest.js" defer=""></script><script src="/_next/static/Vt7j04wIfYAWhKVSMgEoq/_ssgManifest.js" defer=""></script><script src="/_next/static/Vt7j04wIfYAWhKVSMgEoq/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div><main><div><div style="margin-bottom:100px"><a href="/">Misc scribbles</a></div><article><div><h1>An amazing error message if you put more than 2^24 items in a JS Map object</h1><h4>2021-08-15</h4></div><p>One of the fun things about working with big data is that you can often hit
weird limits with a system.</p>
<p>I was personally trying to load every &#x27;common&#x27; single nucleotide polymorphism
for the human genome into memory (dbSNP), of which there are over 37 million
entries (there are many more uncommon ones) for the purposes of making a custom
search index for them [1].</p>
<p>Turns out, you may run into some hard limits. Note that these are all V8-isms
and may not apply to all browsers or engines (I was using node.js for this)</p>
<pre><code class="language-js">const myObject = new Map()
for (let i = 0; i &lt;= 50_000_000; i++) {
  myObject.set(i, i)
  if (i % 100000 == 0) {
    console.log(i)
  }
}
</code></pre>
<p>This will crash after adding approx 16.7M elements and say</p>
<pre><code>0
100000
200000
...
16400000
16500000
16600000
16700000

Uncaught RangeError: Value undefined out of range for undefined options
property undefined
</code></pre>
<p>That is a very weird error message. It says “undefined” three times! Much
better than your usual “TypeError: Can’t find property ‘lol’ of undefined”. See
https://bugs.chromium.org/p/v8/issues/detail?id=11852 for a bug filed to help
improve the error message perhaps.</p>
<p>Now, also interestingly enough, if you use an Object instead of a Map</p>
<pre><code class="language-js">const myObject = {};
for (let i = 0; i &lt;= 50_000_000; i++) {
  myObject[&#x27;myobj_’+i]=i;
  if(i%100000==0) { console.log(i) }
}
</code></pre>
<p>Then it will print...</p>
<pre><code>0
100000
200000
...
8000000
8100000
8200000
8300000
</code></pre>
<p>And it will actually just hang there…frozen…no error message though! And it is
failing at ~8.3M elements. Weird right? This is roughly half the amount of
elements as the 16.7M case</p>
<p>Turns out there is a precise hard limit for the Map case</p>
<p>For the Map: 2^24=16,777,216</p>
<p>For the Object it is around 2^23=8,388,608 HOWEVER, I can actually add more
than this, e.g. I can add 8,388,609 or 8,388,610 or even more, but the
operations start taking forever to run, e.g. 8,388,999 was taking many minutes</p>
<p>Very weird stuff! If you expected me to dig into this and explain it in deep
technical detail, well, you’d be wrong. I am lazy. However, this helpful post
on stackoverflow by a V8 js engine developer clarifies the Map case!!
https://stackoverflow.com/questions/54452896/maximum-number-of-entries-in-node-js-map</p>
<pre><code>V8 developer here. I can confirm that 2^24 is the maximum number of entries in
a Map. That’s not a bug, it’s just the implementation-defined limit.

The limit is determined by:

The FixedArray backing store of the Map has a maximum size of 1GB (independent
of the overall heap size limit) On a 64-bit system that means 1GB / 8B = 2^30 /
2^3 = 2^27 ~= 134M maximum elements per FixedArray A Map needs 3 elements per
entry (key, value, next bucket link), and has a maximum load factor of 50% (to
avoid the slowdown caused by many bucket collisions), and its capacity must be
a power of 2. 2^27 / (3 * 2) rounded down to the next power of 2 is 2^24, which
is the limit you observe.  FWIW, there are limits to everything: besides the
maximum heap size, there’s a maximum String length, a maximum Array length, a
maximum ArrayBuffer length, a maximum BigInt size, a maximum stack size, etc.
Any one of those limits is potentially debatable, and sometimes it makes sense
to raise them, but the limits as such will remain. Off the top of my head I
don’t know what it would take to bump this particular limit by, say, a factor
of two – and I also don’t know whether a factor of two would be enough to
satisfy your expectations.

</code></pre>
<p>Great details there. It would also be good to know what the behavior is for the
Object, which has those 100% CPU stalls after ~8.3M, but not the same error
message...</p>
<p>Another fun note: if I modify the Object code to use only “integer IDs” the
code actually works fine, does not hit any errors, and is “blazingly fast” as
the kids call it</p>
<pre><code class="language-js">const myObject = {}
for (let i = 0; i &lt;= 50_000_000; i++) {
  myObject[i] = i
  if (i % 100000 == 0) {
    console.log(i)
  }
}
</code></pre>
<p>I presume that this code works because it detects that I’m using it like an
array and it decides to transform how it is working internally and not use a
hash-map-style data structure, so does not hit a limit. There is a slightly
higher limit though, e.g. 1 billion elements gives “Uncaught RangeError:
Invalid array length”</p>
<pre><code class="language-js">const myObject = {}
for (let i = 0; i &lt;= 1_000_000_000; i++) {
  myObject[i] = i
  if (i % 100000 == 0) {
    console.log(i)
  }
}
</code></pre>
<p>This has been another episode of ....the twilight zone (other episodes
catalogued here) https://github.com/cmdcolin/technical_oddities/</p>
<p>[1] The final product of this adventure was this, to create a search index for
a large number of elements https://github.com/GMOD/ixixx-js</p></article></div></main></div><footer style="margin-top:100px"><a href="/">Home</a> <a href="/archive">Blog archive</a> <a href="https://github.com/cmdcolin/">Github</a> <a href="https://twitter.com/cmdcolin">Twitter</a> <a href="/projects">Projects</a> <a href="https://scholar.google.com/citations?view_op=list_works&amp;hl=en&amp;user=--FwzsgAAAAJ">Publications</a> <a href="/sketches">Sketches</a> <a href="/kitty">Kitty</a> <a href="/rss.xml">RSS</a><a href="/about">About</a> </footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"An amazing error message if you put more than 2^24 items in a JS Map object","date":"2021-08-15","slug":"2021-08-15-map-limit","mdxSource":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      p: \"p\",\n      pre: \"pre\",\n      code: \"code\"\n    }, _provideComponents(), props.components);\n    return _jsxs(_Fragment, {\n      children: [_jsx(_components.p, {\n        children: \"One of the fun things about working with big data is that you can often hit\\nweird limits with a system.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"I was personally trying to load every 'common' single nucleotide polymorphism\\nfor the human genome into memory (dbSNP), of which there are over 37 million\\nentries (there are many more uncommon ones) for the purposes of making a custom\\nsearch index for them [1].\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Turns out, you may run into some hard limits. Note that these are all V8-isms\\nand may not apply to all browsers or engines (I was using node.js for this)\"\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-js\",\n          children: \"const myObject = new Map()\\nfor (let i = 0; i \u003c= 50_000_000; i++) {\\n  myObject.set(i, i)\\n  if (i % 100000 == 0) {\\n    console.log(i)\\n  }\\n}\\n\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"This will crash after adding approx 16.7M elements and say\"\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          children: \"0\\n100000\\n200000\\n...\\n16400000\\n16500000\\n16600000\\n16700000\\n\\nUncaught RangeError: Value undefined out of range for undefined options\\nproperty undefined\\n\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"That is a very weird error message. It says “undefined” three times! Much\\nbetter than your usual “TypeError: Can’t find property ‘lol’ of undefined”. See\\nhttps://bugs.chromium.org/p/v8/issues/detail?id=11852 for a bug filed to help\\nimprove the error message perhaps.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Now, also interestingly enough, if you use an Object instead of a Map\"\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-js\",\n          children: \"const myObject = {};\\nfor (let i = 0; i \u003c= 50_000_000; i++) {\\n  myObject['myobj_’+i]=i;\\n  if(i%100000==0) { console.log(i) }\\n}\\n\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Then it will print...\"\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          children: \"0\\n100000\\n200000\\n...\\n8000000\\n8100000\\n8200000\\n8300000\\n\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"And it will actually just hang there…frozen…no error message though! And it is\\nfailing at ~8.3M elements. Weird right? This is roughly half the amount of\\nelements as the 16.7M case\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Turns out there is a precise hard limit for the Map case\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"For the Map: 2^24=16,777,216\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"For the Object it is around 2^23=8,388,608 HOWEVER, I can actually add more\\nthan this, e.g. I can add 8,388,609 or 8,388,610 or even more, but the\\noperations start taking forever to run, e.g. 8,388,999 was taking many minutes\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Very weird stuff! If you expected me to dig into this and explain it in deep\\ntechnical detail, well, you’d be wrong. I am lazy. However, this helpful post\\non stackoverflow by a V8 js engine developer clarifies the Map case!!\\nhttps://stackoverflow.com/questions/54452896/maximum-number-of-entries-in-node-js-map\"\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          children: \"V8 developer here. I can confirm that 2^24 is the maximum number of entries in\\na Map. That’s not a bug, it’s just the implementation-defined limit.\\n\\nThe limit is determined by:\\n\\nThe FixedArray backing store of the Map has a maximum size of 1GB (independent\\nof the overall heap size limit) On a 64-bit system that means 1GB / 8B = 2^30 /\\n2^3 = 2^27 ~= 134M maximum elements per FixedArray A Map needs 3 elements per\\nentry (key, value, next bucket link), and has a maximum load factor of 50% (to\\navoid the slowdown caused by many bucket collisions), and its capacity must be\\na power of 2. 2^27 / (3 * 2) rounded down to the next power of 2 is 2^24, which\\nis the limit you observe.  FWIW, there are limits to everything: besides the\\nmaximum heap size, there’s a maximum String length, a maximum Array length, a\\nmaximum ArrayBuffer length, a maximum BigInt size, a maximum stack size, etc.\\nAny one of those limits is potentially debatable, and sometimes it makes sense\\nto raise them, but the limits as such will remain. Off the top of my head I\\ndon’t know what it would take to bump this particular limit by, say, a factor\\nof two – and I also don’t know whether a factor of two would be enough to\\nsatisfy your expectations.\\n\\n\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Great details there. It would also be good to know what the behavior is for the\\nObject, which has those 100% CPU stalls after ~8.3M, but not the same error\\nmessage...\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Another fun note: if I modify the Object code to use only “integer IDs” the\\ncode actually works fine, does not hit any errors, and is “blazingly fast” as\\nthe kids call it\"\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-js\",\n          children: \"const myObject = {}\\nfor (let i = 0; i \u003c= 50_000_000; i++) {\\n  myObject[i] = i\\n  if (i % 100000 == 0) {\\n    console.log(i)\\n  }\\n}\\n\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"I presume that this code works because it detects that I’m using it like an\\narray and it decides to transform how it is working internally and not use a\\nhash-map-style data structure, so does not hit a limit. There is a slightly\\nhigher limit though, e.g. 1 billion elements gives “Uncaught RangeError:\\nInvalid array length”\"\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-js\",\n          children: \"const myObject = {}\\nfor (let i = 0; i \u003c= 1_000_000_000; i++) {\\n  myObject[i] = i\\n  if (i % 100000 == 0) {\\n    console.log(i)\\n  }\\n}\\n\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"This has been another episode of ....the twilight zone (other episodes\\ncatalogued here) https://github.com/cmdcolin/technical_oddities/\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"[1] The final product of this adventure was this, to create a search index for\\na large number of elements https://github.com/GMOD/ixixx-js\"\n      })]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"2021-08-15-map-limit"},"buildId":"Vt7j04wIfYAWhKVSMgEoq","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>