{"pageProps":{"allPosts":[{"title":"Making a twitter bot in the year 2022 with node.js","date":"2022-08-26","slug":"2022-08-26-twitterbot","html":"<p>Recently, we made the logistic map fractal visualizer app, but, how do we keep\nUSER ENGAGEMENT METRICS up? Gotta make a twitter bot to post fractals, that's\nhow!</p>\n<p>I found it somewhat tricky to get this working, because there is a lot of odd\nlingo out there. There are libraries that may help you, but this post goes\nthrough making an OAuth client in node.js to post to the twitter API with\nfairly unabstracted code. OAuth still confuses me, but if you follow these\nsteps, you can make a bot!</p>\n<h3>Step 1</h3>\n<p>Get your keys. I forget the exact procedure for this but go to\n<a href=\"https://developer.twitter.com\">https://developer.twitter.com</a> and sign up, register an \"app\" with them, and\npoke around for awhile.</p>\n<p>After sometime, you should get a screen that looks like this</p>\n<p><img src=\"https://github.com/cmdcolin/twitter_fractal_bot/blob/master/img/devpanel.png?raw=true\" alt=\"\"></p>\n<h3>Step 2</h3>\n<p>Make a repo with a couple dependencies</p>\n<pre><code class=\"language-sh\">mkdir mybot\ncd mybot\nyarn init\nyarn add node-fetch formdata-node oauth dotenv typescript\n</code></pre>\n<h3>Step 3</h3>\n<p>Update <code>package.json</code> with a couple scripts to build these typescript files\n(alternatively use <code>ts-node</code> or similar to run the ts files directly)</p>\n<pre><code class=\"language-json\">{\n  \"dependencies\": {\n    \"dotenv\": \"^16.0.1\",\n    \"formdata-node\": \"^4.4.1\",\n    \"node-fetch\": \"^3.2.10\",\n    \"oauth\": \"^0.10.0\",\n    \"typescript\": \"^4.7.4\"\n  },\n  \"license\": \"MIT\",\n  \"type\": \"module\",\n  \"devDependencies\": {\n    \"@types/node\": \"^18.7.9\",\n    \"@types/oauth\": \"^0.9.1\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"prepost\": \"yarn build\",\n    \"post\": \"node dist/bot.js\"\n  }\n}\n</code></pre>\n<p>And <code>tsconfig.json</code> file</p>\n<pre><code class=\"language-json\">{\n  \"include\": [\"src\"],\n  \"compilerOptions\": {\n    \"target\": \"esnext\",\n    \"outDir\": \"dist\",\n    \"moduleResolution\": \"node\",\n    \"declaration\": true,\n    \"strict\": true,\n    \"esModuleInterop\": true\n  }\n}\n</code></pre>\n<p>This compiles the <code>src</code> folder and outputs js files to the <code>dist</code> directory,\nand we can run the <code>node dist/bot.js</code> file with node to post the file</p>\n<h3>Step 4</h3>\n<p>Create a .env file with your keys that you found in the above screenshot\ncorresponding to the box1 (which gives you API_KEY/API_SECRET, sometimes also\nreferred to as consumer keys in other documents) and box2 (which gives you\nACCESS_TOKEN and ACCESS_TOKEN_SECRET) from\nthe above screenshot</p>\n<pre><code>API_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxx\"\nAPI_SECRET=\"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\"\nACCESS_TOKEN=\"aaaaaaaaaaaaaaaaaaa-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\nACCESS_TOKEN_SECRET=\"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\"\n</code></pre>\n<p>Important: Also add the .env to your .gitignore, you don't want to commit this\nto github!</p>\n<h3>Step 5</h3>\n<p>Make the bot! Create <code>src/bot.ts</code></p>\n<pre><code class=\"language-typescript\">import fs from 'fs'\nimport * as dotenv from 'dotenv'\nimport OAuth from 'oauth'\nimport fetch, { RequestInit } from 'node-fetch'\nimport { FormData, File } from 'formdata-node'\n\ndotenv.config()\n\nasync function mfetch(url: string, params: RequestInit) {\n  const response = await fetch(url, params)\n\n  if (!response.ok) {\n    const text = await response.text()\n    throw new Error(`HTTP ${response.status} ${text}`)\n  }\n  return response.json() as Promise&#x3C;Record&#x3C;string, unknown>>\n}\n\nfunction getAuthHeader(oauth: OAuth.OAuth, url: string) {\n  return oauth.authHeader(\n    url,\n    process.env.ACCESS_TOKEN as string,\n    process.env.ACCESS_TOKEN_SECRET as string,\n    'post',\n  )\n}\n\n;(async () => {\n  try {\n    const client = new OAuth.OAuth(\n      'https://api.twitter.com/oauth/request_token',\n      'https://api.twitter.com/oauth/access_token',\n      process.env.API_KEY as string,\n      process.env.API_SECRET as string,\n      '1.0A',\n      null,\n      'HMAC-SHA1',\n    )\n\n    const picEndpoint = 'https://upload.twitter.com/1.1/media/upload.json'\n    const tweetEndpoint = 'https://api.twitter.com/2/tweets'\n    const clientName = 'v3CreateTweetJS'\n\n    const form = new FormData()\n    form.set(\n      'media',\n      new File([fs.readFileSync('yourpicture.png')], 'yourpicture.png'),\n    )\n\n    // first post a picture\n    const response1 = await mfetch(picEndpoint, {\n      headers: {\n        Authorization: getAuthHeader(client, picEndpoint),\n        'user-agent': clientName,\n      },\n      method: 'POST',\n      //@ts-ignore\n      body: form,\n    })\n\n    // then post a tweet, referring to the media_id_string from response1\n    const response2 = await mfetch(tweetEndpoint, {\n      headers: {\n        Authorization: getAuthHeader(client, tweetEndpoint),\n        'user-agent': clientName,\n        'content-type': 'application/json',\n        accept: 'application/json',\n      },\n      body: JSON.stringify({\n        media: { media_ids: [response1.media_id_string] },\n        text: 'Hello world!',\n      }),\n      method: 'post',\n    })\n    console.log(response2)\n  } catch (e) {\n    console.error(e)\n    process.exit(1)\n  }\n})()\n</code></pre>\n<h3>Step 6</h3>\n<p>Now that you have the bot, you can run it locally with <code>yarn post</code> (runs the\n\"post\" script in <code>package.json</code>) to test it out</p>\n<h3>Step 7</h3>\n<p>Now to create a github action to run the bot.</p>\n<p>Navigate to your repo's settings page, then go to Secrets->Actions and paste\nthe keys one by one, see below screenshot</p>\n<p><img src=\"https://user-images.githubusercontent.com/6511937/187038172-80b35e34-03dd-4613-b6f6-9f8e25d2fc34.png\" alt=\"\"></p>\n<h3>Step 8</h3>\n<p>Create a file with the path <code>.github/workflows/post.yml</code> in your repo to make the post</p>\n<p>You can use a cron/scheduled workflow in the github action, which the below\nexample does\n<a href=\"https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#schedule\">https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#schedule</a></p>\n<p>The below github action posts every 5 hours on the hour (see\n<a href=\"https://crontab.guru/\">https://crontab.guru/</a> for more info on cron syntax)</p>\n<pre><code class=\"language-yaml\">name: Post tweet\non:\n  schedule:\n    - cron: '0 */5 * * *'\n\njobs:\n  test:\n    name: Post tweet\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Use Node.js 14.x\n        uses: actions/setup-node@v2\n        with:\n          node-version: '14'\n      - name: Install deps (with cache)\n        uses: bahmutov/npm-install@v1\n      - name: Post tweet\n        env:\n          API_KEY: ${{ secrets.API_KEY }}\n          API_SECRET: ${{ secrets.API_SECRET }}\n          ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n          ACCESS_TOKEN_SECRET: ${{ secrets.ACCESS_TOKEN_SECRET }}\n        run: yarn post\n</code></pre>\n<h2>Conclusion</h2>\n<p>See <a href=\"https://github.com/cmdcolin/twitter_fractal_bot\">https://github.com/cmdcolin/twitter_fractal_bot</a> for working example. I\ncould have, in retrospect, used a library like <code>twit</code>\n(<a href=\"https://www.npmjs.com/package/twit\">https://www.npmjs.com/package/twit</a>) but this code sample is not substantially\nmore complicated than using the <code>twit</code> library.</p>\n<p>You can also adapt this to post only when you push to your repo, or release a\nnew version from your repo!</p>\n<h2>Footnote 1: Examples of odd lingo or stumbling blocks you might come across</h2>\n<ul>\n<li>Bearer token - The Bearer token is an alternative method of authenticating,\nbut it cannot be used to post tweets</li>\n<li>Twitter API v1 vs v2 - We use a mix of v1 and v2, it's just the different\nURLs that we are posting to and can be mixed in our example</li>\n<li>Consumer key vs API key - they are the same thing in our example. If you look\nclosely at the box 1 in the screenshot it says \"Consumer keys\" and then gives\nyou an API key below *</li>\n</ul>\n<h2>Footnote 2: Posting on release with a github action</h2>\n<p>Replace the cron section of the github action with</p>\n<pre><code>on: release\n</code></pre>\n<h2>Footnote 3: Additional reading</h2>\n<p><a href=\"https://aaronparecki.com/oauth-2-simplified/\">https://aaronparecki.com/oauth-2-simplified/</a></p>"},{"title":"Using Rust/WASM in a monorepo with create-react-app","date":"2022-08-22","slug":"2022-08-22-rustwasm","html":"<p>Behold, the buzzwords:</p>\n<ul>\n<li>Rust / WASM / wasm-bindgen</li>\n<li>React</li>\n<li>Monorepo / Yarn workspaces</li>\n<li>Webpack 5 / create-react-app 5</li>\n<li>Typescript</li>\n</ul>\n<p>The main goal here: To use Rust + WASM in a react app, inside a monorepo.</p>\n<p>TLDR: visit the final product!\n<a href=\"https://github.com/cmdcolin/rust_react_monorepo_template\">https://github.com/cmdcolin/rust_react_monorepo_template</a>. It is also deployed\nlive here <a href=\"https://cmdcolin.github.io/rust_react_monorepo_template\">https://cmdcolin.github.io/rust_react_monorepo_template</a></p>\n<h2>Steps to create this type of integration from scratch</h2>\n<h3>Create repo</h3>\n<pre><code>mkdir template\ncd template\ngit init\n</code></pre>\n<h3>Create root <code>package.json</code></h3>\n<p>Then put this in the monorepo's root <code>package.json</code></p>\n<pre><code class=\"language-json\">{\n  \"private\": true,\n  \"workspaces\": [\"hello-wasm\", \"app\"]\n}\n</code></pre>\n<p>This sets our repo up as a \"monorepo\" with two \"workspaces\". one will be the\nwasm code, in <code>hello-wasm</code>, one will be an instance of <code>create-react-app</code></p>\n<h3>Add a <code>create-react-app</code> instance inside the monorepo</h3>\n<pre><code class=\"language-sh\">npx create-react-app --template typescript app\n</code></pre>\n<p>This will make an <code>app</code> subfolder inside our monorepo</p>\n<h3>Download the hello world rust <code>wasm-bindgen</code> example and put it in a folder named <code>hello-wasm</code></h3>\n<p>Download <a href=\"https://github.com/rustwasm/wasm-bindgen/tree/main/examples/hello_world\">https://github.com/rustwasm/wasm-bindgen/tree/main/examples/hello_world</a> to the hello-wasm folder</p>\n<p>This link can help <a href=\"https://download-directory.github.io/?url=https%3A%2F%2Fgithub.com%2Frustwasm%2Fwasm-bindgen%2Ftree%2Fmain%2Fexamples%2Fhello_world\">https://download-directory.github.io/?url=https%3A%2F%2Fgithub.com%2Frustwasm%2Fwasm-bindgen%2Ftree%2Fmain%2Fexamples%2Fhello_world</a></p>\n<h3>Add some extra fields to the <code>package.json</code> in the <code>hello-wasm</code> folder</h3>\n<pre><code class=\"language-json\">{\n  \"name\": \"hello-wasm\",\n  \"version\": \"1.0.0\",\n  \"files\": [\"pkg\"],\n  \"main\": \"pkg/index.js\"\n  ... rest\n}\n</code></pre>\n<h3>Modify the <code>hello-wasm</code> example to return a value instead of making an alert</h3>\n<p>I changed the rust code to return a String value instead of making an alert box</p>\n<pre><code>#[wasm_bindgen]\npub fn greet(name: &#x26;str) -> String {\n    format!(\"Hello {}\", name)\n}\n</code></pre>\n<h3>Build the <code>hello-wasm</code> pkg</h3>\n<p>Go into the <code>hello-wasm</code> folder and run <code>yarn build</code>. This creates a directory\nnamed <code>pkg</code> which has <code>.wasm</code> files and <code>.js</code> files. Now, the <code>hello-wasm</code>\nfolder is effectively a node package. We could publish this to <code>NPM</code> (see\nfootnote 1)</p>\n<h3>Add the <code>hello-wasm</code> package to the <code>app</code> dependencies</h3>\n<p>Add <code>\"hello-wasm\":\"^1.0.0\"</code> to the <code>dependencies</code> array in <code>app/package.json</code>. This\nwill refer to our local monorepo's rust wasm package!</p>\n<h3>Create craco config for <code>create-react-app</code></h3>\n<p>As of writing, with <code>webpack</code> v5/<code>create-react-app</code> v5, you have to customize\nthe <code>create-react-app</code> to add extra <code>webpack</code> flags.</p>\n<p>So, <code>yarn add @craco/craco</code> in the app folder, then create this <code>craco.config.js</code></p>\n<pre><code class=\"language-js\">module.exports = {\n  webpack: {\n    configure: config => {\n      const wasmExtensionRegExp = /\\.wasm$/\n      config.resolve.extensions.push('.wasm')\n      config.experiments = {\n        syncWebAssembly: true,\n      }\n\n      config.module.rules.forEach(rule => {\n        ;(rule.oneOf || []).forEach(oneOf => {\n          if (oneOf.type === 'asset/resource') {\n            oneOf.exclude.push(wasmExtensionRegExp)\n          }\n        })\n      })\n\n      return config\n    },\n  },\n}\n</code></pre>\n<p>Note: this thread helped me to create the craco config\n<a href=\"https://github.com/Emurgo/cardano-serialization-lib/issues/295\">https://github.com/Emurgo/cardano-serialization-lib/issues/295</a></p>\n<p>Also see footnote 2 for more info</p>\n<h3>Final step: Use async <code>import()</code> to import the <code>hello-wasm</code> greeting</h3>\n<p>We use a <code>useEffect</code> hook to import the code asynchronously, and can call our\nrust function, <code>greet</code>, from javascript!</p>\n<pre><code class=\"language-tsx\">function App() {\n  const [greeting, setGreeting] = useState&#x3C;string>()\n  useEffect(() => {\n    ;(async () => {\n      try {\n        const wasm = await import('hello-wasm')\n        const greeting = wasm.greet('Colin')\n        setGreeting(greeting)\n      } catch (e) {\n        console.error(e)\n      }\n    })()\n  }, [])\n\n  return (\n    &#x3C;div>\n      &#x3C;h1>rust monorepo wasm demo&#x3C;/h1>\n      &#x3C;h2>Greeting from wasm: {!greeting ? 'Loading...' : greeting}&#x3C;/h2>\n    &#x3C;/div>\n  )\n}\n</code></pre>\n<p>In order to greet an arbitrary person, I modified this slightly in the live\ndemo. See\n<a href=\"https://github.com/cmdcolin/rust_react_monorepo_template/blob/master/app/src/App.tsx\">https://github.com/cmdcolin/rust_react_monorepo_template/blob/master/app/src/App.tsx</a></p>\n<h3>Run the app!</h3>\n<p>Go into the <code>app</code> folder, and then run <code>yarn start</code></p>\n<h2>Result!</h2>\n<p>A screenshot of the app, showing the string \"Hello Colin\" which is generated\nvia rust and wasm</p>\n<p><img src=\"/media/rust_wasm_demo.png\" alt=\"\"></p>\n<h2>Conclusion</h2>\n<p>My main aim was to demonstrate creating a \"simple\" monorepo setup showing how\nyou can integrate Rust+WASM and React. Feel free to ask me any questions and go\ncheck out the repo!</p>\n<p><a href=\"https://github.com/cmdcolin/rust_react_monorepo_template\">https://github.com/cmdcolin/rust_react_monorepo_template</a></p>\n<h2>Other resources</h2>\n<p>This article is quite helpful also, but uses a file:/ reference in their\n<code>package.json</code> while my approach uses a monorepo, it is fundamentally quite\nsimilar though!\n<a href=\"https://tkat0.github.io/posts/how-to-create-a-react-app-with-rust-and-wasm\">https://tkat0.github.io/posts/how-to-create-a-react-app-with-rust-and-wasm</a></p>\n<h2>Footnote 1: The <code>hello-wasm</code> folder IS a npm package with wasm files</h2>\n<p>The <code>hello-wasm</code> folder can be published to NPM by itself. When consumers of\nthe package import the module, they would receive <code>pkg/index.js</code> from the\n<code>main</code> field in <code>package.json</code>, and then <code>pkg/index.js</code> in turn imports the\n<code>index.wasm</code> file. Then it is up to the consumers bundler to package that\ncorrectly.</p>\n<h2>Footnote 2: Bundlers and wasm</h2>\n<p>As of writing, I am using <code>webpack</code> v5 (part of <code>create-react-app</code> v5), which has\n\"native support\" for wasm. Still, it is hidden behind a flag called\n\"experiments\" (see first google result for webpack wasm here\n<a href=\"https://webpack.js.org/configuration/experiments/\">https://webpack.js.org/configuration/experiments/</a>) so I use <code>@craco/craco</code> to\nmodify the <code>webpack</code> config of <code>create-react-app</code> v5 to add this.</p>\n<p>Note also: The first time I wrote this, I used <code>webpack</code> v4, which used a\nslightly different workflow (used a special <code>webpack</code> loader called\n<code>wasm-loader</code>)</p>\n<p>You can also likely use similar techniques described in this article to\nincorporate into <code>next.js</code> since it also uses <code>webpack</code>. If you have info on\nhow other bundlers use wasm, feel free to leave a comment.</p>\n<h2>Footnote 3: Why do I have to use async imports?</h2>\n<p>Fundamentally, the <code>.wasm</code> file has to be fetched asynchronously before it can\nbe run (it is not in my experience e.g. embedded as binary data inside a js\nfile) which means it would be difficult to use the wasm code as a synchronous\nimport.</p>\n<p>There are hints that this may be possible but it would rely on the bundler\nembedding the wasm code in the js itself, or maybe top-level-await. If anyone\nhas more info, feel free to leave a comment!</p>\n<h2>Footnote 4: Build setup</h2>\n<p>The <code>hello-wasm</code> package does not automatically recompile when we are running\ne.g. <code>yarn start</code> in the <code>app</code> folder. Therefore, changes to the rust requires\nyou to manually run <code>yarn build</code> in the <code>hello-wasm</code> folder. Just something to\nbe aware of</p>\n<h2>Footnote 5: My first experience with trying to make this work was rocky!</h2>\n<p>I first created an example of rust+wasm+react almost two years ago when\ncreating a fractal viewer\n<a href=\"https://github.com/cmdcolin/logistic_chaos_map\">https://github.com/cmdcolin/logistic_chaos_map</a>\nand it has some development notes on the stumbling blocks I faced\n<a href=\"https://github.com/cmdcolin/logistic_chaos_map/blob/master/NOTES.md\">https://github.com/cmdcolin/logistic_chaos_map/blob/master/NOTES.md</a></p>\n<h2>Footnote 6: I thought you said typescript too</h2>\n<p>Yep! The <code>hello-wasm</code> example generates typescript <code>.d.ts</code> files! Check out the\n<code>hello-wasm/pkg/</code> folder after you build it! This was none of my doing, just a\nbuilt-in feature. PS: I highly recommend inspecting the <code>pkg</code> folder that is\nproduced in the <code>hello-wasm</code> build to help understand the details. I also\nrecommend reading the <a href=\"https://rustwasm.github.io/wasm-bindgen/\">https://rustwasm.github.io/wasm-bindgen/</a> docs and if you\nare getting started with rust, read the Rust Book along with doing rustlings\n<a href=\"https://github.com/rust-lang/rustlings\">https://github.com/rust-lang/rustlings</a></p>"},{"title":"Photos of vacation","date":"2022-08-21","slug":"2022-08-21-vacation","html":"<p>From Aug 1st->Aug 18 I did a road trip from the South Carolina to New Mexico. I\nflew out and met my partner who had driven there earlier to help her mom out at\nthe house.</p>\n<p>It was a great trip</p>\n<p>Route stops</p>\n<ul>\n<li>Charleston, SC (Several days)</li>\n<li>Charlotte, NC (Several days, Wedding)</li>\n<li>Montgomery, AL (One night)</li>\n<li>New Orleans, LA (Several days)</li>\n<li>Houston, TX (One night)</li>\n<li>Junction, TX (One night)</li>\n<li>Alpine, TX (One night)</li>\n<li>Valentine, TX (One night)</li>\n<li>Truth or Consequences, NM (One night)</li>\n<li>Albuquerque, NM (Home)</li>\n</ul>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220803_155653295.resized.jpg\" alt=\"\"></p>\n<p>amazing results from the sea turtle rescue at the charleston, SC aquarium</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220805_145459605_HDR.resized.jpg\" alt=\"\"></p>\n<p>my partner Leah at edisto beach state park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220805_151340018_HDR.resized.jpg\" alt=\"\"></p>\n<p>more edisto, the beach has many dead and standing dead trees on the beach</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220805_151800531_HDR.resized.jpg\" alt=\"\"></p>\n<p>more edisto</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220805_151805593_HDR.resized.jpg\" alt=\"\"></p>\n<p>more edisto</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220806_093129359.resized.jpg\" alt=\"\"></p>\n<p>wetlands near bowen's island SC</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220806_094813656_HDR.resized.jpg\" alt=\"\"></p>\n<p>a worn down house with a mural, and fallen traffic signal in SC</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220807_173529613_HDR.resized.jpg\" alt=\"\"></p>\n<p>photo of a butterfly from wedding site near charlotte, NC</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220808_103832831_HDR.resized.jpg\" alt=\"\"></p>\n<p>our weird airbnb cabin in near charlotte, SC which was a civil war era cabin from virginia moved in it's entirety to NC</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220809_174740904_HDR.resized.jpg\" alt=\"\"></p>\n<p>new orleans skyline from a weird industrial art park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220809_182035943_HDR.resized.jpg\" alt=\"\"></p>\n<p>care bear in new orleans</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220809_190950114.resized.jpg\" alt=\"\"></p>\n<p>reality continues to ruin my life</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220810_121137729.resized.jpg\" alt=\"\"></p>\n<p>wild \"surf n turf po boy\" (right) and a more conventional muffalata sandwich (left) in new orleans, had while hiding from a rain storm</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220811_140043783_HDR.resized.jpg\" alt=\"\"></p>\n<p>city park, new orleans</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220811_140543522_HDR.resized.jpg\" alt=\"\"></p>\n<p>large oak, new orleans city park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220811_142427022.resized.jpg\" alt=\"\"></p>\n<p>apple snail eggs in new orleans city park, an invasive species <a href=\"https://www.theadvocate.com/baton_rouge/entertainment_life/home_garden/article_775fb018-5268-11ea-8f75-f7e8fb8525fe.html\">https://www.theadvocate.com/baton_rouge/entertainment_life/home_garden/article_775fb018-5268-11ea-8f75-f7e8fb8525fe.html</a></p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220811_153012540.resized.jpg\" alt=\"\"></p>\n<p>placard describing \"pit beef\", a baltimore sandwich from the southern museum of cooking</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220811_154040098.resized.jpg\" alt=\"\"></p>\n<p>engraving describing some native american beach early \"bbq\" method (gator and dog possibly visible) from the southern museum of cooking</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220811_154404713.resized.jpg\" alt=\"\"></p>\n<p>history of bbq from the southern museum of cooking</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220811_154549161.resized.jpg\" alt=\"\"></p>\n<p>hot brown sandwich, placard from the southern museum of cooking</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220812_110617743.resized.jpg\" alt=\"\"></p>\n<p>our airbnb in new orleans, in a very cool mansion in the garden district</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220812_113038243.resized.jpg\" alt=\"\"></p>\n<p>more airbnb</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220812_113224854.resized.jpg\" alt=\"\"></p>\n<p>more airbnb</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220812_113234259.resized.jpg\" alt=\"\"></p>\n<p>more airbnb</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220813_211534759.resized.jpg\" alt=\"\"></p>\n<p>our airbnb hosts in junction, TX gave us tickets to the rodeo there, a one night only event. lot's of cowboy hats</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220814_133823204_HDR.resized.jpg\" alt=\"\"></p>\n<p>goats near socorro caverns</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220814_135951677_HDR.resized.jpg\" alt=\"\"></p>\n<p>peacocks (including an albino?) near socorro caverns</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220815_092445801_HDR.resized.jpg\" alt=\"\"></p>\n<p>mule ears, big bend nat'l park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220815_101452396_HDR.resized.jpg\" alt=\"\"></p>\n<p>dung beetle, big bend nat'l park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220815_102214238_HDR.resized.jpg\" alt=\"\"></p>\n<p>canyon at big bend nat'l park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220815_191014795_HDR.resized.jpg\" alt=\"\"></p>\n<p>fire in valentine, tx. note, my feet are very muddy from trying to wade in the rio grande</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220815_203037340.resized.jpg\" alt=\"\"></p>\n<p>cooking on the fire in valentine, TX</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220816_125951579_HDR.resized.jpg\" alt=\"\"></p>\n<p>metal grave placard, el paso, TX</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220817_092022789_HDR.resized.jpg\" alt=\"\"></p>\n<p>duck pond, truth or consequences, NM</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220820_185137107.resized.jpg\" alt=\"\"></p>\n<p>back in ABQ, fermenting some hot peppers we got on trip</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_7091.jpg\" alt=\"\"></p>\n<p>me in city park, new orleans</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_7378.jpg\" alt=\"\"></p>\n<p>me at the mule ears, big bend nat'l park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_7384.JPG\" alt=\"\"></p>\n<p>cows at big bend nat'l park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_7391.jpg\" alt=\"\"></p>\n<p>(wild?) horses at big bend nat'l park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_7403.jpg\" alt=\"\"></p>\n<p>me in the canyon at big bend nat'l park</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_7663.jpg\" alt=\"\"></p>\n<p>beezle celebrating my birthday (8/17) on return to ABQ</p>"},{"title":"Photos of life","date":"2022-07-29","slug":"2022-07-29-happy","html":"<p>I started this post to try to talk about things that make me happy. It's not\nreally a happiness though that I wanted to talk about, it's just about living\nlife...sometimes it's a struggle and that can be rewarding also. So here are\nsome photos of everyday things</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220619_201622482.resized.jpg\" alt=\"\"></p>\n<p>sunset from backyard. my house is adjacent to an \"abandoned\" golf course so it's just a big open space</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220622_100019949.resized.jpg\" alt=\"\"></p>\n<p>a dog named remy i saw at the dog park visiting parents. remy has a funny habit of herding other dogs around</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220624_210922889.resized.jpg\" alt=\"\"></p>\n<p>the sky from my parents backyard</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220626_071349516.resized.jpg\" alt=\"\"></p>\n<p>my parents dogs zoe (right) and tux (left). my parents walk them everyday and it's great to join them</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220627_145201639_HDR.resized.jpg\" alt=\"\"></p>\n<p>the place I live is a house that we rent. unfortunately and the owners are letting the place fall apart due to leaks in the roof</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220704_133123492.resized.jpg\" alt=\"\"></p>\n<p>a salad I made with jicama, carrot, greens, and orange on july 4th</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220705_200242153_HDR.resized.jpg\" alt=\"\"></p>\n<p>some hazy sun before a rainstorm in NM</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220705_201600095_HDR.resized.jpg\" alt=\"\"></p>\n<p>some sun after the rain</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220709_111938701_HDR.resized.jpg\" alt=\"\"></p>\n<p>cooking purple potatos</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220709_115455797.resized.jpg\" alt=\"\"></p>\n<p>after cooking the purple potatos</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220709_175755642.resized.jpg\" alt=\"\"></p>\n<p>hummingbird hanging out in the backyard</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220710_175851539_HDR.resized.jpg\" alt=\"\"></p>\n<p>a fresno pepper from my weird backyard garden (all plants potted with these felt bags, works fairly ok)</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220713_200957865.resized.jpg\" alt=\"\"></p>\n<p>beezle hanging out in his favorite spot, on top of a box spring in the hallway</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220714_200346243.resized.jpg\" alt=\"\"></p>\n<p>the belt I made for myself from a piece of leather that i found in a shop when I visited toronto pre-pandemic (more pics from making it <a href=\"https://imgur.com/a/Ff5Mn3l\">https://imgur.com/a/Ff5Mn3l</a>)</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220715_230433785.resized.jpg\" alt=\"\"></p>\n<p>fruit and yogurts</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220716_132939371.resized.jpg\" alt=\"\"></p>\n<p>making a drip irrigation system for my weird garden. the sunrise timer was particularly funny. I put the battery pack in backwards and it didn't work, and took it apart to fully understand the mechanism (had to see how the battery contacts touched the inside of the system to see it)</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220717_204303427_HDR.resized.jpg\" alt=\"\"></p>\n<p>pork belly</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220717_210017665.resized.jpg\" alt=\"\"></p>\n<p>bibimbap with pork belly</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220718_174420132_HDR.resized.jpg\" alt=\"\"></p>\n<p>some little tomatos. these were eaten by catepillars shortly after taking these. battling these little critters has been a learning experience</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220719_173156813.resized.jpg\" alt=\"\"></p>\n<p>chicken thighs</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220719_173426354.resized.jpg\" alt=\"\"></p>\n<p>jamaican jerk style sauce</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220719_181601967_HDR.resized.jpg\" alt=\"\"></p>\n<p>using a meat thermometer+grill...been a real level-up</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220719_184903147.resized.jpg\" alt=\"\"></p>\n<p>cooked, perhaps overly so, but quite juicy still due to brining</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220720_174949541_HDR.resized.jpg\" alt=\"\"></p>\n<p>my funny backyard setup, the bowl makes little sounds when it rains (sound recording <a href=\"https://cmdcolinphotos.s3.amazonaws.com/out.mp3\">https://cmdcolinphotos.s3.amazonaws.com/out.mp3</a>)</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220722_201326050_HDR.resized.jpg\" alt=\"\"></p>\n<p>synthesizering</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220724_104240569.resized.jpg\" alt=\"\"></p>\n<p>cooked down some strawberries that my neighbor gave me, she's awesome</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220725_095848819_HDR.resized.jpg\" alt=\"\"></p>\n<p>beezle in his tree</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220725_111117447.resized.jpg\" alt=\"\"></p>\n<p>my brother came to visit me, synthesizering around the breakfast table</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220725_111129150.resized.jpg\" alt=\"\"></p>\n<p>beezle playing the synth</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220725_164816159.resized.jpg\" alt=\"\"></p>\n<p>making some kimchi and saurkraut. these got quite bubbly and active, quite fun to make. should get some airlock jars</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220726_162730132_HDR.resized.jpg\" alt=\"\"></p>\n<p>my brother and I visiting the dry river bed of the rio grande in south albuquerque</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220726_163019069_HDR.resized.jpg\" alt=\"\"></p>\n<p>more dry rio grande. note that many the irrigation ditches around the rio are flowing full...</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220728_170257266_HDR.resized.jpg\" alt=\"\"></p>\n<p>the rio grande north of albuquerque is still flowing</p>\n<p><img src=\"https://cmdcolinphotos.s3.amazonaws.com/IMG_20220728_174207531_HDR.resized.jpg\" alt=\"\"></p>\n<p>very tiny purple flowers</p>"},{"title":"Watch out for your pinky finger","date":"2022-07-08","slug":"2022-07-08-pinky","html":"<p>I have, for several months, suffered a twinge in my left pinky finger from\nexcessive strain due to hand contortions largely caused by keyboard use.</p>\n<p>Particularly brutal in my opinion has been the use of the left-ctrl key for\nkeyboard shortcuts</p>\n<h3>Programs that are particularly troublesome for the left pinky</h3>\n<ul>\n<li>many programs: ctrl+c, ctrl+v, ctrl+x, ctrl+f</li>\n<li>terminal: shift+ctrl+v to paste</li>\n<li>tmux: default ctrl+somekey as leader</li>\n<li>chrome: ctrl+w to close tab, ctrl+t for new tab, ctrl+r for refresh,\nshift+ctrl+r for cache-cleared refresh</li>\n<li>vim: using esc key on the far left, custom configurations using ctrl key e.g.\nctrl+p (I also had ctrl+g, ctrl+f for custom file finders for awhile but no\nlonger)</li>\n</ul>\n<p>In order to make these movements, particularly if you use the left hand to do\nit, you have to contort your hand pretty drastically. Chrome is particularly\nbad because it is not possible(?) to reconfigure their keyboard shortcuts.</p>\n<p>Unfortunately, I did not really listen to warning signs from my hand, and now\nthere is a mild pain from almost any keyboard usage</p>\n<h3>General approaches to help out</h3>\n<ul>\n<li>Take time out of my day e.g. pomodoro to take rests</li>\n<li>Taking a vacation</li>\n<li>Do hand exercises and stretches when idle</li>\n<li>Massages to the muscles</li>\n<li>Using touch typing, making my hands float above the keyboard when typing</li>\n<li>Avoiding curling up the pinky while typing (this may just be me suffering at this point)</li>\n<li>Use both hands to execute shortcuts instead of contorting left hand</li>\n</ul>\n<h3>Technical approaches and key remappings</h3>\n<ul>\n<li>Remap caps lock to left+ctrl. This is a common one you hear recommended and\nit may help, and it has less contortion, particularly for chrome with one\nhand use. So you can say capslock+w, capslock+t, but it is still using pinky\nand can be stressful if not careful (capslock+w fine, capslock+t still\ncontortion if done with one hand)</li>\n<li>More use of the right-hand-only shortcuts or at least avoiding using the left\npinky for shortcuts. I don't have many of these but will look into retraining\nmyself for some of these. I am testing out right ctrl+] for leader in tmux,\nand - and = for split window horizontal and vertical</li>\n</ul>\n<h2>Conclusion</h2>\n<p>It is not great to get to this point. My hand posture and body posture at the\nkeyboard has been bad for years, and it has caught up to me. I recommend people\nbe aware of contorting their hand too much, and use good keyboard etiquette\nlest they suffer the same!</p>\n<h2>Any more ideas?</h2>\n<p>Let me know of ways you take care of your hands (hardware, software, or\notherwise)</p>\n<h2>Other resources</h2>\n<p>See this post by Matt Might for some more resources\n<a href=\"https://matt.might.net/articles/preventing-and-managing-rsi/\">https://matt.might.net/articles/preventing-and-managing-rsi/</a></p>\n<h2>Extenuating circumstances</h2>\n<p>I am not sure if it really affects my typing, but I was bitten by a dog several\nyears ago. The dog clamped down on both my hands one after the other and got my\nleft especially bad, which required stitches. One fairly deep puncture was on\nthe left side of my left hand, which could plausibly affect my pinky. Photo\nbelow</p>\n<p><img src=\"/photos/pinky.jpg\" alt=\"\"></p>"},{"title":"You may not need a bundler for your NPM library","date":"2022-05-27","slug":"2022-05-27-youmaynotneedabundler","html":"<p>I have seen a couple threads on twitter where people complain about the\ndifficulty with publishing NPM libraries or ask what starter kit they should\nuse (or, people recommended starter packs anyways)</p>\n<p>Example threads</p>\n<ul>\n<li><a href=\"https://twitter.com/cramforce/status/1513903035197526017\">https://twitter.com/cramforce/status/1513903035197526017</a></li>\n<li><a href=\"https://twitter.com/oleg008/status/1510006191296061441\">https://twitter.com/oleg008/status/1510006191296061441</a></li>\n<li><a href=\"https://twitter.com/iansu/status/1524860613943382017\">https://twitter.com/iansu/status/1524860613943382017</a></li>\n<li><a href=\"https://twitter.com/mpocock1/status/1525075901905522691\">https://twitter.com/mpocock1/status/1525075901905522691</a></li>\n</ul>\n<p>One thing that is notable to me in these threads is that people often recommend\nthat you use a bundler (a program that combines multiple src files into a\nsingle or fewer output files) when developing a library</p>\n<p>Examples of starter packs suggested in these threads that use bundlers</p>\n<ul>\n<li><code>microbundle</code> - <a href=\"https://github.com/developit/microbundle\">https://github.com/developit/microbundle</a> - uses rollup</li>\n<li><code>esno</code> - <a href=\"https://www.npmjs.com/package/esno\">https://www.npmjs.com/package/esno</a> - uses esbuild</li>\n<li><code>unbuild</code> - <a href=\"https://github.com/unjs/unbuild\">https://github.com/unjs/unbuild</a> - uses rollup</li>\n<li><code>preconstruct</code> - <a href=\"https://github.com/preconstruct/preconstruct\">https://github.com/preconstruct/preconstruct</a> - uses rollup</li>\n<li><code>tsup</code> - <a href=\"https://github.com/egoist/tsup\">https://github.com/egoist/tsup</a> - uses esbuild</li>\n<li><code>tsdx</code> - <a href=\"https://github.com/jaredpalmer/tsdx\">https://github.com/jaredpalmer/tsdx</a> - uses rollup</li>\n<li><code>vite library mode</code> - <a href=\"https://vitejs.dev/guide/build.html#library-mode\">https://vitejs.dev/guide/build.html#library-mode</a> - uses rollup</li>\n<li><code>packemon</code> - <a href=\"https://github.com/milesj/packemon\">https://github.com/milesj/packemon</a> - uses rollup</li>\n<li><code>ts-library-template</code> - <a href=\"https://github.com/shortercode/ts-library-template\">https://github.com/shortercode/ts-library-template</a> - uses rollup</li>\n<li><code>parcel</code> - <a href=\"https://twitter.com/devongovett/status/1524944991402999810\">https://twitter.com/devongovett/status/1524944991402999810</a> - uses parcel</li>\n<li><code>dts</code> - <a href=\"https://weiran-zsd.github.io/dts-cli/\">https://weiran-zsd.github.io/dts-cli/</a> - uses rollup</li>\n<li><code>pkgroll</code> - <a href=\"https://github.com/privatenumber/pkgroll\">https://github.com/privatenumber/pkgroll</a> - uses rollup</li>\n<li><code>mkdist</code> - <a href=\"https://github.com/unjs/mkdist\">https://github.com/unjs/mkdist</a> - referenced by <code>unbuild</code>, uses esbuild</li>\n</ul>\n<p>Not using bundlers</p>\n<ul>\n<li><code>gts</code> - <a href=\"https://github.com/google/gts\">https://github.com/google/gts</a></li>\n<li><code>ts-react-toolbox</code> - <a href=\"https://github.com/zzarcon/ts-react-toolbox/\">https://github.com/zzarcon/ts-react-toolbox/</a></li>\n</ul>\n<p>In summary 2/15 do not use a bundler, 13/15 do use a bundler. Sidenote: webpack notably absent</p>\n<h2>Why would you <em>NOT</em> want a bundler for your library?</h2>\n<p>My main argument is that the consumer of your library is the one that should\nuse a bundler if it is relevant to them. If the library uses a bundler:</p>\n<ul>\n<li>in the best case, it has no impact on the consumer</li>\n<li>in the worst case, it affects the complexity of your library and makes\npossible limitations for your consumers also.</li>\n</ul>\n<p>An example where it can actually create limitations, you might consider code\nsplitting with async <code>import()</code>. If you create a single file bundle, then the\nconsumer of your library may not be able to do code splitting properly via\nasync <code>import()</code></p>\n<h2>Why would you <em>MAYBE</em> want a bundler for your library</h2>\n<p>If you really care about producing a <code>UMD</code> bundle that can be used in a script\ntag, maybe you want a bundler, but the future does not seem to be in <code>UMD</code>.\nOne other possible bundle type is maybe you like the idea of a single file\n<code>ESM</code> module. It is similar where you could maybe reference this from a script\ntag with type module, but this seems like a niche usage. For example, you\nwould still have to consider:</p>\n<ul>\n<li>If you are not bundling dependencies, then what is the benefit of using a\nbundler?</li>\n<li>If you are bundling dependencies, you are not allowing people to get updates\nto your sub-dependencies with semver!</li>\n</ul>\n<p>Add-on: Another concern brought up by users in discussion thread: There is a\ncost to having many small files, e.g. in app startup cost on serverless or any\nnodejs application to loading many small files off disk. To me, this is an app\nlevel concern, similar to bundling for the browser though.</p>\n<h2>My suggestion: no bundler, no starter pack, just <code>tsc</code></h2>\n<p>I'd recommend just compiling your code with <code>tsc</code>, no bundler involved. This\nway, you can develop with typescript, it will output <code>js</code> files, and you can\ndirectly deploy a <code>dist</code> folder of <code>js</code> files to NPM.</p>\n<h3>Example <code>package.json</code></h3>\n<pre><code class=\"language-json\">{\n  \"name\": \"yourlib\",\n  \"version\": \"1.0.0\",\n  \"main\": \"dist/index.js\",\n  \"scripts\": {\n    \"clean\": \"rimraf dist\",\n    \"prebuild\": \"npm run clean\",\n    \"build\": \"tsc\",\n    \"preversion\": \"npm run build\",\n    \"postversion\": \"git push --follow-tags\"\n  },\n  \"files\": [\"dist\", \"src\"],\n  \"devDependencies\": {\n    \"rimraf\": \"^3.0.2\",\n    \"typescript\": \"^4.6.2\"\n  }\n}\n</code></pre>\n<h4>Features of the above <code>package.json</code></h4>\n<ul>\n<li>We can use a single command, <code>yarn publish</code> to publish to npm</li>\n<li>The single <code>yarn publish</code> automatically runs <code>clean</code> and <code>build</code> via\n<code>preversion</code>, then <code>postversion</code>, which pushes the tag to the remote repo</li>\n<li>The <code>\"files\": [\"dist\", \"src\"]</code> refers to publishing the <code>dist</code> and <code>src</code>\ndirectories, and src is used for the <code>sourceMap</code></li>\n</ul>\n<h3>Example<code> tsconfig.json</code></h3>\n<pre><code class=\"language-json\">{\n  \"include\": [\"src\"],\n  \"compilerOptions\": {\n    \"target\": \"es2018\",\n    \"outDir\": \"dist\",\n    \"lib\": [\"dom\", \"esnext\"],\n    \"declaration\": true,\n    \"moduleResolution\": \"node\",\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"esModuleInterop\": true\n  }\n}\n</code></pre>\n<h4>Features of the above <code>tsconfig.json</code></h4>\n<ul>\n<li>Uses <code>\"moduleResolution\": \"node\"</code> - this is not pure-ESM because pure-ESM\nexpects you to import filenames with their file extension, while node module\nresolution can import extensionless paths, but node module resolution\ngenerally works well with consumers that use bundlers themselves</li>\n<li>Uses <code>\"target\": \"es2018\"</code> - This is does a small amount of transpilation of\nsuper modern features, but would generally not require your users to\nbabel-ify their <code>node_modules</code> if they consume your library</li>\n</ul>\n<h2>What about testing?</h2>\n<p>Adding testing is not immediately solved by the above, but bundling doesn't\nreally help testing anyways. It's just a starter pack feature we can add on.\nSome options you have include</p>\n<ul>\n<li>Use <code>ts-jest</code></li>\n<li>Use <code>jest</code> on it's own, plus a <code>babel</code> config with <code>@babel/preset-typescript</code></li>\n<li>Use <code>jest</code> on it's own, run over the compiled output without a <code>babel</code> config</li>\n<li>Use <code>vitest</code></li>\n<li>Possibly something else? Node now has a built-in test runner. It will be a\nslow road to adoption but might become more popular over time\n<a href=\"https://fusebit.io/blog/node-testing-comes-to-core/\">https://fusebit.io/blog/node-testing-comes-to-core/</a></li>\n</ul>\n<h2>Conclusion</h2>\n<p>It is tempting to have nice zero-config solutions and starter kits, but to me,\nit is not really beneficial to use the bundler aspect of many of these for\npublishing to NPM. Am I wrong? Let me know if I am.</p>\n<p>Also, these starter kits may not be maintained for perpetuity. Our team used\n<code>tsdx</code> for some time, but it was not maintained well, and used old typescript\nversion 3.x, and it ended up being hard to remove from our codebase. Learning\nthe basic tools like <code>tsc</code> will help</p>\n<h2>Footnote 1: Shipping \"pure-ESM\"</h2>\n<p>Do you want to make a pure-ESM package? Then you do not want to use\n<code>\"moduleResolution\": \"node\"</code> in <code>tsconfig.json</code>, and you will want to set\n<code>\"type\": \"module\"</code> in <code>package.json</code>. You may also need to explicitly import\nwith <code>.js</code> extensions in your source code, even if you write <code>.ts</code>. This is\nawkward, and something the community is still grappling with.</p>\n<p>If you have ever stumbled on this topic, you will probably want to see this\nlink\n<a href=\"https://gist.github.com/sindresorhus/a39789f98801d908bbc7ff3ecc99d99c\">https://gist.github.com/sindresorhus/a39789f98801d908bbc7ff3ecc99d99c</a></p>\n<h2>Footnote 2: Shipping ESM and CommonJS side by side without going \"pure-ESM\"</h2>\n<p>It can be tricky to go pure-ESM, but you can go most of the way there by using\nboth the \"main\" and \"module\" fields in package.json</p>\n<ul>\n<li>Compile with tsc with `tsc --module commonjs --outDir dist</li>\n<li>Compile with tsc again with <code>tsc --module esnext --outDir esm</code></li>\n<li>Then set \"main\":\"dist/index.js\" for node and <code>\"module\":\"esm/index.js\"</code> for\nbundlers</li>\n</ul>\n<p>The \"module\" field is a field only bundlers recognizes\n<a href=\"https://stackoverflow.com/questions/42708484/what-is-the-module-package-json-field-for\">https://stackoverflow.com/questions/42708484/what-is-the-module-package-json-field-for</a></p>\n<p>Note: This is different from the \"type\":\"module\" which marks your module as\npure-ESM!</p>\n<p>I have found this technique can go a long ways towards keeping your package\ncompatible with nodeJS and bundlers and it does not require \"export maps\" or\nanything which I have found to be difficult to configure</p>\n<h2>Footnote 3: Learning your tools</h2>\n<p>Several people on the\n<a href=\"https://cmdcolin.github.io/posts/2022-05-04-findseddangerous\">my recent post</a>\npost suggested that I did not understand my tools, and that I should just learn\nthe tools correctly and I wouldn't have the problem I had.</p>\n<p>Indeed, what I am now telling people to do in this post is similar: I am saying\n\"just use <code>tsc</code> by itself! Understand you tools! You may have to do more\nresearch and create more boilerplate, but it's <strong>better</strong>\".</p>\n<p>Ultimately though, it's up to you to choose your tools and starter packs and\nwhatnot.</p>\n<h2>Footnote 4: What does it look like when you compile with tsc?</h2>\n<p>When I refer to compiling with tsc above, I compile a <code>src</code> directory into a\n<code>dist</code> directory</p>\n<p>So if I have:</p>\n<pre><code>src/index.ts\nsrc/util.ts\nsrc/components/Button.ts\n</code></pre>\n<p>Running <code>tsc</code> will output:</p>\n<pre><code>dist/index.js\ndist/index.d.ts\ndist/index.js.map\ndist/util.js\ndist/util.d.ts\ndist/util.js.map\ndist/components/Button.js\ndist/components/Button.d.ts\ndist/components/Button.js.map\n</code></pre>\n<p>Then, the <code>dist</code> and <code>src</code> directories are published to npm which enables the\n<code>sourceMaps</code> to work.</p>\n<p>Note: We do not need to explicitly say where the typescript types are with\n\"types\" in <code>package.json</code>, many starter packs do this but it is unneeded for\nthis package as the d.ts files are automatically found.</p>\n<p>See\n<a href=\"https://cmdcolin.github.io/posts/2021-12-31-npm-package\">https://cmdcolin.github.io/posts/2021-12-31-npm-package</a>\nfor my article on creating a typescript package for npm</p>\n<h2>Footnote 5: Other things people recommend</h2>\n<p>Other things people recommend in the starter pack threads</p>\n<ul>\n<li><code>deno</code> - not clear to me how this helps when deploying to npm, but I still gotta try deno</li>\n<li><code>ts-node </code> - not clear to me how this helps when deploying to npm</li>\n<li><code>nx</code> - heavy monorepo-based solution</li>\n<li><code>elm</code> - that's just a different thing</li>\n<li><code>Typescript-Node-Starter</code> - <a href=\"https://github.com/microsoft/TypeScript-Node-Starter\">https://github.com/microsoft/TypeScript-Node-Starter</a> - not a library, full stack framework</li>\n<li><code>joystick</code> - <a href=\"https://github.com/CheatCode/joystick\">https://github.com/CheatCode/joystick</a> - not a library, full stack framework</li>\n</ul>\n<h2>Footnote 6:</h2>\n<p>Similar things happen if you take on dependencies of starter kits like\n<code>create-react-app</code>. You become very committed to their particular way of doing\nthings, and can only modify their config with things like <code>rescripts</code>, <code>craco</code>\nor <code>rewired</code>. If you crafted your setup with just <code>webpack</code>, you may not be so\ntied down. But, I still use things like <code>create-react-app</code> because they do seem\nto help me significantly. Now though, the tide seems to be turning other\ndirections like next.js which can do static site generation at a basic level\nand extend to multiple pages more easily.</p>\n<h2>Footnote 7:</h2>\n<p>You may not even need <code>tsc</code> to compile your dist folder. You can literally\npublish your source <code>.js</code> files as-is to NPM. This suggestion comes from\n@trevmanz <a href=\"https://twitter.com/trevmanz/status/1534962190008172545\">1</a>\n<a href=\"https://twitter.com/trevmanz/status/1534962940348092417\">2</a>. I don't\npersonally use this technique yet but there are many users with this workflow</p>\n<p>An interesting thing is you can write in <code>.js</code> but still get <code>typescript</code> to\nget type checking using <code>jsdoc</code>, just use <code>allowJs</code>/<code>checkJs</code> flags in\n<code>tsconfig.json</code></p>\n<h2>Footnote 8:</h2>\n<p>What about React? Bundlers not needed, and not even babel is needed: you can\ncode your library as in <code>jsx</code> or <code>tsx</code> files and use <code>tsc</code> to compile it to\nReact.createElement statements or other jsx transforms.</p>"},{"title":"Notes on performance profiling JS applications","date":"2022-05-10","slug":"2022-05-10-performanceprofiling","html":"<p>Keeping your program fast is important for</p>\n<ul>\n<li>user satisfaction in everyday apps</li>\n<li>making certain things tractable</li>\n</ul>\n<p>In our application, we visualize some large-ish datasets using the browser and\njavascript</p>\n<h2>The Chrome profiler</h2>\n<p>I use the Chrome DevTools \"Performance\" profiler, which is a\nstatistical/sampling profiler\n<a href=\"https://en.wikipedia.org/wiki/Profiling_(computer_programming)#Statistical_profilers\">https://en.wikipedia.org/wiki/Profiling_(computer_programming)#Statistical_profilers</a></p>\n<p>This means it samples at some rate and see's where in the callstack the program\nis executing.</p>\n<ul>\n<li>If you see large rectangles in the profiler, you may have a long running\nfunction</li>\n<li>If you see many small rectangles, your small function may be called many\ntimes</li>\n</ul>\n<p>Note: sometimes your function may be so fast, it is rarely or never encountered\nby the sampling. It is a good thing (TM) to be this fast, but I mention it to\nnote that the sampling profiler does not give us a complete log of all function\ncalls.</p>\n<h2>Creating a flamegraph from the Chrome profiler results</h2>\n<p>Note: sometimes, it is also useful to see the results as a \"flamegraph\" (see\n<a href=\"https://www.brendangregg.com/flamegraphs.html\">https://www.brendangregg.com/flamegraphs.html</a>)</p>\n<p>The website <a href=\"https://www.speedscope.app/\">https://www.speedscope.app/</a> can\ncreate \"flamegraph\" style figures for Chrome profiling results</p>\n<h2>Stacking up many small optimizations</h2>\n<p>Working with large datasets, sometimes your program will take a long time to\ncomplete. Especially if you work with javascript in the browser, it is a\nchallenge to make things go fast. But you can use micro optimizations to help\nimprove performance over time.</p>\n<p>For example, say a program takes 30 seconds to run on a certain dataset</p>\n<p>If you do profiling and find a couple microoptimizations that give you a 15%,\n10% and 5% performance improvement, then you program now takes 20 seconds to\nrun. That is still not instantaneous, but it is saving users a good 10 seconds.</p>\n<h2>Examples of micro optimizations</h2>\n<ul>\n<li>Using <code>Map</code> instead of <code>Object</code> can often get small performance boosts</li>\n<li>Comparing value against <code>undefined</code> e.g. <code>if(val===undefined)</code> vs just\ncomparing against falsy e.g. <code>if(!val)</code></li>\n<li>Using <code>TypedArray</code>/<code>Uint8Array</code> natively instead of <code>Buffer</code> polyfill. This\none is a kicker for me because we relied on <code>Buffer</code> polyfill, and webpack 5\nstopped bundling polyfills by default which made us wake up to this</li>\n<li>When converting <code>Uint8Array</code> to string, use <code>TextDecoder</code> for large strings, and\njust small string concatenations of <code>String.fromCharCode</code> for small ones.\nThere is an inflection point for string size where one is faster</li>\n<li>Use <code>for</code> loops instead of <code>Array.prototype.forEach</code>/<code>Array.prototype.map</code>. I\nthink similar to above, there is an inflection point (not where it gets\nfaster in the <code>forEach</code>/<code>map</code> case, but where you can choose to care whether\nthe small performance diff matters) based on number of elements in your array</li>\n<li>Pre-allocate an array with <code>new Array(N)</code> instead of just <code>[]</code> if possible</li>\n</ul>\n<p>I have tried to keep track of more microoptimizations here, but they are pretty\nspecific to small examples and may not generalize across browsers or browser\nversions <a href=\"https://gist.github.com/cmdcolin/ef57d2783e47b16aa07a03967fd870d8\">https://gist.github.com/cmdcolin/ef57d2783e47b16aa07a03967fd870d8</a></p>\n<h2>Examples of macro optimizations</h2>\n<p>Oftentimes, large scale re-workings of your code or \"macro\" optimizations are\nthe way to make progress.</p>\n<p>A macro optimization may be revealed if you are looking at your performance\nprofiling result and you think: this entire section of the program could be\nreworked to remove this overhead</p>\n<p>In this case, it is hard to advise on because most of these will be very\nspecific to your particular app.</p>\n<p>Just as a specific example of a macro optimization I undertook:</p>\n<p>We use web workers, and had to serialize a lot of data from the web worker to\nthe main thread. I did a large re-working of the codebase to allow, in\nparticular examples, the main thread to request smaller snippets of data from\nthe web worker thread on-demand (the web worker is kept alive indefinitely)\ninstead of serializing all the web worker data and sending to the main thread.</p>\n<p>This change especially pays off with large datasets, where all that\nserialization/data duplication is computationally and memory expensive. Fun\nfact: I remember sitting at a table at a conference in Jan 2020 talking with my\nteam at the Plant and Animal Genome conference, thinking that we should make\nthis change -- finally did it, just took 2 years. [1]</p>\n<h2>End-to-end optimization testing</h2>\n<p>In order to comprehensively measure whether micro or macro optimizations are\nactually improving your real world performance, it can be useful to create an\nend-to-end test</p>\n<p>For our app, I created a <code>puppeteer</code> based test where I loaded the website and\nwaited for a \"DONE\" condition. I created a variety of different tests which\nallowed me to see e.g. some optimizations may only affect certain conditions.</p>\n<p>Developing the end-to-end test suite tool awhile to develop (read: weeks to\nmature, though some earlier result were available), but it let me compare the\ncurrent release vs experimental branches, and over time, the experimental\nbranches were merged and things got faster. [2]</p>\n<h2>Note that memory usage can be very important to your programs performance.</h2>\n<p>Excessive allocations will increase \"GC pressure\" (the garbage collector will\ninvoke more Minor and Major GC, which you will see in your performance\nprofiling reuslts as yellow boxes)</p>\n<h2>Conclusion</h2>\n<p>It is really important to look at the profiling to see what your program\nactually is spending time on. You can make hypothetical optimizations all day\nand dream of rewriting in rust but you may just have a slow hot path in your JS\ncode that, if optimized, can get big speedups.</p>\n<p>Let me know about your favorite optimizations in the comments!</p>\n<h2>Footnotes</h2>\n<p>[1] Note that things like SharedArrayBuffer also offer a means to share data\nbetween worker and main thread, but these come with many security limitations\nfrom the browser (and was even removed for a time while these security\nimplications were sussed out, due to Spectre/Meltdown vulnerabilities)</p>\n<p>[2] I still have not found a good way to get automated memory usage profiling\nvia puppeteer. You can access window.process.memory in puppeteer, but this\nvariable does not provide info about webworker memory usage\n<a href=\"https://github.com/puppeteer/puppeteer/issues/8258\">https://github.com/puppeteer/puppeteer/issues/8258</a></p>"},{"title":"Using find . -exec sed is dangerous in a git repo","date":"2022-05-04","slug":"2022-05-04-findseddangerous","html":"<p>You want to find and replace all instances of a string in your repo, so you\ngoogle \"find replace directory linux command\". You end up here</p>\n<p><a href=\"https://stackoverflow.com/a/6759339/2129219\">https://stackoverflow.com/a/6759339/2129219</a></p>\n<p>They tell us</p>\n<pre><code class=\"language-sh\">find ./ -type f -exec sed -i -e 's/apple/orange/g' {} \\;\n</code></pre>\n<p>Ignoring the fact that this syntax is very long and hard to type, this command\nis dangerous to use in a git repository. Specifically, this can corrupt your\n.git contents.</p>\n<h2>Why?</h2>\n<p>This command is dangerous because, find . will enumerate dotfiles, including\nthe .git directory, and then of course will run the find and replace inside\nthem. We can see this in the following session</p>\n<pre><code class=\"language-sh\">> mkdir corruptme\n> cd corruptme\n> git init\n> echo \"# README\" > README.md\n> git add README.md\n> git commit -m \"Initial commit\"\n> echo \"Hello world, it is a beautiful day. I sure hope someone doesn't corrupt this git repository\" >> README.md\n> git commit -am \"Update README.md\"\n> find .\n.\n./README.md\n./.git\n./.git/branches\n./.git/config\n./.git/COMMIT_EDITMSG\n...more stuff...good indication the next command might be dangerous....\n> find ./ -type f -exec sed -i -e \"s/README/CORRUPTME/g\" {} \\;\n> git status\nerror: index uses md extension, which we do not understand\nfatal: index file corrupt\n</code></pre>\n<p>In this case, you may be able to recover it e.g. with\n<a href=\"https://stackoverflow.com/questions/1115854/how-to-resolve-error-bad-index-fatal-index-file-corrupt-when-using-git\">https://stackoverflow.com/questions/1115854/how-to-resolve-error-bad-index-fatal-index-file-corrupt-when-using-git</a></p>\n<p>If you happened to replace some random text that is actually in an object file though e.g. .git/objects</p>\n<pre><code class=\"language-sh\">> find ./ -type f -exec sed -i -e \"s/VHTHJM/OOOOOO/g\" {} \\;\n> git status\nerror: inflate: data stream error (incorrect data check)\nerror: corrupt loose object '26cfc5964dfa5355a1747eb6eec6250aab5212d5'\nfatal: unable to read 26cfc5964dfa5355a1747eb6eec6250aab5212d5\n</code></pre>\n<h2>What is better?</h2>\n<p>My take: Use ruplacer <a href=\"https://github.com/dmerejkowsky/ruplacer\">https://github.com/dmerejkowsky/ruplacer</a></p>\n<p>This tool won't try to replace stuff in your .git directory. It will also skip\nanything in .gitignore including e.g. node_modules. I was delighted to discover\nthis tool so, just spreading the word</p>\n<p>Addendum: Reddit /u/Snarwin also recommended using git ls-files instead of find</p>\n<pre><code>git ls-files | xargs sed -i -e 's/apple/orange/g'\n</code></pre>"},{"title":"Cognitive reframing","date":"2022-03-03","slug":"2022-03-02-dont-let-it-get-to-you","html":"<p>A big part of my recent career has not been technical but emotional learning</p>\n<p>This has been very hard, but the benefits are worth it</p>\n<p>A lot of it is mental reframing</p>\n<h2>Meetings</h2>\n<ul>\n<li>Make a mental note of things you are grateful for, can do this before a\nmeeting to make sure you go in with a positive attitude</li>\n<li>Deep breathing, especially if you feel anxious</li>\n<li>Try not to drop bombs of negativity during a meeting, and at least consider\nyour tone when saying things</li>\n<li>Don't try to complain about things that are broken too much. It is\nmean and cruel and demoralizes everyone</li>\n</ul>\n<h2>In your off time</h2>\n<ul>\n<li>Try not to dwell on the things that are broken. You are smart. You will fix\nthem in time. It doesn't help to put your anxiety levels so high</li>\n</ul>\n<h2>Remote work anxieties/feeling appreciated</h2>\n<ul>\n<li>If someone isn't getting back to your issues, don't take it personally. Try\nreviewing their code, or check in with them</li>\n<li>If you don't feel appreciated, also try not to take it personally. Instead,\ntry to exude positivity in your own life</li>\n<li>Try not to be a hero programmer <a href=\"https://incident.io/blog/no-capes\">https://incident.io/blog/no-capes</a></li>\n</ul>\n<p>This cognitive reframing is extremely important and you can apply it to\nmany areas of your life. You tell your own personal story, and what goes on\nin your head makes it to the real world through your behavior.</p>"},{"title":"Memoizing async functions so that you don't cache errors","date":"2022-02-26","slug":"2022-02-26-memoize-async","html":"<p>There are two hard problems in computer science: <a href=\"https://martinfowler.com/bliki/TwoHardThings.html\">Cache invalidation and naming\nthings</a>. In this post we'll\nshow how memoize an async function, and how to invalidate the memoization when\nthe promise throws an error.</p>\n<p>This helps us with being able to re-try because since the error is not cached,\ncalling it again after an error retries automatically.</p>\n<p>Example async function: fetch from the pokemon API</p>\n<pre><code class=\"language-javascript\">async function getPokemon() {\n  const id = Math.floor(Math.random() * 150)\n  const url = 'https://pokeapi.co/api/v2/pokemon/' + id\n  const ret = await fetch(url)\n  if (!ret.ok) {\n    throw new Error(\n      `Failed to fetch ${url} HTTP ${ret.status} ${ret.statusText}`,\n    )\n  }\n  return ret.json()\n}\n</code></pre>\n<p>Here is a technique that can be used to memoize this function</p>\n<pre><code class=\"language-javascript\">function getPokemonMemoized() {\n  if (!this.promise) {\n    this.promise = getPokemon().catch(e => {\n      this.promise = undefined\n      throw e\n    })\n  }\n  return this.promise\n}\n</code></pre>\n<p>The promise is held in this.promise, and the important part of this function is\nthat when I get an error, I clear this.promise and re-throw the error. The caller\nof the function, on error, will receive the error message, but caching will not\ntake place, allowing retries to take place later on.</p>\n<p>See <a href=\"https://cmdcolin.github.io/pokemon.html\">https://cmdcolin.github.io/pokemon.html</a> for demo</p>\n<h2>Footnote 0: Arguments to function</h2>\n<p>If your function takes arguments, then you can use a hashmap associating the\nargument with the promise. You may also consider using an LRU cache so that\nyour hashmap doesn't grow infinitely in size</p>\n<p>Generally you need a way to stringify or otherwise make them able to be stored\nin a Map or Object to do this.</p>\n<p>See <a href=\"https://github.com/nodeca/promise-memoize\">https://github.com/nodeca/promise-memoize</a> for example</p>\n<h2>Footnote 1: Error handling of <code>fetch</code></h2>\n<p>This demo also demonstrates some basic fetch error handling, and uses\n<code>statusText</code> <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Response/statusText\">which happens to not exist in\nHTTP/2</a>.\nIf you want a semblence of status message text in HTTP/2 you can try to use\nawait ret.json() (if the API returns json error messages) or await ret.text()\ninside the catch clause, but note that it could cause yet another error to be\nthrown</p>\n<h2>Footnote 2: Global cache</h2>\n<p>You could also keep a cache in a global variable, or as a property on a class,\nor other methods. I have also found it useful to have a specific function for\nclearing the cache, so you can get a clean slate each time a test runs in unit\ntesting or similar</p>\n<pre><code class=\"language-javascript\">let promise\nasync function getPokemonMemoized() {\n  if (!promise) {\n    promise = getPokemon().catch(e => {\n      promise = undefined\n      throw e\n    })\n  }\n  return promise\n}\nfunction clearCache() {\n  promise = undefined\n}\n</code></pre>\n<p>You can also make a general purpose utility to memoize any promise function</p>\n<pre><code class=\"language-javascript\">function memoize(fn) {\n  let promise\n  return () => {\n    if (!promise) {\n      promise = fn().catch(e => {\n        promise = undefined\n        throw e\n      })\n    }\n  }\n}\n</code></pre>\n<h2>Footnote 3 - Aborting</h2>\n<p>If you want to handle aborting, it is a bit trickier. Aborting in javascript is\nhandled by\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/AbortController/AbortController\">AbortController</a>.\nThis is an object that gives you an\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal\">AbortSignal</a>\nthat can be passed to fetch calls and the like to stop a big download from\nhappening.</p>\n<p>In our above example, if we passed an abort signal to the first call to fetch,\nand then aborted it, it would abort the fetch, <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/AbortController/abort\">which throws a DOMException\ncalled\n\"AbortError\"</a>.\nYou can detect that it is an AbortError like this, and may choose not to\ndisplay or re-throw the abort exception</p>\n<pre><code class=\"language-javascript\">function isAbortException(e) {\n  return e instanceof Error &#x26;&#x26; exception.name === 'AbortError'\n}\n</code></pre>\n<p>Now, what if 5 functions call getPokemonMemoized(), all passing different abort\nsignals. What if the first one aborts? Then all the rest will get aborted also.\nBut what if we only want to abort the cached call if literally all of them\naborted? Then we may have to synthesize an abortcontroller inside our function</p>\n<pre><code class=\"language-javascript\">let promise\nlet abortcontroller\nlet listeners = 0\nasync function getPokemonMemoized(signal) {\n  if (!promise) {\n    abortcontroller = new AbortController()\n\n    // synthesize a new signal instead of using the passed in signal\n    promise = getPokemon(abortcontroller.signal).catch(e => {\n      promise = undefined\n      throw e\n    })\n  }\n  if (signal) {\n    listeners++\n    // add listener to the passed in signal\n    signal.addEventListener('abort', () => {\n      listeners--\n      if (listeners === 0) {\n        abortcontroller.abort()\n      }\n    })\n  }\n  return promise\n}\n</code></pre>\n<p>A library my team created,\n<a href=\"https://github.com/GMOD/abortable-promise-cache\">abortable-promise-cache</a>,\ntries to help with this scenario with a cleaner abstraction.</p>\n<h2>Footnote 4</h2>\n<p>I have been playing through Pokemon Yellow and find it really amusing hence the\npokemon theme</p>\n<p>Fun stuff: The cutting room floor wiki with unused moves, sounds, and sprites\nin Pokemon Yellow <a href=\"https://tcrf.net/Pok%C3%A9mon_Yellow\">https://tcrf.net/Pok%C3%A9mon_Yellow</a></p>\n<h2>Footnote 5</h2>\n<p>This blog post mentioned in a comment thread <a href=\"https://zansh.in/memoizer.html\">https://zansh.in/memoizer.html</a> has\ngreat interactive examples and shows the \"invalidate on .catch()\" behavior!</p>"},{"title":"Ukraine","date":"2022-02-24","slug":"2022-02-24-ukraine","html":"<p>As Russia is actively invading Ukraine my heart goes out to them.</p>\n<p>Even if I had no personal connection with Ukraine, I would find this abhorrent,\nbut I do have personal connections to Ukraine in various ways that makes me\nthink of them more fondly</p>\n<p>My first, and so far only, consulting job was with a company in Ukraine, around 2012. I helped them configure JBrowse, and prepared an official looking word\ndocument with recommendations and hours (a total of like...2 hours) and they\nwere very nice.</p>\n<p>Stand strong Ukraine</p>"},{"title":"Back when I was a noise musician...","date":"2022-02-15","slug":"2022-02-15-noise","html":"<p>Found this photo of myself from a photo book published by john cates called\nauditory depravation...great documentation of michigan area noise activity. I\nam actually the last photo in the book amongst so many amazing\nartists...totally honored. I forgot I was even in the book when I picked it up\nrandomly.</p>\n<p>thank you john cates!!!</p>\n<p>cover</p>\n<p>index p1</p>\n<p>index p2</p>\n<p>index p3, i'm last as xephedradap</p>\n<p>me</p>\n<p>abstract b/w</p>\n<p>some photos from the book here <a href=\"https://www.johncatesphoto.com/noise\">https://www.johncatesphoto.com/noise</a></p>\n<ul>\n<li><a href=\"https://soundcloud.com/xephedradap/amazingnoisetreasure\">lightyear fluctuations</a> (hnw)</li>\n<li><a href=\"https://soundcloud.com/xephedradap/symptomatic-of-extreme-decay\">symptomatic of extreme decay</a> (circuit bent hnw)</li>\n<li><a href=\"https://soundcloud.com/xephedradap/2016-10-10-13-35-49a\">2016-10-10-13-35-49a</a> (circuit bent drum machine)</li>\n</ul>\n<p>I will make more noise soon...stay tuned</p>"},{"title":"Structural variants and the SAM format - the long (reads) and short (reads) of it","date":"2022-02-06","slug":"2022-02-06-sv-sam","html":"<p>The <code>SAM</code> specification is pretty amazing\n(<a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\">https://samtools.github.io/hts-specs/SAMv1.pdf</a>) but it is also fairly terse\nand abstract. True understanding might come from playing with real world data.\nI will try to relay some things I have learned over the years, with a bit of a\nfocus on how <code>SAM</code> file concepts can relate to structural variants.</p>\n<p>Disclaimer: I'm a developer of JBrowse 2. This document has some screenshots\nand links for it, feel free to try it at <a href=\"https://jbrowse.org\">https://jbrowse.org</a>.</p>\n<h2>Basics</h2>\n<h3>What is a <code>SAM</code> file and how does it relate to <code>BAM</code> and <code>CRAM</code>?</h3>\n<ul>\n<li>\n<p>A <code>SAM</code> file <strong>generally</strong> contains \"reads\" from a sequencer, with information\nabout how they are mapped to a reference genome [1][2].</p>\n</li>\n<li>\n<p>A <code>SAM</code> file is <strong>generally</strong> produced when an aligner takes in raw unaligned reads\n(often stored in <code>FASTQ</code> format files) and aligns them to a reference genome [3].</p>\n</li>\n<li>\n<p>A <code>SAM</code> file is a text format that you can read with your text editor. <code>BAM</code> and\n<code>CRAM</code> are compressed representations of the <code>SAM</code> format.</p>\n</li>\n</ul>\n<p>You can convert <code>SAM</code> to <code>BAM</code> with samtools</p>\n<p><code>samtools view file.sam -o file.bam</code></p>\n<p>You can also convert a <code>BAM</code> back to <code>SAM</code> with samtools view</p>\n<p><code>samtools view -h file.bam -o file.sam</code></p>\n<p>The -h just makes sure to preserve the header.</p>\n<p>If you are converting <code>SAM</code> to <code>CRAM</code>, it may require the -T argument to\nspecify your reference sequence (this is because the <code>CRAM</code> is \"reference\ncompressed\")</p>\n<p><code>samtools view -T reference.fa file.sam -o file.cram</code></p>\n<p>Also see Appendix C: piping FASTQ from <code>minimap2</code> directly to CRAM</p>\n<p>[1] <code>SAM</code> can contain any type of sequence, not specifically reads. If you\ncreated a <em>de novo</em> assembly, you could align the contigs of the <em>de novo</em> assembly\nto a reference genome and store the results in <code>SAM</code>.</p>\n<p>[2] Does not always have to have information about mapping to a reference\ngenome. You can also store unaligned data in <code>SAM</code>/<code>BAM</code>/<code>CRAM</code> (so-called\n<code>uBAM</code> for example) but most of the time, the reads in <code>SAM</code> format are aligned\nto a reference genome.</p>\n<p>[3] Examples of programs that do alignment include <code>bwa</code>, <code>bowtie</code>, and <code>minimap2</code>\n(there are many others). These programs all can produce <code>SAM</code> outputs</p>\n<h3>What is in a <code>SAM</code> file</h3>\n<p>A <code>SAM</code> file contains a header (<code>BAM</code> and <code>CRAM</code> files also have the <code>SAM</code>\nheader) and a series of records. A record is a single line in a <code>SAM</code> file, and\nit generally corresponds to a single read, but as we will see, a split\nalignment may produce multiple records that refer to the same source read.</p>\n<p>Note: if a read failed to align to the reference genome, it may still be in\nyour <code>SAM</code> file, marked as unmapped using the flag column. Sometimes, \"dumpster\ndiving\" (looking at the unmapped records from a <code>SAM</code> file) can be used to aid\nstructural variant searches (e.g. there may be novel sequence in there not from\nthe reference genome that could be assembled)</p>\n<h3>What are tags in a <code>SAM</code> file</h3>\n<p>A SAM file has a core set of required fields, and then an arbitrary list of\nextra columns called tags. The tags have a two character abbreviation like <code>MQ</code>\n(mapping quality) or many others. They can be upper or lower case. Upper case\nare reserved for official usages (except those with X, Y, or Z prefixed). See\n<a href=\"https://samtools.github.io/hts-specs/SAMtags.pdf\">SAMtags.pdf</a> for more\ndetails</p>\n<h3>What is a <code>CIGAR</code> string</h3>\n<p>A <code>CIGAR</code> string is a \"compact idiosyncratic gapped alignment report\". It tells\nyou about insertions, deletions, and clipping. It is a series of \"operators\"\nwith lengths.</p>\n<p>Insertion example:</p>\n<p><code>50M50I50M</code></p>\n<p>That would be 50bp of matching bases (<code>50M</code>), followed by a 50bp insertion\n(<code>50I</code>), followed by another 50bp of matches (<code>50M</code>). The 50bp insertion means\nthe read contains 50 bases in the middle which did not match the reference\ngenome that you are comparing the read to.</p>\n<p>Clipping example:</p>\n<p><code>50S50M50S</code></p>\n<p>This means that 50bp matched (<code>50M</code> in the middle of the <code>CIGAR</code> string) and\nboth sides of the read are soft clipped. The clipping means the aligner was not\nable to align the reads on either side. You could imagine clipping being like\n\"an insertion on either side of the read\" if you like: basically those bases on\neither side did not align.</p>\n<p>Notes:</p>\n<ul>\n<li>\n<p><em>Finding mismatches</em>: A <code>CIGAR</code> string match like <code>50M</code> means 50 bases\n\"matched\" the reference genome, but that only means that there are no\ninsertions or deletions in those 50 bases. There could be underlying\nmismatches in the read compared to the reference. Note: there is also\nextended <code>CIGAR</code> that replaces <code>M</code> with <code>=</code> (exact match) and <code>X</code> (mismatch).\nAlso see Appendix D on the <code>MD</code> tag and finding where the mismatches are, but\nnote that <code>MD</code> tag is tricky</p>\n</li>\n<li>\n<p><em>Ambiguity of representation</em>: A <code>CIGAR</code> string with insertions and deletions\ncould be <code>50M1D1I50M</code>. This string had a 1bp deletion and a 1bp insertion\nback-to-back. This could be just a mismatch! There is ambiguity in sequence\nalignment representations. Downstream programs must accomodate this.</p>\n</li>\n<li>\n<p><em>Split records and soft-clipping</em>: A <code>CIGAR</code> string with soft-clipping\n<code>500S50M</code> this means that 500 bases of the read were not aligned at this\nposition, but 50 bases were! Note that the alignment might have been a split\nalignment (see section on split alignments below) so another record in the\n<code>SAM</code> file, linked by the <code>SA</code> (supplmentary alignment) tag, might contain\ninfo on where the other 500 bases aligned! (or, they might not map anywhere).\nThe linked split or supplementary alignments all have the same read name\n(<code>QNAME</code>).</p>\n</li>\n</ul>\n<p>See <a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\">SAMv1.pdf</a> for all the\nCIGAR operators.</p>\n<p>If you are working with <code>SAM</code> data, you will often write loops that directly\nparse CIGAR strings. See Appendix B for handy functions for parsing <code>CIGAR</code>\nstrings. Don't fear the <code>CIGAR</code>!</p>\n<h2>Detecting SVs from long reads</h2>\n<p>Long reads offer a wide array of methods for detecting SVs</p>\n<ul>\n<li><em>Small insertions/deletions</em>: Long reads can completely span moderate sized\ninsertions and deletions, indicated by <code>I</code> or <code>D</code> in a <code>CIGAR</code> string.</li>\n<li><em>Large insertions/deletions</em>: If a long read does not completely span an\ninsertion or deletion, it may be split aligned on either side of the SV or\ncould be soft/hard clipped where it can't align all the way through an\ninsertion.</li>\n<li><em>Translocations</em>: A split long alignment can span long range or even\ninter-chromosomal translocations, so part of the read maps to one chromosome\nand one part maps to the other</li>\n<li><em>Inversions</em>: A split alignment can span an inversion, the long read is split\ninto multiple parts, one part of it aligns in the reverse orientation, while\nthe other part aligns in the forward orientation</li>\n</ul>\n<p>Note that there are many different methods for detecting SVs from long reads,\ne.g. not all use mapped reads from SAM files, some use <em>de novo</em> assembly, but\nit is still useful to be familiar with mapped read methods.</p>\n<h3>What are split alignments?</h3>\n<p>Split alignments, or chimeric alignments, are alignments where part of the read\nmaps to one place, and another part to another. For example, part of a long\nread may map to <code>chr1</code> and part of it maps to <code>chr4</code>. It is worth reading the\ndefinition of \"Chimeric alignment\" from\n<a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\">SAMv1.pdf</a> when you get the\nchance.</p>\n<p>Split alignments are especially common with long reads, and it can indicate\nlarge structural variants. There may be a structural variant where the two\nchromosomes are fused together, and parts of the read align to multiple\nchromosomes, or the split alignment may align to either side of a large\ndeletion, or they may be split to align through an inversion (part of it aligns\nto the forward strand, part of it to the reverse strand, and again the forward\nstrand)</p>\n<p>There is no limitation on how many splits might occur so the split can align to\n3, 4, or more different places. Each part of the split puts a new line in the\nSAM file, and note that all the records also have the same read name, or <code>QNAME</code> (first\ncolumn of <code>SAM</code>).</p>\n<p>As <a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\">SAMv1.pdf</a> tells us, one\nrecord is marked as \"representative\", I call this the \"primary\" record, while\nthe other components of the split read are maked supplementary, given the 2048\nflag. Only the \"primary\" record generally has a <code>SEQ</code> field. The split\nalignments are generally also all given a <code>SA</code> tag that gives info on where all\nthe other parts of the split are. See\n<a href=\"https://samtools.github.io/hts-specs/SAMtags.pdf\">SAMtags.pdf</a> for more info\non the <code>SA</code> tag.</p>\n<p>Note: split alignments are different from \"multi-mappers\" where the entire read\nmaps maps equally well to, say, <code>chr4</code> and <code>chr1</code>. Split reads maps part to\nchr1, and part to <code>chr4</code>. See again the\n<a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\">SAMv1.pdf</a> for the definition\nof multi-mapping</p>\n<h3>What is the <code>SA</code> tag?</h3>\n<p>The <code>SA</code> tag is outputted on each part of the split alignment, e.g. the primary\ncontains an <code>SA</code> tag that refers to all the locations, <code>CIGAR</code> strings, and\nmore for all the supplementary reads, and each of the supplementary reads also\ncontains an <code>SA</code> tag that refers to the primary alignment and each other\nsupplementary alignment.</p>\n<p>Fun fact: The <code>SA</code> tag conceptually can result in a 'quadratic explosion' of\ndata, because each part of the split contains references to every other part.\nFor example, if a read is split into 4 pieces, then each record would would\nhave an <code>SA</code> tag with 3 segments, so 3*4 segments will be documented in the\n<code>SA</code> tag. In many cases, this is not a problem, but if you imagine a finished\nchromosome aligned to a draft assembly, it may get split so many times\nthis could be a factor.</p>\n<h3>Visualizing split reads across a breakend or translocation</h3>\n<p>This is a specialized JBrowse 2 feature, but if there is an inter-chromosomal\ntranslocation, you can load this into JBrowse and visualize support for this\nevent using our \"breakpoint split view\". This view shows the evidence for the\nreads that are split aligned across an SV, and can show connections between\npaired-end reads across an SV too.</p>\n<p>We also have a workflow called the \"SV inspector\" that helps you setup the\n\"breakpoint split views\"\n(<a href=\"https://jbrowse.org/jb2/docs/user_guide/#sv-inspector\">https://jbrowse.org/jb2/docs/user_guide/#sv-inspector</a>.\nThe SV inspector and Breakpoint split view work best on Breakends (e.g. VCF 4.3\nsection 5.4) and <code>&#x3C;TRA></code> (translocation) events from <code>VCF</code>, or <code>BEDPE</code>\nformatted SV calls, and you can launch the \"breakpoint split view\" from the \"SV\ninspector\"</p>\n<p><img src=\"/media/breakpoint_split_view.png\" alt=\"\"></p>\n<h3>Visualizing a 'read vs reference' view given a split alignment</h3>\n<p>If we are given the the primary alignment of an arbitrary split read, then we\ncan construct what that split looks like compared to the reference genome.</p>\n<p>If we are not given the primary alignment (e.g. we are starting from a\nsupplementary alignment) then we can search the <code>SA</code> list for the one that is\nprimary, because at least one will be.</p>\n<p>Now that we have the primary alignment, it will have the <code>SEQ</code> (of the entire\nread, the supplementary alignments typically have a blank <code>SEQ</code>!) and the <code>SA</code>\ntag containing the <code>CIGAR</code> of all the different parts of the split. We can then\nconstruct how the entire read, not just a particular record of the split\nalignment, compares to the genome. In JBrowse 2 we implemented this and it uses\na synteny-style rendering. [1]</p>\n<p><img src=\"/media/linear_alignment.png\" alt=\"\"></p>\n<p>Figure showing JBrowse 2 piecing together a long read vs the reference genome\nfrom a single read</p>\n<p>In order to do this reconstruction, JBrowse 2 takes the <code>CIGAR</code> strings of the\nprimary alignment and each of the pieces of the <code>SA</code> tag (it is a semi-colon\nseparated list of chunks), sort them by the amount of softclipping (the\nsoftclipping values will progressively trim off more of the <code>SEQ</code> telling you\nit aligned further and further on in the long read), and then this tells me\nwhere each piece of the split alignment came from in the original <code>SEQ</code>, so we\ncan plot the alignments of the read vs the reference genome using synteny style\ndisplay.</p>\n<p>[1] Similar functionality also exists in GenomeRibbon\n<a href=\"https://genomeribbon.org\">https://genomeribbon.org</a></p>\n<h3><code>SAM</code> vs <code>VCF</code> - Breakends vs split alignments</h3>\n<p>An interesting outcome (to me) is that from a single record in a <code>SAM</code> file, I\ncan reconstruct the \"derived\" genome around a region of interest from a single\nread.</p>\n<p>If I was to try to do this with the <code>VCF</code> Breakend specification (section 5.4\nof <a href=\"https://samtools.github.io/hts-specs/VCFv4.3.pdf\">VCF4.3.pdf</a>), it may\nactually be more challenging than from a <code>SAM</code> read. This is because a Breakend\nin <code>VCF</code> format is only an edge in a graph (and the sequences are nodes).\nTherefore, in order to properly reconstruct a structural variant from a <code>VCF</code>\nwith Breakends, I would have to construct a graph and decode paths through it.</p>\n<p>I like the ability to reconstruct the derived genome from a single read, but\nindividual reads can be noisy (contain errors). That said, <em>de novo</em> assembled\ncontigs can also be stored in <code>SAM</code> format and is significantly less noisy\n(being composed of the aggregate information of many reads).</p>\n<p>The point though is that interpretation of the <code>VCF</code> breakend specification is\nchallenging due to imposing a sequence graph on the genome, while the <code>SA</code> tag\nremains just a simple set of linear alignments that can easily be pieced\ntogether, and you only need to refer to a single record in the <code>SAM</code> file to do\nso.</p>\n<p>I am not aware of a lot of tools that work on the <code>VCF</code> Breakend graph, and\nexpect more will need to be created to truly work with this standard. An\ninversion for example may create 4 record in the <code>VCF</code> file (see section 5.4 in\nthe <a href=\"https://samtools.github.io/hts-specs/VCFv4.3.pdf\">VCF4.3.pdf</a> for\nexample), and needs careful interpretation.</p>\n<h3>Haplotype tagged reads</h3>\n<p>A new trend has been to create <code>SAM</code>/<code>BAM</code>/<code>CRAM</code> files with tagged reads,\nwhich tells us which haplotype a read was inferred to have come from. This is\ncommonly done with the <code>HP</code> tag, which might have <code>HP=0</code> and <code>HP=1</code> for a\ndiploid genome. Tools like <code>whatshap</code> can add these tags to a <code>SAM</code> file, and\nIGV and JBrowse 2 can color and sort by these tags.</p>\n<p><img src=\"/media/color_by_tag.png\" alt=\"\"></p>\n<p>Screenshot of JBrowse 2 with the \"Color by tag\" and \"Sort by tag\" setting\nenabled (coloring and sorting by the <code>HP</code> tag) letting us see that only one\nhaplotype has a deletion. Tutorial for how to do this in JBrowse 2 here\n<a href=\"https://jbrowse.org/jb2/docs/user_guide/#sort-color-and-filter-by-tag\">https://jbrowse.org/jb2/docs/user_guide/#sort-color-and-filter-by-tag</a></p>\n<h2>How do you detect SVs with paired-end reads?</h2>\n<p>Paired-end reads are short reads, e.g. 150bp each. This makes them unable to\nrecover some large structural variants.</p>\n<p>However, paired-end reads have a number of attributes that can be used to\ndetect paired end reads</p>\n<h3>Distance between pairs being abnormally large or short</h3>\n<p>The distance between pairs is encoded by the <code>TLEN</code> column in the <code>SAM</code> format.\nThe distance between pairs with good mapping is relatively constant and called\nthe \"insert length\". This comes from how the sequencing is done: paired-end\nsequencing performs sequencing on both ends of a fragment.</p>\n<p>But, if you are mapping reads vs the reference genome, and you observe that\nthey are abnormally far apart, say 50kb apart instead of 1kb apart, this may\nindicate there your sample contains a deletion relative to the reference.</p>\n<p><img src=\"/media/aberrant_size.png\" alt=\"\"></p>\n<p>Screenshot of JBrowse 1 with \"View as pairs\" enabled, and large insert size\ncolored as red (from\n<a href=\"https://jbrowse.org/docs/paired_reads.html\">https://jbrowse.org/docs/paired_reads.html</a>.\nNote that some of JBrowse 1's View as pairs features are not yet available in\nJBrowse 2</p>\n<h3>An abundance of reads being \"clipped\" at a particular position</h3>\n<p>This can indicate that part of the reads map well, but then there was an abrupt\nstop to the mapping. This might mean that there is a sequence that was an\ninsertion at that position, or a deletion, or a translocation.</p>\n<p>The clipping is indicated by the <code>CIGAR</code> string, either at the start or end of\nit by an <code>S</code> or an <code>H</code>. The <code>S</code> indicates \"soft clipping\", and indicates that\nthe sequence of the clipped portion can be found in the <code>SEQ</code> field of the\nprimary alignment. The <code>H</code> is hard clipped, and the sequence that is hard\nclipped will not appear in the <code>SEQ</code>.</p>\n<p><img src=\"/media/clipping_pileup.png\" alt=\"\"></p>\n<p>Screenshot of JBrowse 2 showing blue clipping indicator with a \"pileup\" of\nsoft-clipping at a particular position shown in blue. The clipping is an\n\"interbase\" operation (it occurs between base pair coordinates) so it is\nplotted separately from the normal coverage histogram.</p>\n<p><img src=\"/media/show_soft_clipping.png\" alt=\"\"></p>\n<p>Screenshot of JBrowse 2 showing an insertion with Nanopore (top), PacBio\n(middle) and Illumina short reads. The long reads may completely span the\ninsertion, so the <code>CIGAR</code> string on those have an <code>I</code> operator and are indicated\nby the purple triangle above the reads. For the short reads, the reads near the\ninsertion will be clipped since they will not properly map to the reference\ngenome and cannot span the sinsertion. The \"Show soft clipping\" setting in\nJBrowse 2 and IGV can be used to show visually the bases that extend into the\ninsertion (shown on the bottom track).</p>\n<h3>Unexpected pair orientation</h3>\n<p>With standard paired end sequencing, the pairs normally point at each other</p>\n<pre><code>forward reverse\n --->    &#x3C;---\n</code></pre>\n<p>If the stranded-ness of the pair is off, then it could indicate a structural\nvariant. See Appendix A for a handy function for calculating pair orientation.</p>\n<p>This guide from IGV is helpful for interpreting the pair directionality with\npatterns of SVs using \"Color by pair orientation\"</p>\n<p><a href=\"https://software.broadinstitute.org/software/igv/interpreting_pair_orientations\">https://software.broadinstitute.org/software/igv/interpreting_pair_orientations</a></p>\n<p><img src=\"/media/inverted_duplication.png\" alt=\"\"></p>\n<p>Figure: JBrowse 2 showing an inverted (tandem) duplication in 1000 genomes\ndata. It uses the same coloring as IGV for pair orientation. The tandem\nduplication can produce green arrows which have reads pointing in opposite\ndirections e.g. <code>&#x3C;--</code> and <code>--></code>, while blue arrows which can indicate an\ninversion point in the same direction e.g. <code>--></code> and <code>--></code></p>\n<h3>Caveat about TLEN</h3>\n<p>Note that <code>TLEN</code> is a field in the SAM format that is somewhat ill defined,\nat least in the sense that different tools may use it differently\n<a href=\"https://github.com/pysam-developers/pysam/issues/667#issuecomment-381521767\">https://github.com/pysam-developers/pysam/issues/667#issuecomment-381521767</a></p>\n<p>If needed, you can calculate <code>TLEN</code> yourself if you process the file yourself\n(e.g. process all reads, get the actual records for the pairs, and calculate\ndistance) but I have not had trouble with relying on the <code>TLEN</code> from the data\nfiles themselves.</p>\n<h2>Calling copy number variants with your short or long reads</h2>\n<p>Another type of SV that you can get from your <code>SAM</code> files are copy number\nvariants (CNVs). By looking at the depth-of-coverage for your data files, you\ncan look for abnormalities that may indicate copy number variants. By using a\ntool like <code>mosdepth</code>, you can quickly get a file showing the coverage across\nthe genome.</p>\n<p>Be aware that if you are comparing the coverage counts from different tools,\nthat they have different defaults that may affect comparison. Some discard\n<code>QC_FAIL</code>, <code>DUP</code>, and <code>SECONDARY</code> flagged reads. This is probably appropriate,\nand corresponds to what most genome browsers will display (see\n<a href=\"https://gist.github.com/cmdcolin/9f677eca28448d8a7c5d6e9917fc56af\">https://gist.github.com/cmdcolin/9f677eca28448d8a7c5d6e9917fc56af</a> for a short\nsummary of depth calculated from different tools)</p>\n<p>Note that both long and short reads can be used for CNV detection. Long reads\nmay give more accurate measurements also, with their better ability to map\nsmoothly through difficult regions of the genome.</p>\n<p><img src=\"/media/coverage_cnv.png\" alt=\"\"></p>\n<p>Screenshot showing coverage in <code>BigWig</code> format from nanopore reads on normal\nand tumor tissue from a melanoma cancer cell line (COLO829) plotted using\nJBrowse 2. This coverage data is calculated from nanopore sequencing from\n<a href=\"https://www.biorxiv.org/content/10.1101/2020.10.15.340497v1.full\">here</a> using\n<a href=\"https://github.com/brentp/mosdepth\"><code>mosdepth</code></a>, converted from <code>BedGraph</code> to\n<code>BigWig</code>, and loaded into JBrowse 2. See\n(<a href=\"https://jbrowse.org/code/jb2/v1.6.4/?config=test_data%2Fconfig_demo.json&#x26;session=share-MZj3d18lzH&#x26;password=3X7bS\">demo</a>\nand\n<a href=\"https://jbrowse.org/jb2/docs/user_guide/#viewing-whole-genome-coverage-for-profiling-cnv\">tutorial</a>)</p>\n<h2>The future, with graph genomes and <em>de novo</em> assemblies</h2>\n<p>Currently, SV visualization is highly based on comparing data versus a\nreference genome (and the <code>SAM</code> format is a signature of this: it stores data\nin terms of reference genome coordinates). In the future, SV visualization may\nlook more similar to comparative genomics, where we compare an SV to a\npopulation specific reference from a graph genomes or something like this.</p>\n<p>It is known that <em>de novo</em> assembly has more power to detect SVs than some read\noperations (<a href=\"https://twitter.com/lh3lh3/status/1362921612690010118/photo/1\">https://twitter.com/lh3lh3/status/1362921612690010118/photo/1</a>\nas <em>de novo</em> assembled genomes improve and become more widespread, we may see a\nshift in how SVs are called</p>\n<p>I would also like to see improved ability to do fast or 'on the fly' gene\nprediction on the <em>de novo</em> assembled genomes, and we can see what SNPs or\nmodified splicing might look like in copies of genes (e.g. derived regions of\nthe CNV duplications).</p>\n<p>Fun fact: the\n<a href=\"https://github.com/lh3/gfatools/blob/master/doc/rGFA.md#the-graph-alignment-format-gaf\"><code>GAF</code></a>\n(graphical alignment format) is a strict superset of\n<a href=\"https://github.com/lh3/miniasm/blob/master/PAF.md\"><code>PAF</code></a> (pairwise alignment\nformat) by storing graph node labels in the <code>target name</code> slot of <code>PAF</code>, and\ncan refer to an <code>rGFA</code> (reference genome graph)! Looking forward to the graph\ngenome world.</p>\n<h2>Conclusion</h2>\n<p>Algorithms that actually call structural variants face many challenges, but\nunderstanding how the reads are encoded in SAM format, and seeing what they\nlook like in the genome browser is a useful first step to gaining a better\nunderstanding.</p>\n<p>In summary, some of the signatures of SVs may include:</p>\n<ul>\n<li>Aberrant insert size (<code>TLEN</code>) detection (longer for deletion, shorter for\ninsertion)</li>\n<li>Aberrant pair orientation (pairs are not pointing at each other)</li>\n<li>Split-read detection (<code>SA</code> tag)</li>\n<li><code>CIGAR</code> string processing (<code>D</code> operator for deletions, <code>I</code> operator for\ninsertions)</li>\n<li>Over-abundance of clipping (<code>S</code> or <code>H</code> operators in <code>CIGAR</code>)</li>\n<li>Depth of coverage changes for CNVs</li>\n<li>Aligning <em>de novo</em> assembly vs a reference genome\n(<a href=\"https://twitter.com/lh3lh3/status/1362921612690010118/photo/1\">https://twitter.com/lh3lh3/status/1362921612690010118/photo/1</a>) which can\noutput <code>SAM</code>, but it can also output\n<a href=\"https://github.com/lh3/miniasm/blob/master/PAF.md\"><code>PAF</code></a> format (which can\nbe loaded in JBrowse 2 in the synteny views). Techniques of detecting SVs on\nPAF will be fundamentally pretty similar to the techniques listed above but\nmay look a bit different (see <code>cs</code> tag in <code>PAF</code> for example, it is a modified\n<code>CIGAR</code>-like string)</li>\n</ul>\n<p>If you have any ideas I should include here, let me know!</p>\n<h3>Appendix A: Parsing <code>CIGAR</code> strings</h3>\n<p>This is code that can help determine the pair orientation from a single BAM\nrecord. Might be too much detail but follow along</p>\n<pre><code class=\"language-typescript\">// @param flags - flags from a single read\n// @param ref - the string of the reference sequence, just used to determine if it matches rnext\n// @param rnext - the string of the RNEXT, just used to determine if it matches ref\n// @param tlen - the TLEN field from SAM\n// @return e.g. F1R2 normal paired end orientation\nfunction getPairOrientation(\n  flags: number,\n  ref: string,\n  rnext: string,\n  tlen: number,\n) {\n  // this read is not unmapped &#x26;&#x26;\n  // this read's mate is also not unmapped &#x26;&#x26;\n  // this read's mate is on the same reference genome\n  if (!flags &#x26; 4 &#x26;&#x26; !flags &#x26; 8 &#x26;&#x26; ref === rnext) {\n    const s1 = flags &#x26; 16 ? 'R' : 'F'\n    const s2 = flags &#x26; 32 ? 'R' : 'F'\n    let o1 = ' '\n    let o2 = ' '\n\n    // if first in pair\n    if (flags &#x26; 64) {\n      o1 = '1'\n      o2 = '2'\n    }\n\n    // else if second in pair\n    else if (flags &#x26; 128) {\n      o1 = '2'\n      o2 = '1'\n    }\n\n    const tmp = []\n    if (tlen > 0) {\n      tmp[0] = s1\n      tmp[1] = o1\n      tmp[2] = s2\n      tmp[3] = o2\n    } else {\n      tmp[2] = s1\n      tmp[3] = o1\n      tmp[0] = s2\n      tmp[1] = o2\n    }\n    return tmp.join('')\n  }\n  return null\n}\n</code></pre>\n<p>Then this can be broken down further by orientation type</p>\n<p>Paired end reads are \"fr\"\nMate pair reads are \"rf\"</p>\n<p>So you can interpret e.g. F1R2 in relation to being a paired end read (fr) or mate pair (rf) below and with this link <a href=\"https://software.broadinstitute.org/software/igv/interpreting_pair_orientations\">https://software.broadinstitute.org/software/igv/interpreting_pair_orientations</a></p>\n<pre><code class=\"language-json\">{\n  \"fr\": {\n    \"F1R2\": \"LR\",\n    \"F2R1\": \"LR\",\n\n    \"F1F2\": \"LL\",\n    \"F2F1\": \"LL\",\n\n    \"R1R2\": \"RR\",\n    \"R2R1\": \"RR\",\n\n    \"R1F2\": \"RL\",\n    \"R2F1\": \"RL\"\n  },\n\n  \"rf\": {\n    \"R1F2\": \"LR\",\n    \"R2F1\": \"LR\",\n\n    \"R1R2\": \"LL\",\n    \"R2R1\": \"LL\",\n\n    \"F1F2\": \"RR\",\n    \"F2F1\": \"RR\",\n\n    \"F1R2\": \"RL\",\n    \"F2R1\": \"RL\"\n  }\n}\n</code></pre>\n<h3>Appendix B - <code>CIGAR</code> parsing</h3>\n<pre><code class=\"language-typescript\">// @param cigar: CIGAR string in text form\nfunction parseCigar(cigar: string) {\n  return cigar.split(/([MIDNSHPX=])/)\n}\n</code></pre>\n<p>Then parse the returned array two at a time</p>\n<pre><code class=\"language-typescript\">// this function does nothing, but is informative for how to parse interpret a\n// CIGAR string\n// @param cigar:CIGAR string from record\n// @param readSeq: the SEQ from record\n// @param refSeq: the reference sequence underlying the read\nfunction interpretCigar(cigar: string, readSeq: string, refSeq: string) {\n  const opts = parseCigar(cigar)\n  let qpos = 0 // query position, position on the read\n  let tpos = 0 // target position, position on the reference sequence\n  for (let i = 0; i &#x3C; ops.length; i += 2) {\n    const length = +opts[i]\n    const operator = opts[i + 1]\n    // do things. refer to the CIGAR chart in SAMv1.pdf for which operators\n    // \"consume reference\" to see whether to increment\n    if (op === 'M' || op === '=') {\n      // matches consume query and reference\n      qpos += len\n      tpos += len\n    }\n    if (op === 'I') {\n      // insertions only consume query\n      // sequence of the insertion from the read is\n      const insSeq = readSeq.slice(qpos, qpos + len)\n      qpos += len\n    }\n    if (op === 'D') {\n      // deletions only consume reference\n      // sequence of the deletion from the reference is\n      const delSeq = refSeq.slice(tpos, tpos + len)\n      tpad += len\n    }\n    if (op === 'N') {\n      // skips only consume reference\n      // skips are similar to deletions but are related to spliced alignments\n      tpad += len\n    }\n    if (op === 'X') {\n      // mismatch using the extended CIGAR format\n      // could lookup the mismatch letter in a string containing the reference\n      const mismatch = refSeq.slice(tpos, tpos + len)\n      qpos += len\n      tpos += len\n    }\n    if (op === 'H') {\n      // does not consume query or reference\n      // hardclip is just an indicator\n    }\n    if (op === 'S') {\n      // softclip consumes query\n      // below gets the entire soft clipped portion\n      const softClipStr = readSeq.slice(qpos, qpos + len)\n      qpos += len\n    }\n  }\n}\n</code></pre>\n<p>Note for example, that to determine how long a record is on the reference\nsequence, you have to combine the records start position with the CIGAR string,\nbasically parsing the CIGAR string to add up tpos and return tpos</p>\n<h3>Appendix C - align <code>FASTQ</code> directly to <code>CRAM</code></h3>\n<p>This example from the htslib documentation\n(<a href=\"http://www.htslib.org/workflow/fastq.html\">http://www.htslib.org/workflow/fastq.html</a>\nshows how you can stream directly from <code>FASTQ</code> to <code>CRAM</code> (and generate the\nindex file .crai too)</p>\n<p>If you want, you can make this a little shell script, easy_align_shortreads.sh</p>\n<p>easy_align_shortreads.sh</p>\n<pre><code class=\"language-sh\">#!/bin/bash\nminimap2 -t 8 -a -x sr \"$1\" \"$2\" \"$3\"  | \\\nsamtools fixmate -u -m - - | \\\nsamtools sort -u -@2 - | \\\nsamtools markdup -@8 --reference \"$1\" - --write-index \"$4\"\n</code></pre>\n<p>Similar idea for longreads, except just a single fastq file is generally used for longreads</p>\n<p>easy_align_longreads.sh</p>\n<pre><code class=\"language-sh\">#!/bin/bash\nminimap2 -t 8 -a \"$1\" \"$2\"  | \\\nsamtools fixmate -u -m - - | \\\nsamtools sort -u -@2 - | \\\nsamtools markdup -@8 --reference \"$1\" - --write-index \"$3\"\n</code></pre>\n<p>Then call</p>\n<pre><code class=\"language-sh\">bash easy_align_shortreads.sh ref.fa reads1.fq reads2.fq out.cram\nbash easy_align_longreads.sh ref.fa reads.fq out.cram\n\n## output BAM instead\nbash easy_align_shortreads.sh ref.fa reads1.fq reads2.fq out.bam\nbash easy_align_longreads.sh ref.fa reads.fq out.bam\n</code></pre>\n<p>This same concept works with other common aligners as well like bwa</p>\n<p>Bonus: CRAM to bigwig, for looking at CNV/coverage</p>\n<pre><code class=\"language-sh\">#!/bin/bash\n# quickalign.sh ref.fa 1.fq 2.fq out.cram\n# produces out.cram and out.bw\nsamtools faidx $1\nminimap2 -t 8 -a -x sr \"$1\" \"$2\" \"$3\"  | \\\nsamtools fixmate -u -m - - | \\\nsamtools sort -u -@2 - | \\\nsamtools markdup -@8 --reference \"$1\" - --write-index \"$4\"\n\n\nmosdepth $4 -f $1 $4\ngunzip $4.per-base.bed.gz\nbedGraphToBigWig $4.per-base.bed $1.fa.fai $4.bw\n</code></pre>\n<p>Call as \"quickalign.sh ref.fa 1.fq 2.fq out.cram\" gives you out.cram, out.cram.crai, and out.cram.bw (coverage)</p>\n<h3>Appendix D - the <code>MD</code> tag and finding SNPs in reads</h3>\n<p>The <code>MD</code> tag helps tell you where the mismatches are without looking at the\nreference genome. This is useful because as I mentioned, <code>CIGAR</code> can say <code>50M</code>\n(50 matches) but some letters inside those 50 matches can be mismatches, it\nonly says there are no insertions/deletions in those 50 bases, but you have to\ndetermine where in those 50 bases where the mismatches are. The <code>MD</code> tag can\nhelp tell you where those are, but it is somewhat complicated to decode\n(<a href=\"https://vincebuffalo.com/notes/2014/01/17/md-tags-in-bam-files.html\">https://vincebuffalo.com/notes/2014/01/17/md-tags-in-bam-files.html</a>).\nYou have to combine it with the <code>CIGAR</code> to get the position of the mismatches\non the reference genome. If you have a reference genome to look at, you might\njust compare all the bases within the 50M to the reference genome and look for\nmismatches yourself and forget about the <code>MD</code> tag</p>\n<p>The <code>MD</code> tag is also not required to exist, but the command <code>samtools calmd yourfile.bam --reference reference.fa</code> can add <code>MD</code> tags to your <code>BAM</code> file. It\nis generally not useful for <code>CRAM</code> because <code>CRAM</code> actually does store\nmismatches with the reference genome in it's compression format. Note that\nthere are also some oddities about <code>MD</code> tag representation leading to\ncomplaints (e.g. <a href=\"https://github.com/samtools/hts-specs/issues/505\">https://github.com/samtools/hts-specs/issues/505</a>) leading more\ncredence to \"doing it yourself\" e.g. finding your own mismatches\nby comparing the read sequence with the reference, keeping track of where you\nare on the read and ref position with the <code>CIGAR</code> string (a la Appendix B)</p>"},{"title":"How to make your own npm package with typescript","date":"2021-12-31","slug":"2021-12-31-npm-package","html":"<p>There is a lot of mystery around making your own <code>npm</code> package. Every package\nlikely does it a bit differently, and it can be tricky to get a setup you like.\nShould you use a \"starter kit\" or a boilerplate example? Or just roll your own?\nShould you use a bundler? How do you use typescript?</p>\n<p><strong>*Record scratch **</strong></p>\n<p>Why don't we try starting from scratch and seeing where we can get?</p>\n<p>TLDR: here is a github repo with a template package\n<a href=\"https://github.com/cmdcolin/npm-package-tutorial/\">https://github.com/cmdcolin/npm-package-tutorial/</a></p>\n<h2>Introduction</h2>\n<p>An <code>npm</code> package can be very bare bones. In some sense, npmjs.com is just an\narbitrary file host, and you can upload pretty much anything you want to it.</p>\n<p>The magic is in the package.json file, which tells npm:</p>\n<ul>\n<li>what files are part of your package</li>\n<li>what to use as the \"entry point\" (e.g. the file that should be referenced\nwhen you say <code>const lib = require('mypackage')</code>)</li>\n<li>what pre- and post- processing steps should be done when the package is being\npublished</li>\n<li>and more!</li>\n</ul>\n<p>Let's try an experiment...</p>\n<h2>Initializing a package</h2>\n<p>Open up a terminal, and run</p>\n<pre><code class=\"language-sh\">mkdir mypackage\ncd mypackage\ngit init # make mypackage version controlled\nnpm init\n# or\nyarn init\n</code></pre>\n<p>This init command outputs something like this, and we accept the defaults</p>\n<pre><code class=\"language-sh\">This utility will walk you through creating a package.json file.\nIt only covers the most common items, and tries to guess sensible defaults.\n\nSee `npm help init` for definitive documentation on these fields\nand exactly what they do.\n\nUse `npm install &#x3C;pkg>` afterwards to install a package and\nsave it as a dependency in the package.json file.\n\nPress ^C at any time to quit.\npackage name: (mypackage)\nversion: (1.0.0)\ndescription:\nentry point: (index.js)\ntest command:\ngit repository:\nkeywords:\nlicense: (ISC)\nAbout to write to /home/cdiesh/mypackage/package.json:\n\n{\n  \"name\": \"mypackage\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" &#x26;&#x26; exit 1\"\n  },\n  \"author\": \"Colin\",\n  \"license\": \"ISC\"\n}\n\n</code></pre>\n<p>Then, you can create a file named <code>index.js</code> (in your package.json it says\n<code>\"main\": \"index.js\"</code> to refer to this file, the entrypoint)</p>\n<p>In your <code>index.js</code> file, generally, you would do things like export a function\nor functions. I will use commonjs exports here for maximum compatibility:</p>\n<pre><code class=\"language-js\">module.exports = {\n  hello: () => {\n    console.log('hello world')\n  },\n}\n</code></pre>\n<h2>Publishing a package</h2>\n<p>This npm package, <code>mypackage</code> can now be published to <code>npm</code> with a simple\ncommand.</p>\n<pre><code class=\"language-sh\">npm publish\n# or\nyarn publish\n</code></pre>\n<p>This will prompt you for your npmjs.com username, password, email, and if\nneeded, 2FA token (highly recommended)</p>\n<h2>Using your package after it is published</h2>\n<p>Once it is published, you can use it in your create-react-app app or other npm\npackage.</p>\n<pre><code class=\"language-sh\">npm install mypackage\n# or\nyarn add mypackage\n</code></pre>\n<p>Then you can use</p>\n<pre><code class=\"language-js\">import { hello } from 'mypackage'\n</code></pre>\n<p>in any of your other codebases</p>\n<h2>Summary of the simplest NPM package</h2>\n<p>This all seems pretty boring thus far but it tells us a couple things</p>\n<ol>\n<li>packages can be very very bare bones</li>\n<li>no transpiler or bundler is needed for publishing an npm package</li>\n<li>our package can consist of a single file and it is uploaded to npm, and the\n\"main\" field in package.json provides an entry point</li>\n<li>the filename index.js is not special, probably it is a hangover from the\nname index.html. you can use whatever name you want</li>\n</ol>\n<h2>Adding typescript</h2>\n<p>Let's try adding typescript</p>\n<p>To do this, we will use the typescript compiler to compile a directory of files\nin our \"src\" directory and output the compiled files to a directory named\n\"dist\"</p>\n<p>To start, let's add typescript</p>\n<pre><code class=\"language-sh\">npm install --save-dev typescript\n# or\nyarn add -D typescript\n</code></pre>\n<p>Our package.json now will have <code>typescript</code> in it's <code>devDependencies</code> (this\nmeans that when someone installs your package, it they don't get typescript as\na dependency, it is just a dependency for while you are developing the library\nlocally).</p>\n<p>Then we need to create a tsconfig.json for typescript to use</p>\n<pre><code class=\"language-sh\">yarn tsc --init\n# or\nnpx tsc --init\n</code></pre>\n<p>This will generate a <code>tsconfig.json</code> file (needed by <code>typescript</code>) with a bunch of\noptions, but I have stripped it down in my projects to look like this</p>\n<pre><code class=\"language-json\">{\n  \"include\": [\"src\"],\n  \"compilerOptions\": {\n    \"target\": \"es2018\",\n    \"moduleResolution\": \"node\",\n    \"declaration\": true, // generate .d.ts files\n    \"sourceMap\": true, // generate source map\n    \"outDir\": \"dist\", // output compiled js, d.ts, and source map to dist folder\n    \"strict\": true,\n    \"esModuleInterop\": true\n  }\n}\n</code></pre>\n<p>Now, we want to change our <code>js</code> to <code>ts</code> files to use <code>typescript</code>, let's change them\nto use normal ESM import/exports</p>\n<p>util.ts</p>\n<pre><code class=\"language-typescript\">export function getMessage() {\n  return 'hello'\n}\n</code></pre>\n<p>index.ts</p>\n<pre><code class=\"language-typescript\">import { getMessage } from './util'\nexport function sayMessage() {\n  console.log(getMessage())\n}\n</code></pre>\n<p>And then we will add a <code>\"build\"</code> script to <code>package.json</code> to compile the\nlibrary, and refer to the <code>\"dist\"</code> directory for the <code>\"files\"</code> and <code>\"main\"</code>\nfields in <code>package.json</code></p>\n<pre><code class=\"language-json\">{\n  \"name\": \"mypackage\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"src/index.js\",\n  \"files\": [\"dist\"],\n  \"scripts\": {\n    \"build\": \"tsc\"\n  },\n  \"author\": \"Colin\",\n  \"license\": \"ISC\",\n  \"devDependencies\": {\n    \"typescript\": \"^4.5.4\"\n  }\n}\n</code></pre>\n<p>We can now run</p>\n<pre><code class=\"language-sh\">npm run build\n# or\nyarn build\n</code></pre>\n<p>And this will run the <code>\"build\"</code> script we created, which in turn, just runs\n<code>tsc</code> with no arguments.</p>\n<p>You can also add a <code>\"prebuild\"</code> script that clears out the old contents. In fact,\nnpm scripts generalizes the naming system -- you can make scripts with whatever name you want, e.g.</p>\n<pre><code class=\"language-json\">{\n  \"scripts\": {\n    \"preparty\": \"echo preparty\",\n    \"party\": \"echo party\",\n    \"postparty\": \"echo postparty\"\n  }\n}\n</code></pre>\n<p>Then running</p>\n<pre><code class=\"language-sh\">$ yarn party\npreparty\nparty\npostparty\n</code></pre>\n<p>To make this useful, we will use <code>rimraf</code> (a node package) to make a\ncross-platform removal of the <code>dist</code> directory</p>\n<pre><code class=\"language-sh\">npm install --save-dev rimraf\n# or\nyarn add -D rimraf\n</code></pre>\n<p>and then update your package.json</p>\n<pre><code class=\"language-json\">{\n  ...\n  \"scripts\": {\n    \"clean\": \"rimraf dist\",\n    \"prebuild\": \"npm run clean\",\n    \"build\": \"tsc\"\n  },\n  \"devDependencies\": {\n    \"rimraf\": \"^3.0.2\",\n    \"typescript\": \"^4.5.4\"\n  }\n}\n</code></pre>\n<p>We could make it say \"rm -rf dist\" instead of \"rimraf dist\" (e.g. run arbitrary\nshell commands), but rimraf allows it to be cross-platform</p>\n<h2>Making sure you create a fresh build before you publish</h2>\n<p>Without extra instructions, your <code>yarn publish</code> command would not create a\nfresh build and you could publish an older version that was lingering in the\n<code>dist</code> folder.</p>\n<p>We can use a <code>preversion</code> script that will automatically get invoked when you\nrun <code>yarn publish</code> to make sure you get a fresh build in the <code>dist</code> folder\nbefore you publish</p>\n<pre><code class=\"language-json\">{\n  ...\n  \"scripts\": {\n    ...\n    \"preversion\": \"npm run build\",\n  },\n}\n</code></pre>\n<h2>Making sure you push your tag to github after publish</h2>\n<p>When you run <code>yarn publish</code>, npm will automatically create a commit with the\nversion name and a git tag, it <em>will not</em> automatically push tag to your\nrepository.</p>\n<p>Add a <code>postversion</code> script that pushes the tag to your repo after your publish</p>\n<pre><code class=\"language-json\">{\n  ...\n  \"scripts\": {\n    ...\n    \"postversion\": \"git push --follow-tags\",\n  },\n}\n</code></pre>\n<h2>Incremental builds</h2>\n<p>We can use this to do incremental/watch builds</p>\n<pre><code>npm run build --watch\n# or\nyarn build --watch\n</code></pre>\n<h2>Adding testing with ts-jest</h2>\n<p>You can use ts-jest to test your code. This involves installing jest, typescript, ts-jest, @types/jest, and then initializing a jest.config.json</p>\n<pre><code class=\"language-sh\">npm i -D jest typescript\n# or\nyarn add --dev jest typescript\n</code></pre>\n<pre><code class=\"language-sh\">npm i -D ts-jest @types/jest\n# or\nyarn add --dev ts-jest @types/jest\n</code></pre>\n<pre><code class=\"language-sh\">npx ts-jest config:init\n# or\nyarn ts-jest config:init\n</code></pre>\n<p>We can then create a test</p>\n<p><code>test/util.spec.ts</code></p>\n<pre><code class=\"language-typescript\">import { getMessage } from '../src/util'\ntest('expected message returned', () => {\n  expect(getMessage()).toBe('hello')\n})\n</code></pre>\n<p>Then we can then create a script in the package.json that says <code>\"test\": \"jest\"</code>, and then we can say</p>\n<pre><code>npm run test\n# or\nyarn test\n</code></pre>\n<p>You can also create an alternative system where you use <code>babel-eslint</code> and\nvarious babel strategies to test your code, but if you are using typescript,\nts-jest+typescript works great.</p>\n<h2>Add a .gitignore</h2>\n<p>Create a .gitignore with just a line that references this <code>dist</code> folder and <code>node_modules</code> folder</p>\n<pre><code>dist\nnode_modules\n</code></pre>\n<h2>The future of ESM modules</h2>\n<p>There is a shift happening where modules are changing to be pure ESM rather\nthan keeping commonjs equivalents</p>\n<p><a href=\"https://gist.github.com/sindresorhus/a39789f98801d908bbc7ff3ecc99d99c\">https://gist.github.com/sindresorhus/a39789f98801d908bbc7ff3ecc99d99c</a></p>\n<p>There are many challenges here, and will not be discussed, but it may be a\nuseful further reading page</p>\n<h2>Conclusion</h2>\n<p>This tutorial shows you how you can create a basic package that you can publish\nto <code>npm</code>. This little boilerplate includes these features:</p>\n<ul>\n<li>Makes clean build when running <code>yarn build</code> or <code>yarn publish</code></li>\n<li>Pushes to github after publish</li>\n<li>Uses ts-jest for testing</li>\n<li>Uses esm modules</li>\n</ul>\n<p>You also have full control, and understand the decisions we took to get to this\npoint. This package does not use any bundling (rollup or webpack or otherwise).\nIt just uses <code>tsc</code> is used to compile the files to the <code>dist</code> folder, and the\ndist folder is published to <code>npm</code>! If you need your package to be usable by\nconsumers that don't themselves use bundlers, consider looking into <code>&#x3C;script type=\"module\"></code> for importing ESM modules in the browser, or you can bundle\nyour library using rollup or webpack and output e.g. a UMD bundle</p>\n<h2>Final product</h2>\n<p>See <a href=\"https://github.com/cmdcolin/npm-package-tutorial/\">https://github.com/cmdcolin/npm-package-tutorial/</a></p>"},{"title":"My next.js static blog setup","date":"2021-12-26","slug":"2021-12-26-nextjs","html":"<p>TLDR src here <a href=\"https://github.com/cmdcolin/cmdcolin.github.io\">https://github.com/cmdcolin/cmdcolin.github.io</a></p>\n<p>My personal homepage originally used statocles, a perl-based static site\ngenerator (<a href=\"http://preaction.me/statocles/\">http://preaction.me/statocles/</a>). I didn't really blog using it, just\na homepage for myself plus some links to my tumblr blog. But, if I linked\npeople to the tumblr blog directly, it would give people terrible popup ads and\ntrackers. So, I switched to github pages+next.js this year. I considered a\nnumber of alternative static site systems, but next.js seemed to hit some nice\ngoals</p>\n<ul>\n<li>Flexible</li>\n<li>React-based (as opposed to template-based like jekyll, eleventy, etc.)</li>\n<li>Markdown driven, and can use MDX (edit 2022: I removed MDX, I disliked the complicated that it brought. Now all posts are plain markdown, parsed with <code>remark-gfm</code> for github flavored markdown, and all pages are <code>tsx</code>)</li>\n<li>RSS feed (bonus)</li>\n<li>Active community</li>\n</ul>\n<p>Other systems almost worked and were attempted but aborted</p>\n<h3>First and second iterations</h3>\n<p>The first iteration of my next.js blog</p>\n<ul>\n<li>I put every blog post in the \"pages\" folder. This worked ok but I had to\nmanually edit the index.mdx file to have long lists of stuff like this\n<code>![link to new blogpost](manually_inserted_link_here)</code></li>\n</ul>\n<p>The second iteration, I wanted to automatically generate a list of recent\nblogposts from files on disk</p>\n<ul>\n<li>\n<p>I used the next.js \"blog-template-typescript\" example folder from their\n<a href=\"https://github.com/vercel/next.js/tree/canary/examples/blog-starter-typescript\">monorepo</a>.</p>\n</li>\n<li>\n<p>The new blog posts are generated from markdown files in the <code>_posts</code> folder,\nand get rendered by the file <code>pages/posts/[slug].tsx</code> (yes, the filename\nincludes square brackets).</p>\n</li>\n<li>\n<p>getAllPosts in<br>\n<a href=\"https://github.com/cmdcolin/cmdcolin.github.io/blob/master/lib/api.ts\"><code>lib/api.ts</code></a>\ngets a listing of the files in _posts folder, which I can call from the <code>getStaticProps</code> method on next.js pages</p>\n</li>\n</ul>\n<h3>Stripping off unnecessary stuff from blog-starter-typescript</h3>\n<p>The <code>blog-starter-typescript</code> template has many tiny components, I removed some\nof them to make it easier for me to orient myself</p>\n<ul>\n<li><a href=\"https://github.com/vercel/next.js/tree/canary/examples/blog-starter-typescript/components\">theirs</a></li>\n<li><a href=\"https://github.com/cmdcolin/cmdcolin.github.io/tree/master/components\">mine</a></li>\n</ul>\n<h3>Removing tailwind CSS</h3>\n<p>The <code>blog-starter-typescript</code> template uses tailwind CSS and uses \"modern web design\" (aka:\ngigantic \"tiles\" instead of links, images that are way too large, etc)</p>\n<p>I started making a more basic design. I tried to roll with the tailwind CSS for\na bit, but ended up removing it entirely.</p>\n<p>Tailwind CSS is sort of like a CSS-in-JS system, except every CSS attribute is encoded in a CSS classname. For example, here are some tailwind CSS snippets</p>\n<pre><code class=\"language-html\">&#x3C;div className=\"container mx-auto px-5\">&#x3C;/div>\n&#x3C;footer className=\"bg-accent-1 border-t border-accent-2\">&#x3C;/footer>\n&#x3C;div className=\"max-w-1xl mx-auto\">&#x3C;/div>\n&#x3C;div className=\"min-h-screen\">&#x3C;/div>\n&#x3C;a className=\"hover:underline\">&#x3C;/a>\n&#x3C;h1\n  className=\"text-2xl md:text-2xl lg:text-2xl font-bold tracking-tighter leading-tight md:leading-none mb-12 text-center md:text-left\"\n>&#x3C;/h1>\n</code></pre>\n<p>They claim this is better than using external CSS (see comparison here\n<a href=\"https://tailwindcss.com/docs/utility-first\">https://tailwindcss.com/docs/utility-first</a>) but it is yet another language to\nlearn, and kind of tricky.</p>\n<p>But, the reason I gave up with tailwind is actually because tailwind CSS resets\na lot of HTML styles so things like <code>&#x3C;h1></code>, <code>&#x3C;h2></code>, <code>&#x3C;ul></code>, <code>&#x3C;li></code>, <code>&#x3C;a></code> have\nno styling at all. This is done by <code>tailwind preflight</code>\n<a href=\"https://tailwindcss.com/docs/preflight\">https://tailwindcss.com/docs/preflight</a> (which you can disable, but it is\nenabled by default)</p>\n<p>Stackoverflow has some ways to help restore styling and keep preflight, but it\nstill struck me as odd. Examples</p>\n<ul>\n<li>\n<p><a href=\"https://stackoverflow.com/a/68853223/2129219\">Example: you have to manually restore underlines on <code>&#x3C;a></code> elements if using tailwind XSS</a></p>\n</li>\n<li>\n<p><a href=\"https://stackoverflow.com/questions/69264976/cant-display-markdown-on-nextjs\">Another example: \"It looks like you're using TailwindCSS, the default\nstyles for elements are reset, that's why the h1 text will look like any other\ntext.\"</a></p>\n</li>\n<li>\n<p><a href=\"https://raw.githubusercontent.com/vercel/next.js/canary/examples/blog-starter-typescript/components/markdown-styles.module.css\">Another example <code>blog-template-typescript</code> uses this file to try to style\nthe markdown using some general\nstyles</a></p>\n</li>\n</ul>\n<p>To me it was surprising the extend that tailwind goes to unstyle the default\nbrowser styles, removing \"idiomatic HTML\" styles, so I removed tailwind for\nnow. Perhaps I'll return to it another time</p>\n<h2>Using MDX for blogposts in next.js</h2>\n<p>In the template from next.js team, the <code>blog-template-typescript</code>, it uses a\nfairly simple <code>lib/markdownToHtml.ts</code> function right in the\n<code>pages/posts/[slug].tsx</code> file (the markdown is statically pre-rendered in the\ntrue static blog sense, using the getStaticProps function). This is,\nunfortunately, over-simplified for the MDX case, because MDX properly needs to\nhydrate the components using react on the client side also</p>\n<p>To fix, the module <a href=\"https://github.com/hashicorp/next-mdx-remote\">https://github.com/hashicorp/next-mdx-remote</a> offers a way to\nload actual MDX files.</p>\n<h2>Adding syntax highlighting the next.js code snippets</h2>\n<p>There are a couple results from google about how to add syntax highlighting to\nnext.js but I still found it difficult.</p>\n<p>My method ended up a bit different where I manually included the prism JS and\nCSS from a CDN essentially and it worked</p>\n<p><a href=\"https://github.com/cmdcolin/cmdcolin.github.io/blob/aa080193f45cb3e3d11ca1ead2bbd5eb2ae09633/styles/index.css#L14-L15\">https://github.com/cmdcolin/cmdcolin.github.io/blob/aa080193f45cb3e3d11ca1ead2bbd5eb2ae09633/styles/index.css#L14-L15</a></p>\n<p><a href=\"https://github.com/cmdcolin/cmdcolin.github.io/blob/aa080193f45cb3e3d11ca1ead2bbd5eb2ae09633/pages/_document.tsx#L12-L17\">https://github.com/cmdcolin/cmdcolin.github.io/blob/aa080193f45cb3e3d11ca1ead2bbd5eb2ae09633/pages/_document.tsx#L12-L17</a></p>\n<p>Other methods e.g. adding react-prism in next.config.js (like\n<a href=\"https://github.com/mikeesto/next-mdx-prism-example\">https://github.com/mikeesto/next-mdx-prism-example</a> does) I think clashed with\nMDXRemote perhaps, or maybe I was tussling with tailwind CSS too much to make a\nclear thought out of it, but syntax blocks on my blogposts should now be\nproperly highlighted</p>\n<h2>RSS feed</h2>\n<p>I also followed this great guide to add a RSS file for next.js\n<a href=\"https://ashleemboyer.com/how-i-added-an-rss-feed-to-my-nextjs-site\">https://ashleemboyer.com/how-i-added-an-rss-feed-to-my-nextjs-site</a></p>\n<p>Link here, for your feed readers\n<a href=\"https://cmdcolin.github.io/rss.xml\">https://cmdcolin.github.io/rss.xml</a></p>\n<p>Not many people may use RSS much anymore, but I do use it (via feedly), and I\nlove music blogs that keep posting on blogspot year after year, and the\noccasional programming post is nice too</p>"},{"title":"A spooky error when you have a string bigger than 512MB in Chrome","date":"2021-10-30","slug":"2021-10-30-spooky","html":"<p>Now gather round for a spooky story</p>\n<p>Late one night... in the haunted office space castle (hindenbugs cackling in\nthe background amongst the dusty technical books) the midnight candles were\nburning bright and we entered data for a user file</p>\n<p>A simple 52MB gzipped datafile that we want to process in the browser. We unzip\nit, decode it, and ...an error</p>\n<p>ERROR: data not found</p>\n<p><img src=\"/media/pumpkin-dark.jpg\" alt=\"\"></p>\n<p>But... our code is so simple (we of course abide by the religion of writing\n\"simple code\" you know)...what could be happening?</p>\n<p>The code looks like this</p>\n<pre><code class=\"language-js\">const buf = unzip(file)\nconst str = new TextDecoder().decode(buf)\n</code></pre>\n<p>We trace it back and run a console.log(str)</p>\n<p>It looks empty. We try running console.log(str.length) ... it prints out 0</p>\n<p>But if we console.log(buffer.length) we get 546,483,710 bytes...</p>\n<p>What could be happening?</p>\n<p>We see in the TextDecoder documentation that it has a note called \"fatal\". We\ntry</p>\n<pre><code class=\"language-js\">const buf = unzip(file)\nconst str = new TextDecoder('utf8', { fatal: true }).decode(buf)\n</code></pre>\n<p>This doesn't change the results though</p>\n<p>Then it dawns on us while the lightning hits and the thunderclap booms and the\nwind blows through the rattly windows</p>\n<p>We have hit...the maximum string length in Chrome</p>\n<p>BWAHAHAHAHA</p>\n<p>The maximum string length!!! Nooooooo</p>\n<p>It is 512MB on the dot... 536,870,888 bytes. We test this to be sure</p>\n<pre><code class=\"language-js\">const len = 536_870_888\nconst buf = new Uint8Array(len)\nfor (let i = 0; i &#x3C; len; i++) {\n  buf[i] = 'a'.charCodeAt(0)\n}\nconst str = new TextDecoder().decode(buf)\nconsole.log(str.length)\n</code></pre>\n<p>This is correct, outputs 536,870,888</p>\n<p>With anything, even one byte more, it fails and outputs 0</p>\n<p>happy halloween!!</p>\n<p>pumpkin photo source:\n<a href=\"http://mountainbikerak.blogspot.com/2010/11/google-chrome-pumpkin.html\">http://mountainbikerak.blogspot.com/2010/11/google-chrome-pumpkin.html</a></p>\n<p>chrome 95 tested</p>\n<p>nodejs 15 - at 512MB+1 bytes it prints an error message <code>Error: Cannot create a string longer than 0x1fffffe8 characters</code> for significantly greater than 512MB\ne.g. 600MB it actually prints a different error <code>TypeError [ERR_ENCODING_INVALID_ENCODED_DATA]: The encoded data was not valid for encoding utf-8</code>)</p>\n<p>firefox 93 - goes up to ~1GB but then gives Exception <code>{ name: \"NS_ERROR_OUT_OF_MEMORY\", message: \"\", result: 2147942414 }</code></p>\n<p>midori 6 (safari-alike/webkit) - goes up to ~2GB fine! will have to test more</p>"},{"title":"Jest parallelization, globals, mocks, and squawkless tests","date":"2021-10-05","slug":"2021-10-05-jest","html":"<p>I found that there is a little bit of confusion and misunderstanding around how\nthings like parallelization work in jest, which sometimes leads to additional\nhacking around problems that may not exist or speculating incorrectly about\ntest failure. This is also of course a point of concern when you have code that\nfor some reason or another uses global variables. Here are a short summary of\nthings that may cause confusion.</p>\n<h2>Tests in a single file are NOT run in parallel</h2>\n<p>Simple example, the global variable r is included in the test condition, but it\nis accurately run in all cases because the tests are not run in parallel.</p>\n<pre><code class=\"language-js\">let r = 0\n\nfunction timeout(ms) {\n  return new Promise(resolve => setTimeout(resolve, ms))\n}\n\ndescribe('tests', () => {\n  it('t1', async () => {\n    await timeout(1000)\n    expect(r).toBe(0)\n    r++\n  })\n  it('t2', async () => {\n    await timeout(1000)\n    expect(r).toBe(1)\n    r++\n  })\n  it('t3', async () => {\n    await timeout(1000)\n    expect(r).toBe(2)\n    r++\n  })\n})\n</code></pre>\n<p>This test will take 3 seconds, and will accurately count the global variable.\nIf it was in parallel, it may only take 1 second, and would inaccurately count\nthe global variable due to race conditions</p>\n<h2>Tests in different files ARE run in parallel</h2>\n<p>Let's take another example where we use a global variable, and then two\ndifferent tests use the global variable.</p>\n<p>file_using_some_globals.js</p>\n<pre><code class=\"language-js\">let myGlobal = 0\n\nexport function doStuff() {\n  myGlobal++\n  return myGlobal\n}\n\nexport function resetMyGlobal() {\n  myGlobal = 0\n}\n\nexport function timeout(ms) {\n  return new Promise(resolve => setTimeout(resolve, ms))\n}\n</code></pre>\n<p>test_global_vars1.test.js</p>\n<pre><code class=\"language-js\">import { doStuff, timeout } from './dostuff'\ntest('file1', async () => {\n  doStuff()\n  await timeout(1000)\n  expect(doStuff()).toEqual(2)\n})\n</code></pre>\n<p>test_global_vars2.test.js</p>\n<pre><code class=\"language-js\">import { doStuff, timeout } from './dostuff'\n\ntest('file1', async () => {\n  await timeout(1000)\n  expect(doStuff()).toEqual(1)\n})\n</code></pre>\n<p>This test completes in less than 2 seconds, and these tests are run in\nparallel. They use different instances of the global state, and therefore have\nno worries with colliding their state.</p>\n<h2>Does a mock from one test affect another test?</h2>\n<p>While seeking the fabled \"squawk-less\" test, it is often useful to mock console\nso that tests that produce an expected error don't actually print an error\nmessage. However, if not done carefully, you will remove errors across tests</p>\n<p>So, could a mock from one test affect another test? If it's in the same file,\nyes!</p>\n<p>mock_console.test.js</p>\n<pre><code class=\"language-js\">test('test1', () => {\n  console.error = jest.fn()\n  console.error('wow')\n  expect(console.error).toHaveBeenCalled()\n})\n\ntest('test2', () => {\n  // this console.error will not appear because test1 mocked away console.error\n  // without restoring it\n  console.error(\"Help I can't see!\")\n})\n</code></pre>\n<p>To properly mock these, you should restore the console mock at the end of your\nfunction</p>\n<pre><code class=\"language-js\">test('test1', () => {\n  const orig = console.error\n  console.error = jest.fn()\n  console.error('I should not see this!')\n  expect(console.error).toHaveBeenCalled()\n  console.error = orig\n})\n\ntest('test2', () => {\n  const consoleMock = jest.spyOn(console, 'error').mockImplementation()\n  console.error('I should not see this!')\n  consoleMock.mockRestore()\n})\n\ntest('test3', () => {\n  console.error('I should see this error!')\n})\n</code></pre>\n<h2>Add-on: Achieve squawkless tests!</h2>\n<p>Your test output should just be a big list of PASS statements, not interleaved\nwith console.error outputs from when you are testing error conditions of your\ncode</p>\n<p>\"Squawkless tests\" is a term I made up, but it means that if you have code\nunder test that prints some errors to the console, then mock the console.error\nfunction, as in the previous section. Don't stand for having a bunch of verbose\nerrors in your CI logs! However, I also suggest only mocking out console.error\nfor tests that are <strong>expected</strong> to have errors, lest you paper over unexpected\nerrors.</p>\n<p><img src=\"/media/squawkless_tests.png\" alt=\"\"></p>\n<p>Figure: a nice clean test suite without a bunch of crazy console.error outputs</p>\n<h2>Conclusion</h2>\n<p>Getting better at testing requires exercise, and understanding the basics of\nyour tools can help! Hopefully this helps you achieve a better understanding\nand write cleaner jest tests.</p>"},{"title":"Decrease your idle CPU usage when developing typescript apps with this one weird environment variable","date":"2021-09-05","slug":"2021-09-05-typescript","html":"<p>TL;DR:</p>\n<p>add this to your bashrc</p>\n<pre><code>export TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling\n</code></pre>\n<p>By default, the typescript watcher configuration e.g. tsc --watch or whatever\nis run internally to a create-react-app typescript app (I see it in the process\nmanager as fork-ts-checker-webpack-plugin cpu usage) can have high idling\n(doing nothing...) CPU usage</p>\n<p>This is because the default configuration polls for file changes (constantly\nasks the computer if there are changes every 250ms or so). There is an\nalternative configuration for this to change it to a file watcher so it\nreceives file system notifications on file change. There is discussion here on\nthis.</p>\n<p>The main summary is that a env variable set to\nTSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling allows this</p>\n<p><a href=\"https://github.com/microsoft/TypeScript/issues/31048\">https://github.com/microsoft/TypeScript/issues/31048</a></p>\n<p>The issue thread shows that it can go from roughly ~7% idle CPU usage to 0.2%.\nThis corresponds with what I see too after applying this! Detailed docs for\ntypescript discuss some of the reasoning behing not making this the default</p>\n<p><a href=\"https://github.com/microsoft/TypeScript-Handbook/blob/master/pages/Configuring%20Watch.md#background\">https://github.com/microsoft/TypeScript-Handbook/blob/master/pages/Configuring%20Watch.md#background</a></p>\n<p>It claims that some OS specific behaviors of file watching could be harmful to\nmaking it the default. For example, that (maybe?) on linux, it may use a large\nnumber of file watchers which can exceed notify handles (this is a setting I\ncommonly have to increase in linux, guide here\n<a href=\"https://dev.to/rubiin/ubuntu-increase-inotify-watcher-file-watch-limit-kf4\">https://dev.to/rubiin/ubuntu-increase-inotify-watcher-file-watch-limit-kf4</a>)</p>\n<p>PS: if you have a package.json of a <code>create-react-app --template typescript</code> or\nsomething like this then you can edit the package.json to apply this\nautomatically</p>\n<pre><code>-\"start\": \"react-scripts start\"\n+\"start\": \"cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling react-scripts start\"\n</code></pre>\n<p>Phew. I can already feel my laptop running cooler...or at least I can sleep\nmore soundly knowing that my readers adopt this and save some CPU cycles for\nplanet earth...and hopefully don't run into any of the caveats</p>\n<p>Edit: It may be worth it to note, the 'UseFsEvents' part of this uses the\nnode.js fs.watch API and the polling based API is based on fs.watchFile</p>\n<p>Fun table of how the watchers are implemented on different OSs\n[<a href=\"https://github.com/microsoft/TypeScript/issues/31048#issuecomment-495483957\">1</a>]</p>\n<pre><code>On Linux systems, this uses inotify(7).\nOn BSD systems, this uses kqueue(2).\nOn macOS, this uses kqueue(2) for files and FSEvents for directories.\nOn SunOS systems (including Solaris and SmartOS), this uses event ports.\nOn Windows systems, this feature depends on ReadDirectoryChangesW.\nOn Aix systems, this feature depends on AHAFS, which must be enabled.\n</code></pre>\n<p>And in general, these should all respond more or less the same, but there are\nsmall corner cases that are discussed\n<a href=\"https://nodejs.org/docs/latest/api/fs.html#fs_availability\">https://nodejs.org/docs/latest/api/fs.html#fs_availability</a></p>\n<p>Disclaimer: it may be worth reading the reasons that typescript does not have\nthis enabled by default before pushing this into your dev environment and all\nyour teammates, but as far as I could tell, it seems ok!</p>"},{"title":"An amazing error message if you put more than 2^24 items in a JS Map object","date":"2021-08-15","slug":"2021-08-15-map-limit","html":"<p>One of the fun things about working with big data is that you can often hit\nweird limits with a system.</p>\n<p>I was personally trying to load every 'common' single nucleotide polymorphism\nfor the human genome into memory (dbSNP), of which there are over 37 million\nentries (there are many more uncommon ones) for the purposes of making a custom\nsearch index for them [1].</p>\n<p>Turns out, you may run into some hard limits. Note that these are all V8-isms\nand may not apply to all browsers or engines (I was using node.js for this)</p>\n<pre><code class=\"language-js\">const myObject = new Map()\nfor (let i = 0; i &#x3C;= 50_000_000; i++) {\n  myObject.set(i, i)\n  if (i % 100000 == 0) {\n    console.log(i)\n  }\n}\n</code></pre>\n<p>This will crash after adding approx 16.7M elements and say</p>\n<pre><code>0\n100000\n200000\n...\n16400000\n16500000\n16600000\n16700000\n\nUncaught RangeError: Value undefined out of range for undefined options\nproperty undefined\n</code></pre>\n<p>That is a very weird error message. It says \"undefined\" three times! Much\nbetter than your usual <code>TypeError: Can’t find property ‘lol’ of undefined</code>. See\n<a href=\"https://bugs.chromium.org/p/v8/issues/detail?id=11852\">https://bugs.chromium.org/p/v8/issues/detail?id=11852</a>\nfor a bug filed to help improve the error message perhaps.</p>\n<p>Now, also interestingly enough, if you use an Object instead of a Map</p>\n<pre><code class=\"language-js\">const myObject = {};\nfor (let i = 0; i &#x3C;= 50_000_000; i++) {\n  myObject['myobj_’+i]=i;\n  if(i%100000==0) { console.log(i) }\n}\n</code></pre>\n<p>Then it will print...</p>\n<pre><code>0\n100000\n200000\n...\n8000000\n8100000\n8200000\n8300000\n</code></pre>\n<p>And it will actually just hang there...frozen...no error message though! And it\nis failing at ~8.3M elements. Weird right? This is roughly half the amount of\nelements as the 16.7M case</p>\n<p>Turns out there is a precise hard limit for the Map case</p>\n<p>For the Map: 2^24=16,777,216</p>\n<p>For the Object it is around 2^23=8,388,608 HOWEVER, I can actually add more\nthan this, e.g. I can add 8,388,609 or 8,388,610 or even more, but the\noperations start taking forever to run, e.g. 8,388,999 was taking many minutes</p>\n<p>Very weird stuff! If you expected me to dig into this and explain it in deep\ntechnical detail, well, you’d be wrong. However, this helpful post on\nstackoverflow by a V8 js engine developer clarifies the Map case!!\n<a href=\"https://stackoverflow.com/questions/54452896/maximum-number-of-entries-in-node-js-map\">https://stackoverflow.com/questions/54452896/maximum-number-of-entries-in-node-js-map</a></p>\n<pre><code>V8 developer here. I can confirm that 2^24 is the maximum number of entries in\na Map. That’s not a bug, it’s just the implementation-defined limit.\n\nThe limit is determined by:\n\nThe FixedArray backing store of the Map has a maximum size of 1GB (independent\nof the overall heap size limit) On a 64-bit system that means 1GB / 8B = 2^30 /\n2^3 = 2^27 ~= 134M maximum elements per FixedArray A Map needs 3 elements per\nentry (key, value, next bucket link), and has a maximum load factor of 50% (to\navoid the slowdown caused by many bucket collisions), and its capacity must be\na power of 2. 2^27 / (3 * 2) rounded down to the next power of 2 is 2^24, which\nis the limit you observe.  FWIW, there are limits to everything: besides the\nmaximum heap size, there’s a maximum String length, a maximum Array length, a\nmaximum ArrayBuffer length, a maximum BigInt size, a maximum stack size, etc.\nAny one of those limits is potentially debatable, and sometimes it makes sense\nto raise them, but the limits as such will remain. Off the top of my head I\ndon’t know what it would take to bump this particular limit by, say, a factor\nof two – and I also don’t know whether a factor of two would be enough to\nsatisfy your expectations.\n\n</code></pre>\n<p>Great details there. It would also be good to know what the behavior is for the\nObject, which has those 100% CPU stalls after ~8.3M, but not the same error\nmessage...</p>\n<p>Another fun note: if I modify the Object code to use only “integer IDs” the\ncode actually works fine, does not hit any errors, and is “blazingly fast” as\nthe kids call it</p>\n<pre><code class=\"language-js\">const myObject = {}\nfor (let i = 0; i &#x3C;= 50_000_000; i++) {\n  myObject[i] = i\n  if (i % 100000 == 0) {\n    console.log(i)\n  }\n}\n</code></pre>\n<p>I presume that this code works because it detects that I’m using it like an\narray and it decides to transform how it is working internally and not use a\nhash-map-style data structure, so does not hit a limit. There is a slightly\nhigher limit though, e.g. 1 billion elements gives “Uncaught RangeError:\nInvalid array length”</p>\n<pre><code class=\"language-js\">const myObject = {}\nfor (let i = 0; i &#x3C;= 1_000_000_000; i++) {\n  myObject[i] = i\n  if (i % 100000 == 0) {\n    console.log(i)\n  }\n}\n</code></pre>\n<p>This has been another episode of ....the twilight zone (other episodes\ncatalogued here) <a href=\"https://github.com/cmdcolin/technical_oddities/\">https://github.com/cmdcolin/technical_oddities/</a></p>\n<p>[1] The final product of this adventure was this, to create a search index for\na large number of elements <a href=\"https://github.com/GMOD/ixixx-js\">https://github.com/GMOD/ixixx-js</a></p>"},{"title":"Do you understand your NPM dependencies?","date":"2021-07-27","slug":"2021-07-27-npm-dependencies","html":"<p>You are writing a library...or you are writing an app and you want to publish\nsome of the components of it as a library...</p>\n<p>Here are some questions in the form of comments</p>\n<ul>\n<li>\n<p>Did you realize that your yarn.lock will be ignored for anyone who installs\nyour libraries?</p>\n</li>\n<li>\n<p>Did you realize this means that your perfectly running test suite with your\nyarn.lock could be a failing case for consumers of your app unless you don’t\nuse semver strings like ^1.0.0 and just hardcode it to 1.0.0?</p>\n</li>\n<li>\n<p>Did you realize the default of ^1.0.0 automatically gets minor version bumps\nwhich are often fairly substantial changes, e.g. even breaking possibly?</p>\n</li>\n<li>\n<p>Did you know that larger libraries like @material-ui/core don’t like to bump\ntheir major version all the time for example so large changes are often made\nto the minor version?</p>\n</li>\n<li>\n<p>Did you know if you run <code>yarn upgrade</code>, it may update what is in your\nyarn.lock file but will not update what is in your package.json?</p>\n</li>\n<li>\n<p>Did you realize that this means that if you depend on the results of running\n<code>yarn upgrade</code> e.g. it gave you a bugfix, you could be shipping buggy code to\nconsumers of your library?</p>\n</li>\n</ul>\n<p>Just something to be aware of! You can always ride the dragon and accept these\nminor breakages from semver bumps, but it can introduce some issues for your\nconsumers</p>\n<p>Random fun thing: Adding a yarn package can even downgrade some other packages.\nFor example if you have ^6.0.0 in your package.json, you yarn upgrade it so in\nthe lockfile it says 6.1.0 but then later install another library that requires\na hard 6.0.1, yarn will decide to downgrade you to 6.0.1 (it will not have a\nduplicate entry in yarn.lock, just that the 6.1.0 in the yarn.lock will\ndowngrade to 6.0.1)</p>"},{"title":"Making a HTTPS accessible S3 powered static site with CloudFront+route 53","date":"2020-12-26","slug":"2020-12-26-pt2","html":"<p>This is not a very authoritative post because I stumbled though this but\nI think I got it working now on my website :)</p>\n<h2>Setup your S3 bucket</h2>\n<p>First setup your S3 bucket, your bucket must be named yourdomain.com\ne.g. named after your domain</p>\n<p>Then if you have a create-react-app setup I add a script in package.json\nthat runs</p>\n<pre><code> \"predeploy\": \"npm run build\",\n \"deploy\": \"aws sync --delete build s3://yourdomain.com\"\n</code></pre>\n<p>Then we can run \"yarn deploy\" and it will automatically upload our\ncreate-react-app website to our S3 static site bucket.</p>\n<p>Then make sure your bucket has public permissions enabled\n<a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2</a>.\nThen make sure your bucket has \"static site hosting\" enabled too</p>\n<h2>Setup route 53, and make your NS entries in domains.google.com</h2>\n<p>I bought a domain with domains.google.com</p>\n<p>Google then emailed me to validate my ownership</p>\n<p>Then I went to aws.amazon.com route 53 and I created a hosted zone</p>\n<p>This generated 4 name server entries and I added those to the\ndomains.google.com site</p>\n<p><img src=\"/media/638618421776515072_0.png\" alt=\"\"></p>\n<p>Screenshot shows copying the NS values from route 53 to the name servers\narea of domains.google.com</p>\n<h2>Setup your Amazon certificate for making SSL work on CloudFront</h2>\n<p>To properly setup However, this does not work so you need to go to\nAmazon Certificates->Provision certificates</p>\n<p>We request the certificate for</p>\n<p><a href=\"http://www.yourdomain.com\">www.yourdomain.com</a>\nyourdomain.com</p>\n<p>Then it generates some codes for a CNAME value for each of those two\nentries, and has a button to autoimport those CNAME values to route53</p>\n<p>Then it will say \"Pending validation\"...I waited like an hour and then\nit changed to \"Success\".</p>\n<p><img src=\"/media/638618421776515072_1.png\" alt=\"\"></p>\n<p>Screenshot shows the now successful Amazon Certificate. After you get\nthis, you can proceed to finishing your cloudfront</p>\n<h2>Create a CloudFront distribution and add \"Alternative CNAME\" entries for your domain</h2>\n<p>Then we can update our CloudFront distribution and add these to\nthe \"Alternative CNAME\" input box</p>\n<p>yourdomain.com\n<a href=\"http://www.yourdomain.com\">www.yourdomain.com</a></p>\n<p>Note also that I first generated my certificate in us-east-2 but the\n\"Import certificate form\" in cloudfront said I had to create it in\nus-east-1</p>\n<p><img src=\"/media/638618421776515072_2.png\" alt=\"\"></p>\n<h2>Add a default object index.html to the CloudFront setting</h2>\n<p>Make your CloudFront \"default object\" is index.html</p>\n<p>You have to manually type this in :)</p>\n<h2>Add the CloudFront distribution to your Route 53</h2>\n<p>Add a Route 53 \"A\" record that points to the CloudFront domain name e.g.\nd897d897d87d98dd.cloudfront.net</p>\n<h2>Summary of steps needed</h2>\n<p>The general hindsight 20/20 procedure is</p>\n<ol>\n<li>Upload your static content to an S3 bucket called yoursite.com (must\nbe your domain name)</li>\n<li>Make your S3 bucket have the \"static website\" setting on in the\nproperties menu and add a permissions policy that supports getObject\ne.g. <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2</a></li>\n<li>Create a CloudFront distribution for your website</li>\n<li>Make the CloudFront default object index.html</li>\n<li>Create your domain with domains.google.com or similar</li>\n<li>Point the google domain's name server to Route 53 NS list from AWS</li>\n<li>Add Route 53 A records that point to the CloudFront domain name e.g.\nd897d897d87d98dd.cloudfront.net</li>\n<li>Create Amazon issued certificate for yourdomain.com, which can\nauto-import a validation CNAME to your Route 53</li>\n<li>Make your CloudFront domain support your Alternative CNAME's e.g.\nyourdomain.com which requires importing (e.g. selecting from a list\nthat they auto-populate) your Amazon-issued-certificate</li>\n</ol>\n<h2>Troubleshooting and notes</h2>\n<p>Problem: Your website gives 403 CloudFlare error\nSolution: You have to get the Alternateive CNAME configuration setup\n(pre-step involves the certificate request and validation)</p>\n<p>Problem: Your website gives an object not found error\nSolution: Set the CloudFront \"default object\" to index.html</p>\n<h2>Random comment</h2>\n<p>This is one of those processes (creating the cloudfront/route 53) that\nprobably could have done with the aws-sam CLI and it would have possibly\nbeen easier, it is quite fiddly doing all these steps in the web\ninterface</p>"},{"title":"Making a serverless website for photo and video upload pt. 2","date":"2020-12-26","slug":"2020-12-26","html":"<p>This post follows\non <a href=\"https://cmdcolin.github.io/2020-12-24.html\">https://cmdcolin.github.io/2020-12-24.html</a></p>\n<p>It is possible I zoomed ahead too fast to make this a continuous tutorial, but\noverall I just wanted to post an update</p>\n<p>In pt. 1 I learned how to use the <code>aws-sam</code> CLI tool. This was a great insight\nfor me about automating deployments. I can now simply run <code>sam deploy</code> and it\nwill create new dynamodb tables, lambda functions, etc.</p>\n<p>After writing pt 1. I converted the existing vue-js app that was in the aws\ntutorial and converted it to react. Then I extended the app to allow</p>\n<ul>\n<li>Posting comments on photos</li>\n<li>Uploading multiple files</li>\n<li>Uploading videos etc.</li>\n</ul>\n<p>It will be hard to summarize all the changes since now the app has taken off a\nlittle bit but it looks like this:</p>\n<p>Repo structure</p>\n<pre><code> ./frontend # created using npx create-react-app frontend --template\n typescript\n ./frontend/src/App.tsx # main frontend app code in react\n ./lambdas/\n ./lambdas/postFile # post a file to the lambda, this uploads a row to\n dynamodb and returns a pre-signed URL for uploading (note that if the\n client failed it's upload, that row in the lambda DB might be in a bad\n state...)\n ./lambdas/getFiles # get all files that were ever posted\n ./lambdas/postComment # post a comment on a picture with POST\n request\n ./lambdas/getComments?file=filename.jpg # get comments on a\n picture/video with GET request\n</code></pre>\n<p>Here is a detailed code for uploading the file. We upload one file at a\ntime, but the client code post to the lambda endpoint individually for\neach file</p>\n<p>This generates a pre-signed URL to allow the client-side JS (not the\nlambda itself) to directly upload to S3, and also posts a row in the S3\nto the filename that will. It is very similar code in\nto <a href=\"https://cmdcolin.github.io/2020-12-24.html\">https://cmdcolin.github.io/2020-12-24.html</a></p>\n<p>./lambdas/postFile/app.js</p>\n<pre><code class=\"language-js\">'use strict'\n\nconst AWS = require('aws-sdk')\nconst multipart = require('./multipart')\nAWS.config.update({ region: process.env.AWS_REGION })\nconst s3 = new AWS.S3()\n\n// Change this value to adjust the signed URL's expiration\nconst URL_EXPIRATION_SECONDS = 300\n\n// Main Lambda entry point\nexports.handler = async event => {\n  return await getUploadURL(event)\n}\n\nconst { AWS_REGION: region } = process.env\n\nconst dynamodb = new AWS.DynamoDB({ apiVersion: '2012-08-10', region })\n\nasync function uploadPic({\n  timestamp,\n  filename,\n  message,\n  user,\n  date,\n  contentType,\n}) {\n  const params = {\n    Item: {\n      timestamp: {\n        N: `${timestamp}`,\n      },\n      filename: {\n        S: filename,\n      },\n      message: {\n        S: message,\n      },\n      user: {\n        S: user,\n      },\n      date: {\n        S: date,\n      },\n      contentType: {\n        S: contentType,\n      },\n    },\n    TableName: 'files',\n  }\n  return dynamodb.putItem(params).promise()\n}\n\nconst getUploadURL = async function (event) {\n  try {\n    const data = multipart.parse(event)\n    const { filename, contentType, user, message, date } = data\n    const timestamp = +Date.now()\n    const Key = `${timestamp}-${filename}` // Get signed URL from S3\n\n    const s3Params = {\n      Bucket: process.env.UploadBucket,\n      Key,\n      Expires: URL_EXPIRATION_SECONDS,\n      ContentType: contentType, // This ACL makes the uploaded object publicly readable. You must also uncomment // the extra permission for the Lambda function in the SAM template.\n\n      ACL: 'public-read',\n    }\n\n    const uploadURL = await s3.getSignedUrlPromise('putObject', s3Params)\n\n    await uploadPic({\n      timestamp,\n      filename: Key,\n      message,\n      user,\n      date,\n      contentType,\n    })\n\n    return JSON.stringify({\n      uploadURL,\n      Key,\n    })\n  } catch (e) {\n    const response = {\n      statusCode: 500,\n      body: JSON.stringify({ message: `${e}` }),\n    }\n    return response\n  }\n}\n</code></pre>\n<p>./lambdas/getFiles/app.js</p>\n<pre><code class=\"language-js\">// eslint-disable-next-line import/no-unresolved\nconst AWS = require('aws-sdk')\n\nconst { AWS_REGION: region } = process.env\n\nconst docClient = new AWS.DynamoDB.DocumentClient()\n\nconst getItems = function () {\n  const params = {\n    TableName: 'files',\n  }\n\n  return docClient.scan(params).promise()\n}\n\nexports.handler = async event => {\n  try {\n    const result = await getItems()\n    return {\n      statusCode: 200,\n      body: JSON.stringify(result),\n    }\n  } catch (e) {\n    return {\n      statusCode: 400,\n      body: JSON.stringify({ message: `${e}` }),\n    }\n  }\n}\n</code></pre>\n<p>./frontend/src/App.tsx (excerpt)</p>\n<pre><code class=\"language-tsx\">async function myfetch(params: string, opts?: any) {\n  const response = await fetch(params, opts)\n  if (!response.ok) {\n    throw new Error(`HTTP ${response.status}\n ${response.statusText}`)\n  }\n  return response.json()\n}\n\nfunction UploadDialog({\n  open,\n  onClose,\n}: {\n  open: boolean\n  onClose: () => void\n}) {\n  const [images, setImages] = useState&#x3C;FileList>()\n  const [error, setError] = useState&#x3C;Error>()\n  const [loading, setLoading] = useState(false)\n  const [total, setTotal] = useState(0)\n  const [completed, setCompleted] = useState(0)\n  const [user, setUser] = useState('')\n  const [message, setMessage] = useState('')\n  const classes = useStyles()\n\n  const handleClose = () => {\n    setError(undefined)\n    setLoading(false)\n    setImages(undefined)\n    setCompleted(0)\n    setTotal(0)\n    setMessage('')\n    onClose()\n  }\n\n  return (\n    &#x3C;Dialog onClose={handleClose} open={open}>\n           &#x3C;DialogTitle>upload a file (supports picture or video)&#x3C;/DialogTitle> \n         \n      &#x3C;DialogContent>\n               &#x3C;label htmlFor=\"user\">name (optional) &#x3C;/label>\n               &#x3C;input\n          type=\"text\"\n          value={user}\n          onChange={event => setUser(event.target.value)}\n          id=\"user\"\n        />\n               &#x3C;br />       &#x3C;label htmlFor=\"user\">message (optional) &#x3C;/label>\n               \n        &#x3C;input\n          type=\"text\"\n          value={message}\n          onChange={event => setMessage(event.target.value)}\n          id=\"message\"\n        />\n               &#x3C;br />\n               \n        &#x3C;input\n          multiple\n          type=\"file\"\n          onChange={e => {\n            let files = e.target.files\n            if (files &#x26;&#x26; files.length) {\n              setImages(files)\n            }\n          }}\n        />       {error ? (\n          &#x3C;div className={classes.error}>{`${error}`}&#x3C;/div>\n        ) : loading ? (\n          `Uploading...${completed}/${total}`\n        ) : completed ? (\n          &#x3C;h2>Uploaded &#x3C;/h2>\n        ) : null}       &#x3C;DialogActions>\n                   \n          &#x3C;Button\n            style={{ textTransform: 'none' }}\n            onClick={async () => {\n              try {\n                if (images) {\n                  setLoading(true)\n                  setError(undefined)\n                  setCompleted(0)\n                  setTotal(images.length)\n                  await Promise.all(\n                    Array.from(images).map(async image => {\n                      const data = new FormData()\n                      data.append('message', message)\n                      data.append('user', user)\n                      data.append('date', new Date().toLocaleString())\n                      data.append('filename', image.name)\n                      data.append('contentType', image.type)\n                      const res = await myfetch(API_ENDPOINT + '/postFile', {\n                        method: 'POST',\n                        body: data,\n                      })\n\n                      await myfetch(res.uploadURL, {\n                        method: 'PUT',\n                        body: image,\n                      })\n\n                      setCompleted(completed => completed + 1)\n                    }),\n                  )\n                  setTimeout(() => {\n                    handleClose()\n                  }, 500)\n                }\n              } catch (e) {\n                setError(e)\n              }\n            }}\n            color=\"primary\"\n          >\n                       upload          \n          &#x3C;/Button>\n                   &#x3C;Button\n            onClick={handleClose}\n            color=\"primary\"\n            style={{ textTransform: 'none' }}\n          >\n                       cancel          \n          &#x3C;/Button>       \n        &#x3C;/DialogActions>     \n      &#x3C;/DialogContent>\n         \n    &#x3C;/Dialog>\n  )\n}\n</code></pre>\n<p>template.yaml for AWS</p>\n<pre><code class=\"language-yaml\"> AWSTemplateFormatVersion: 2010-09-09\n Transform: AWS::Serverless-2016-10-31\n Description: S3 Uploader\n\n Resources:\n  filesDynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      AttributeDefinitions:\n        - AttributeName: \"timestamp\"\n          AttributeType: \"N\"\n      KeySchema:\n        - AttributeName: \"timestamp\"\n          KeyType: \"HASH\"\n      ProvisionedThroughput:\n        ReadCapacityUnits: \"5\"\n        WriteCapacityUnits: \"5\"\n      TableName: \"files\"\n\n  # HTTP API\n  MyApi:\n    Type: AWS::Serverless::HttpApi\n    Properties:\n      # CORS configuration - this is open for development only and\n should be restricted in prod.\n      # See\n &#x3C;https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-property-httpapi-httpapicorsconfiguration.html>\n      CorsConfiguration:\n        AllowMethods:\n          - GET\n          - POST\n          - DELETE\n          - OPTIONS\n        AllowHeaders:\n          - \"*\"\n        AllowOrigins:\n          - \"*\"\n\n  UploadRequestFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: lambdas/postFile/\n      Handler: app.handler\n      Runtime: nodejs12.x\n      Timeout: 3\n      MemorySize: 128\n      Environment:\n        Variables:\n          UploadBucket: !Ref S3UploadBucket\n      Policies:\n        - AmazonDynamoDBFullAccess\n        - S3WritePolicy:\n            BucketName: !Ref S3UploadBucket\n        - Statement:\n            - Effect: Allow\n              Resource: !Sub \"arn:aws:s3:::${S3UploadBucket}/\"\n              Action:\n                - s3:putObjectAcl\n      Events:\n        UploadAssetAPI:\n          Type: HttpApi\n          Properties:\n            Path: /postFile\n            Method: post\n            ApiId: !Ref MyApi\n\n\n  FileReadFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: lambdas/getFiles/\n      Handler: app.handler\n      Runtime: nodejs12.x\n      Timeout: 3\n      MemorySize: 128\n      Policies:\n        - AmazonDynamoDBFullAccess\n      Events:\n        UploadAssetAPI:\n          Type: HttpApi\n          Properties:\n            Path: /getFiles\n            Method: get\n            ApiId: !Ref MyApi\n\n  ## S3 bucket\n  S3UploadBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      CorsConfiguration:\n        CorsRules:\n          - AllowedHeaders:\n              - \"*\"\n            AllowedMethods:\n              - GET\n              - PUT\n              - HEAD\n            AllowedOrigins:\n              - \"*\"\n\n\n ## Take a note of the outputs for deploying the workflow templates\n in this sample application\n Outputs:\n  APIendpoint:\n    Description: \"HTTP API endpoint URL\"\n    Value: !Sub\n \"https://${MyApi}.execute-api.${AWS::Region}.amazonaws.com\"\n  S3UploadBucketName:\n    Description: \"S3 bucket for application uploads\"\n    Value: !Ref \"S3UploadBucket\"\n\n</code></pre>\n<p>To display all the pictures I use a switch from video or img tag based\non contentType.startsWith('video'). I also use the \"figcaption\" HTML tag\nto have a little caption on the pics/videos</p>\n<p>./frontend/src/App.tsx</p>\n<pre><code class=\"language-tsx\">function Media({\n  file,\n  style,\n  onClick,\n  children,\n}: {\n  file: File\n  onClick?: Function\n  style?: React.CSSProperties\n  children?: React.ReactNode\n}) {\n  const { filename, contentType } = file\n  const src = `${BUCKET}/${filename}`\n  return (\n    &#x3C;figure style={{ display: 'inline-block' }}>\n           \n      &#x3C;picture>\n               \n        {contentType.startsWith('video') ? (\n          &#x3C;video style={style} src={src} controls onClick={onClick as any} />\n        ) : (\n          &#x3C;img style={style} src={src} onClick={onClick as any} />\n        )}\n             \n      &#x3C;/picture>\n           &#x3C;figcaption>{children}&#x3C;/figcaption>   \n    &#x3C;/figure>\n  )\n}\n</code></pre>\n<p>Now the really fun part: if you get an image of a picture frame\nlike <a href=\"https://www.amazon.com/Paintings-Frames-Antique-Shatterproof-Osafs2-Gld-A3/dp/B06XNQ8W9T\">https://www.amazon.com/Paintings-Frames-Antique-Shatterproof-Osafs2-Gld-A3/dp/B06XNQ8W9T</a></p>\n<p>You can make it a border for any image or video using border-image CSS</p>\n<pre><code class=\"language-js\">style = {\n  border: '30px solid',\n  borderImage: `url(borders/${border}) 30 round`,\n}\n</code></pre>\n<p><img src=\"/media/638602799897329664_0.png\" alt=\"\"></p>\n<p>Summary</p>\n<p>The template.yaml automatically deploys the lambdas for postFile/getFile\nand the files table in dynamoDB</p>\n<p>The React app uses postFile for each file in an <code>&#x3C;input type=\"file\"/></code>,\nthe code uses React hooks and functional components but is hopefully not\ntoo complex</p>\n<p>I also added commenting on photos. The code is not shown here but you\ncan look in the source code for details</p>\n<p><img src=\"/media/638602799897329664_1.png\" alt=\"\"></p>\n<p>Overall this has been a good experience learning to develop this app and\nlearning to automate the cloud deployment is really good for ensuring\nreliability and fast iteration.</p>\n<p>Also quick note on serverless CLI vs aws-sam. I had tried a serverless\nCLI tutorial from another user but it didn't click with me, while the\naws-sam tutorial from\n<a href=\"https://searchvoidstar.tumblr.com/post/638408397901987840/making-a-serverless-website-for-photo-upload-pt-1%C2%A0was\">https://searchvoidstar.tumblr.com/post/638408397901987840/making-a-serverless-website-for-photo-upload-pt-1 was</a>\na great kick start for me. I am sure the serverless CLI is great too and\nit ensures a bit less vendor lock in, but then is also a little bit\nremoved from the native aws config schemas. Probably fine though</p>\n<p>Source code <a href=\"https://github.com/cmdcolin/aws_photo_gallery/\">https://github.com/cmdcolin/aws_photo_gallery/</a></p>"},{"title":"Making a serverless website for photo upload pt. 1","date":"2020-12-24","slug":"2020-12-24","html":"<p>I set out to make a serverless website for photo uploads. Our dearly\ndeparted dixie dog needed a place to have photo uploads.</p>\n<p>I didn't want to get charged dollars per month for a running ec2\ninstance, so I wanted something that was lightweight e.g. serverless,\nand easy</p>\n<p>I decided to follow this tutorial</p>\n<p><a href=\"https://aws.amazon.com/blogs/compute/uploading-to-amazon-s3-directly-from-a-web-or-mobile-application/\">https://aws.amazon.com/blogs/compute/uploading-to-amazon-s3-directly-from-a-web-or-mobile-application/</a></p>\n<p>I really liked the command line deployment (aws-sam) because fiddling\naround with the AWS web based control panel is ridiculously complicated</p>\n<p>For example I also tried following this tutorial which uses the web\nbased UI (<a href=\"https://www.youtube.com/watch?v=mw_-0iCVpUc\">https://www.youtube.com/watch?v=mw_-0iCVpUc</a>) and it just did\nnot work for me....I couldn't stay focused (blame ADHD or just my CLI\nobsession?) and certain things like \"Execution role\" that they say to\nmodify are not there in the web UI anymore, so I just gave up (I did try\nthough!)</p>\n<p>To install aws-sam I used homebrew</p>\n<pre><code> brew tap aws/tap\n brew install aws-sam-cli\n brew install aws-sam-cli # I had to run the install command twice ref https://github.com/aws/aws-sam-cli/issues/2320#issuecomment-721414971\n\n git clone https://github.com/aws-samples/amazon-s3-presigned-urls-aws-sam\n cd amazon-s3-presigned-urls-aws-sam\n sam deploy --guided\n\n # proceeeds with a guided installation, I used all defaults except I\n made \"UploadRequestFunction may not have authorization defined, Is\n this okay? [y/N]: y\"\n</code></pre>\n<p><img src=\"/media/638408397901987840_0.png\" alt=\"\"></p>\n<p>They then in the tutorial describe trying to use postman to test</p>\n<p>I test with <code>curl</code> instead</p>\n<pre><code>curl 'https://fjgbqj5436.execute-api.us-east-2.amazonaws.com/uploads' {\"uploadURL\":\"https://sam-app-s3uploadbucket-1653634.s3.us-east-2.amazonaws.com/112162.jpg?Content-Type=image%2Fjpeg&#x26;X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Credential=ASIAU6CQBER6YBNCDDMJ%2F20201224%2Fus-east-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20201224T174804Z&#x26;X-Amz-Expires=300&#x26;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDIaCXVzLWVhc3QtMiJGMEQCIH65IvgJsofUpIX46lTaG3Pi5WC85ti1lukM3iICh%2BB%2BAiAJEyynPNPhZN8%2Bg1ylO7wthqud9cBcNIChIp2H%2F%2BR7mCryAQjb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDMzOTQ3MDI2MzQyMSIMLqPo1IYyH7udCGZuKsYBSEF3c50YXkmPeSWcLsEzq%2BFBTpeOIrwZTyCUjbJ7fgJUakhM1YRX40jExstN8eJcMXqw00Xd5lYHvZDbU9ajwWPLRAxcEN5BQ0utqn0NGTLyJhibzJUj8cjgm5RguIEKe9GUtMVWa9mi7C5%2FlFpS0i9jK5BSVf74JyPSLETV5mzMMzy5kHBQMGjw1dR66E3MG8PjIqfgKjhVtZmlaicf5OmeqNI2%2F8T5ye%2FICRsH4d7KNEmj4FELa8buW8U%2Fn97ThfH3P7XmMNOok%2F8FOuEBDj1EHluCT4DfZ1jIXjvrJsVv1WtV4POQDn2Dah%2BWosBn%2BFNTtQtw841ACDarYR1ZVbuwcpTjfBPlGuSOncPsbzOhzDy7wYyumsPKsXoPdxTncMWbx4BQkbU5SeF9hjpfIKRMSOqkJBN7%2BtgHXwuW1rfYMDN2OAlQZpTj7uWMPWojUMbvMzyHvI2pfgcRAlrBdGGYDigyjWl9QXP%2Bdi6WiR7XCSXbWcIAJDZh%2Beb%2BIH1asmMJtpAK6nMP8gWczaYh7PMeYyVOIs2B20xQBy%2Bz7oe%2BYQ2GfdEr2hgqPH3jd%2B7c&#x26;X-Amz-Signature=11b8cd524c25ef51193e3b3fc4816760ebcde8bfc74bd52f3f91d8bf409620f5&#x26;X-Amz-SignedHeaders=host\",\"Key\":\"112162.jpg\"}% \n\n</code></pre>\n<p>The premise of this is you make a request, and then the response from\nthe API is a pre-signed URL that then allows you to upload directly to\nS3. You can use <code>curl &#x3C;url> --upload-file yourfile.jpg</code>. This\nautomatically does a PUT request to the s3 bucket (yes, this is talking\ndirectly to s3 now, not the lambda! the lambda is just for generating\nthe \"pre-signed URL\" to let you upload). Careful to copy it exactly as\nis</p>\n<pre><code> curl \"https://sam-app-s3uploadbucket-1653634.s3.us-east-2.amazonaws.com/112162.jpg?Content-Type=image%2Fjpeg&#x26;X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Credential=ASIAU6CQBER6YBNCDDMJ%2F20201224%2Fus-east-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20201224T174804Z&#x26;X-Amz-Expires=300&#x26;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDIaCXVzLWVhc3QtMiJGMEQCIH65IvgJsofUpIX46lTaG3Pi5WC85ti1lukM3iICh%2BB%2BAiAJEyynPNPhZN8%2Bg1ylO7wthqud9cBcNIChIp2H%2F%2BR7mCryAQjb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDMzOTQ3MDI2MzQyMSIMLqPo1IYyH7udCGZuKsYBSEF3c50YXkmPeSWcLsEzq%2BFBTpeOIrwZTyCUjbJ7fgJUakhM1YRX40jExstN8eJcMXqw00Xd5lYHvZDbU9ajwWPLRAxcEN5BQ0utqn0NGTLyJhibzJUj8cjgm5RguIEKe9GUtMVWa9mi7C5%2FlFpS0i9jK5BSVf74JyPSLETV5mzMMzy5kHBQMGjw1dR66E3MG8PjIqfgKjhVtZmlaicf5OmeqNI2%2F8T5ye%2FICRsH4d7KNEmj4FELa8buW8U%2Fn97ThfH3P7XmMNOok%2F8FOuEBDj1EHluCT4DfZ1jIXjvrJsVv1WtV4POQDn2Dah%2BWosBn%2BFNTtQtw841ACDarYR1ZVbuwcpTjfBPlGuSOncPsbzOhzDy7wYyumsPKsXoPdxTncMWbx4BQkbU5SeF9hjpfIKRMSOqkJBN7%2BtgHXwuW1rfYMDN2OAlQZpTj7uWMPWojUMbvMzyHvI2pfgcRAlrBdGGYDigyjWl9QXP%2Bdi6WiR7XCSXbWcIAJDZh%2Beb%2BIH1asmMJtpAK6nMP8gWczaYh7PMeYyVOIs2B20xQBy%2Bz7oe%2BYQ2GfdEr2hgqPH3jd%2B7c&#x26;X-Amz-Signature=11b8cd524c25ef51193e3b3fc4816760ebcde8bfc74bd52f3f91d8bf409620f5&#x26;X-Amz-SignedHeaders=host\" --upload-file test.jpg\n</code></pre>\n<p>There is no response, but I can then check the s3 console and see the\nfile upload is successful (all files are renamed)</p>\n<p><img src=\"/media/638408397901987840_1.png\" alt=\"\"></p>\n<p>Figure shows that the file upload is successful :)</p>\n<p>Then we can edit the file frontend/index.html from the repo we cloned to\ncontain the lambda with the /uploads/ suffix</p>\n<p><img src=\"/media/638408397901987840_2.png\" alt=\"\"></p>\n<p>Figure shows editing the index.html with the lambda endpoint</p>\n<p>Then we manually upload this file to another s3 bucket or test it\nlocally</p>\n<pre><code> aws s3 cp index.html s3://mybucket/\n\n\n# then ...visit that in the browser\n</code></pre>\n<p>At this point the files are getting uploaded but not publically\naccessible. To make them publically accessible we uncomment the\nACL: 'public-read' in the getSignedURL/app.js folder in the github repo</p>\n<p><img src=\"/media/638408397901987840_3.png\" alt=\"\"></p>\n<p>Figure showing the public-read uncommented</p>\n<p><img src=\"/media/638408397901987840_4.png\" alt=\"\"></p>\n<p>Figure showing the lines that need uncommenting in template.yaml in the\nroot of the github repo that allows putObject in s3 with the public-read\nACL</p>\n<p>Re-run <code>sam deploy --guided</code>, same thing as at the start</p>\n<p>Now the objects are publicly accessible!</p>"},{"title":"Challenges I have faced learning React","date":"2020-07-04","slug":"2020-07-04","html":"<p>Learning React was a big challenge for me. I started learning React in earnest\nin 2019. It was a difficult experience overall, but I wanted to go over my\nlearning experience</p>\n<p>If I were to take away anything from this</p>\n<ul>\n<li>pair programming was really useful especially as a remote worker, I had\nnothing before that except weekly standups where I felt really depressed</li>\n<li>reading the book \"Learning React\" was also very helpful, it helped provide\nsome mental structure</li>\n</ul>\n<p>Also stay patient, stay thankful, and try to focus while you learn</p>\n<h2>Introduction to me</h2>\n<p>I am maybe what you'd call a front-end engineer. I have done web development\nfor about 7 years now. I worked on various projects created with Ruby on Rails,\nPHP, Perl CGI, Java servlets, and client side JS. I am pretty steeped in the\n\"jquery spaghetti\" / mutable DOM everywhere era of things.</p>\n<h2>My new job!</h2>\n<p>When I got a call about a new job in 2018, I was really happy and started in\nJune 2018. They decided they are going to do \"the big rewrite\" and are going to\nuse React.</p>\n<p>My coworker started building the new React app prototype. My coworker keeps\nasking me \"what state management library should we use\". I just had no idea\nabout React still, I had not ever looked into state management, and basically\njust was like \"I dunno!\". I had no way to form an opinion. I was also working\non some misc stuff sort of unrelated to the rewrite and remained pretty out of\nthe loop. We would have weekly meetings but I just wouldn't really understand\nthe goings ons. The project started using mobx-state-tree and I saw them start\nto write fresh code for the project but things like prop-types just were\nconfusing to me, e.g. there were the mobx-state-tree model types, and suddenly\nand the React prop-types and it was still the days of class-based React\ncomponents. I couldn't get any clear idea of what was happening</p>\n<h2>I am floundering...not understanding what's going on with the rewrite</h2>\n<p>It's December 2018, I go home for Christmas and I have an honest talk with my\nparents and tell them \"I don't get what is happening in the new codebase, I'm\nhonestly unhappy, and it just does all this 'React' stuff\" but I can't explain\nReact to them I just say the code is automatically reacting to other things. My\nparents say \"well if you are unhappy you might have to leave your job\" and they\nare not like, cheering for me to leave, but they tell me that. At this point,\nit really hit me that I do like this job and I decided to try to focus on work.</p>\n<h2>I try and make an honest attempt to get involved in the project</h2>\n<p>On January 10th 2019, I make my first commit to the rewrite by doing some\nmonkey-see monkey-do type coding. I copy a bunch of files and just put them in\nthe right place, tweak some lines, and start to figure out how to make things\nrun. By the end of January 2019 I get my first code change merged.</p>\n<p>I also suggested that we start doing <strong>pair-programming sessions</strong>. Once I\nstarted these it made a huge difference for me in learning how to code. The\npair programming often still way over my head due to how my coworker presented\nstuff or how much he assumed I understood. Nevertheless, these were extremely\nhelpful for me to help get caught up.</p>\n<h2>Reading \"Learning React\"</h2>\n<p>In March 2019, I got the book \"Learning React\" (O'Reilly2017\n<a href=\"https://www.oreilly.com/library/view/learning-react/9781491954614/\">https://www.oreilly.com/library/view/learning-react/9781491954614/</a>) for my\nkindle.  Reading this book was a big help I felt, and provided a needed \"brain\nreset\" for me. The book worked well for me, I read it each night on my kindle,\nand the function component concepts were super enlightening. To me it was so\nmuch better reading a book than, say, an internet tutorial. With the book, I\ncould focus, not have distractions, etc. My eyes would just glaze over every\ntime I clicked on internet tutorials and stuff before this.</p>\n<p>So anyways, March 2019 goes on, and I'm learning, but our codebase still feels\npretty complicated and alien. We use mobx-state-tree and the glue for\nmobx-state-tree to React e.g. the mobx-react doesn't really make sense to me. I\nremember asking my coworkers why my component was not updating and they\neventually find out it's because I keep not using the observe() wrapper around\nmy components.</p>\n<h2>Trying out Typescript</h2>\n<p>In April 2019, I start to experiment with Typescript and release a Typescript\nversion of some data parsing code. I start by explicity specifying a lot of\ntypes but I eventually start getting into the zen of \"type inference\" and I\nturn off the @typescript-eslint/explicit-function-return-type so I get implied\nreturn types.</p>\n<h2>Using React hooks</h2>\n<p>In May 2019 I try out my first React hook, a useState. It worked well. I\ncouldn't really figure out why I would use it instead of the mobx state\nmanagement we used elsewhere, but the example was that it was a click and drag\nand it made sense to keep that click and drag state local to the component\nrather than the \"app\". The book \"Learning React\" also helped me to understand\nhooks but only indirectly...it helped me understand function components vs\nclass components, and now, we all just write only function components with\nhooks and rarely see class components.</p>\n<h2>Using react-testing-library</h2>\n<p>In June 2019, I create \"integration test\" level tests for our app. I had used\nreact-testing-library for some components before this, but this was using\nreact-testing-library to render the entire \"app level\" component. I was happy\nto pioneer this and was happy to try this out instead of doing true browser\ntests, and I think this has worked out well.</p>\n<p>Some caveats: I got very caught up with trying to do canvas tests initially. I\nreally wanted to use jest-mock-canvas but we were using offscreencanvas via a\npretty complicated string of things, so I don't make progress here, and I also\ngot confused about the relationship between node-canvas and jest-mock-canvas\n(they are basically totally different approaches). Later on, I find using\njest-image-snapshot of the canvas contents works nice (ref\n<a href=\"https://stackoverflow.com/questions/33269093/how-to-add-canvas-support-to-my-tests-in-jest\">https://stackoverflow.com/questions/33269093/how-to-add-canvas-support-to-my-tests-in-jest</a>)</p>\n<p>Other random note: when building out the integration tests, we got a lot\nof \"act warnings\" which were confusing. These were fixed in React 16.9\n(released August 2019), but we had to ignore them and they basically just\nconfused me a lot and made it feel like I was battling a very complex system\nrather than a nice simple one.</p>\n<h2>Conclusions</h2>\n<p>Overall, I just wanted to write up my whole experience. It felt really\ndifficult for me to make these changes. I also went through a breakup during\nthis time, had a bad living situation, etc. so things were a struggle. If\nanyone else has had struggles learning React, tell your story, and let me know.\nI'd like to also thank everyone who helped me along the way. I feel like a much\nbetter coder now, yet, I should always keep growing. The feeling of\nuncomfortableness could be a growing experience.</p>\n<h2>Footnote</h2>\n<p>I also had tried using React in 2016... but it was unsuccessful, but maybe for\ninteresting reasons?</p>\n<p>I was tasked with making a normal form with text input elements, and I wanted\nto code and wanted to try using React. I tried importing React via a CDN  and\ngave it a shot, and it seemed simple enough, but I kept getting really confused\nabout how to even read and initialize the value of a textbox for example\nproperly. TLDR: I was not aware of what a <em>controlled component</em> was.</p>\n<p>The idea of controlled components (not a word in my vocabulary at the time) was\nquite unintuitive and instead, I kept googling weird things like \"two way data\nbinding react\" and variants of this. I had never used Angular but I heard of\ntwo-way data binding from Angular, and I just felt like it was what I needed. I\neven posted about my frustrations about this on the React subreddit and was\ndownvoted. Felt bad. I was just really confused. I abandoned the project in\nReact and just used our normal jqueryish thing.</p>"},{"title":"Misconceptions your team might have during The Big Rewrite","date":"2020-06-03","slug":"2020-06-03","html":"<p>Disclaimer: I enjoy the project I am working on and this is still a work\nin progress. I just had to rant about the stuff I go through in my job\nhere, but it does not reflect the opinions of my emplorer, and my\npersonal opinion is despite these troubles we are coming along nicely</p>\n<p>I joined a team that was doing the big rewrite in 2018. I was involved\nin the project before then and knew it's ins and outs, and frankly think\nit's still a great system. In order to break it's \"limitations\" a grand\nv2 gets started. I think my team has been good. My tech lead is really\ngood at architecture. Where I really resist kind of \"writing new\narchitecture that is not already there\", he can pull up entirely new\nconcepts and abstractions that are all pretty good. Myself, I don't much\nenjoy writing \"new architecture\" if there is something already there\nthat I can use, and I'll try to refer to the existence of an existing\nthing instead of creating new exotic stuff.</p>\n<p>Now, what happened during the big rewrite so far. 4 people on the team,\n2 years in</p>\n<p>Persistent confusion about sources of slowness in our app</p>\n<ul>\n<li>it's only slow because devtools is open (maybe it is! but this is\ndefinitely a red herring. the code should work with devtools open.\nreason that's been stated: devtools adds a \"bunch of instrumentation to\nthe promises that slows it down\"...stated without any evidence during a\n3 hour long planning call...)\n - it's only slow because we're using a development build of react, try\na production build (the production build makes some stuff faster, but it\nis NOT going to save your butt if you are constantly rerending all your\ncomponents unnecessarily every millisecond during user scroll, which is\nsomething we suffered from, and it creeps back in if you are not careful\nbecause you can't write tests against this so often one day I'll be\nlooking at my devtools and suddenly things are rendering twice per frame\n(signature of calling an unnecessary setState), tons of unnecessary\ncomponents rendering in every frame (signature of\ncomponentShouldUpdate/bad functional react memoizing, etc))\n - it's slow because we are hogging the main thread all the time, our\nkiller new feature in v2 is an intense webworker framework. now main\nthread contention is a concern, but really our app needs to just be\nperformant all around, webworkers just offloads that cpu spinning to\nanother core. what we have done in v2 is we went whole hog and made our\ncode rely on OffscreenCanvas which 0 browsers support. also, our\nwebworker bundles (worker-loader webpack build) are huge webpack things\nthat pretty much contain all the code that is on the main thread so it's\njust massive. that makes it slow at loading time, and makes it harder to\nthink about our worker threads in a lighter-weight way, and the worker\nconcept is now very deeply entrenched in a lot of the code (all code has\nto think of things in terms of rpc calls)\n - it's slow because there are processes that haven't been aborted\nspinning in the background, so we must build out an intensive\nAbortController thing that touches the entirety of all our code\nincluding sending abort signals across the RPC boundary in hopes that a\nlocked up webworker will respond to this (note: our first version of the\nsoftware had zero aborting, did not from my perspective suffer.\narguments with the team have gotten accusatory where I just claim that\nthere is no evidence that the aborting is helping us, pointing to the\nfact that our old code works fine, and that if our new code suffers\nwithout aborting, that means something else is wrong. I have not really\nbeen given a proper response for this, and so the curse of passing\nAbortSignals onto every function via an extra function parameter drags\non\n - it's slow because we are not multithreading..., so we put two views\nof the same data into different webworkers (but now each webworker\nseparately downloads the same data, which leads to more resource spent,\nmore network IO, more slowness)</li>\n</ul>\n<p>confusion about what our old users needs are</p>\n<ul>\n<li>\n<p>tracks not having per-track scroll (problem: leads to many scrolls\nwithin-scrolls, still unresolved problem)\n - the name indexing was always a big problem (yes it is slow but is it\nreally THE critical problem we face? likely not: bioinformatics people\nrun a data pipeline, it takes a couple days, so what). use elasticsearch\nif it sucks so bad\n - our users are \"stupid\" so they need to have every single thing GUI\neditable (interesting endeavor, but our design for this has been\ndifficult, and has not yet delivered on simplifying the system for\nusers)\n - our users \"do not like modal popups\" so we design everything into a\ntiny sidedrawer that barely can contain the relevant data that they want\nto see</p>\n</li>\n<li>\n<p>having interest in catering to obscure or not very clear \"user\nstories\" like displaying the same exact region twice on the screen at\nonce saying \"someone will want to do this\", but causing a ton of extra\nlogical weirdness from this</p>\n</li>\n<li>\n<p>not catering to emerging areas of user needs such as breaking our\nlarge app into components that can be re-used, and instead just going\nfull hog on a large monolith project and treating our monolith as a\ngiant hammer that will solve everyones problems, when in reality, our\nusers are also programmers that could benefit from using smaller\ncomponentized versions of our code</p>\n</li>\n<li>\n<p>confusion about \"what our competitors have\". sometimes my team one day\nwas like \"alright we just do that and then we have everything product X\nhas?\" and I just had to be clear and be like, no! the competitor has a\nreall pretty intricate complex system that we could never hope to\nreplicate. but does that matter? probably not, but even still, we likely\ndon't have even 20% of the full set of functions of a competitor.\nluckily we have our own strengths that make us compelling besides that\n20%</p>\n</li>\n<li>\n<p>making it so our product requires a server side component to run,\nwhere our first version was much more amenable to running as a static\nsite</p>\n</li>\n<li>\n<p>etc...</p>\n</li>\n</ul>\n<p>but what does all this imply?</p>\n<p>there are persistent confusion about what the challenges we face are,\nwhat the architectural needs are, what our user stores are, what our new\nv2 design goals are, and more. It's really crazy</p>"},{"title":"Behind the release - the story of the bugs and features in JBrowse 1.16.0","date":"2018-12-17","slug":"2018-12-17","html":"<p>Every once in awhile, you might see that your favorite program, JBrowse,  has a\nnew release. There are a ton of little snippets in the release notes, you might\nas well just go ahead and upgrade, but what went into all those little fixes?\nGoing to the blog post has links to the github\nissues, <a href=\"http://jbrowse.org/blog/2018/12/13/jbrowse-1-16-0.html%C2%A0but\">http://jbrowse.org/blog/2018/12/13/jbrowse-1-16-0.html but</a> I felt like\nmaybe I'd add a little more context for some of them:</p>\n<p>PS This is sort of motivated by @zcbenz blog on Electron\n(<a href=\"https://twitter.com/zcbenz%C2%A0http://cheng.guru/\">https://twitter.com/zcbenz http://cheng.guru/</a>) which tells the software in\nterms of actual commit messages and such.</p>\n<ul>\n<li>\n<p>The webpack build doing a production build by default. This seems pretty\nstraightforward, but was also difficult because I use WSL and the UglifyJs\nplugin had trouble on WSL using the parallel: 4 option to use multiple\nprocessors. This was really annoying and resulted in the webpack build just\nhanging for no reason and only careful google-fu really uncovered other\npeople having this issue. I removed the parallelism as the speed gain wasn't\neven really justifiable <a href=\"https://github.com/gmod/jbrowse/pull/1223\">https://github.com/gmod/jbrowse/pull/1223</a></p>\n</li>\n<li>\n<p>The incorporation of the <code>@gmod/bam</code> module. This was an almost 2 months\nprocess after my first module, <code>@gmod/indexedfasta</code>. It required really\ngetting down to the binary level for BAM and was pretty tough. The module has\nalready itself had 12 releases\n<a href=\"https://github.com/GMOD/bam-js/blob/master/CHANGELOG.md\">here</a></p>\n</li>\n</ul>\n<p>- Added support for indexing arbitrary fields from GFF3Tabix files. This was\nfairly straightforward but required making design decisions about this.\nPreviously flatfile-to-json.pl files would have a command line flag to index\narbitrary fields. Since gff3tabix files are specified via config, I allowed\nspecifying arbitrary fields via config.</p>\n<p>- Added ability to render non-coding transcript types to the default Gene\nglyph. This one was a nice feature and enables you to see non-coding types, but\nrequired some weird design decisions because I could not override\nthe <code>box->style->color</code> from a higher level type simply using the\n<code>_defaultConfig</code> function, so I needed to override the <code>getStyle</code> callback that\nwas passed down to the lower levels, so that it was able to use the default\nlower level style and also our non-coding transcript style. See this part of\nthe code for\ndetails <a href=\"https://github.com/GMOD/jbrowse/commit/ec638ea1cc62c8727#diff-a14e88322d8f4e8e940f995417277878R22\">https://github.com/GMOD/jbrowse/commit/ec638ea1cc62c8727#diff-a14e88322d8f4e8e940f995417277878R22</a></p>\n<p>- Added <code>hideImproperPairs</code> filter. This was fairly straightforward but it is\none of these bugs that went unnoticed for years...the <code>hideMissingMatepairs</code>\nflag would hide things didn't have the sam 0x02 flag for \"read mapped in proper\npair\", but reads with this flag could still be paired. Doing the 1.16 release\nthat focused on paired reads helped focus on this issue and now\nhideMissingMatepairs filters on \"mate unmapped\" and <code>hideImproperPairs</code> is\nthe \"read mapped in proper pair\"</p>\n<ul>\n<li>Added <code>useTS</code> flag. This one is fairly straightforward, it is similar to\n<code>useXS</code> which colors reads based on their alignment in canonical splice site\norientations. I figured I could just copy the <code>useXS</code> to the <code>useTS</code> since I\nfigured they are the same, but I went ahead and manually generated RNA-seq\nalignments with minimap2 and found that the useTS is actually flipped the\nopposite of <code>useXS</code>, so it was valuable to get actual test data here.</li>\n</ul>\n<p>- Fixed issue where some <code>generate-names</code> setups would fail to index features.\nThis was a bad bug that was brought to light by a user. I was kind of mind\nboggled when I saw it. In JBrowse 1.13-JBrowse 1.15 a change was introduced to\nname indexing with a memory leak. In JBrowse 1.15 that was removed. But, there\nwas another change where refseqs could return empty name records, because they\nwere handled separately. But if the initial fill up of the name buffer of 50000\nwas exceeded by the reference sequence, then there would be empty name records\nafter this point and cause the name indexing to stop. Therefore this bug would\nonly happen when the reference sequence indexing buffer exceeded 50000 items\nwhich could happen even when there are less than 50000 refseqs due to\nautocompletions</p>\n<p>-  Fixed issue with getting feature density from BAM files via the index stats\nestimation. This involved parsing the \"dummy bin\" from index files, and I found\nit was failing on certain 1000 genomes files. I actually don't really know what\nthe story behind this was, but our tabix code was better at parsing the dummy\nbins than my bam code, and it was the same concept, so I took a note from their\ncodebase to use it in bam-js code. Commit\nhere <a href=\"https://github.com/GMOD/bam-js/commit/d5796dfc8750378ac8b875615ae0a7e81371af76\">https://github.com/GMOD/bam-js/commit/d5796dfc8750378ac8b875615ae0a7e81371af76</a></p>\n<p>-  Fixed issue with some GFF3Tabix tracks having some inconsistent layout of\nfeatures. This is a persistently annoying fact in tabix files where we cannot\nreally get a unique ID of a feature based on it's file offset. Therefore this\ntakes the full crc32 of a line as it's unique ID.</p>\n<ul>\n<li>Fixed CRAM store not renaming reference sequences in the same way as other\nstores. This one was interesting because rbuels made a fix but it caused\nfeatures from one chromosome to show up on the wrong ones, so chr1 reads\nwhere showing up on chrMT. This happened because it was falling back to the\nrefseq index if it chrMT wasn't in the embedded \"sam header\" in the CRAM\nfile, but it should only fallback to refseq index if there is not any\nembedded \"sam header\" in the CRAM file.</li>\n</ul>\n<p>-  Fixed bug where older browsers e.g. IE11 were not being properly supported\nvia babel. This was a absolutely terrible bug that I found over thanksgiving\nbreak. It was a regression from 1.15 branch of JBrowse. Previous versions from\n1.13 when webpack was up until 1.15 used <code>@babel/env</code>. It was changed to\nbabel-preset-2015 but it was not being run correctly. Then I found that even if\nI did get it running correctly, it was unable to properly babel-ify the\nlru-cache module because it used something called\n<code>Object.defineProperty('length', ...)</code> to change how the length property was\nintepreted which was illegal in IE11. The 'util.promisify' NPM module also did\nthis in some contexts. I found that I could use the quick-lru module and the\nes6-promisify module instead of lru-cache and util.promisify as a workaround.\nThen I had to update all <code>@gmod/tabix</code>, <code>@gmod/vcf</code>, <code>@gmod/bgzf-filehandle</code>,\n<code>@gmod/indexedfasta</code>, <code>@gmod/tribble-index</code>, <code>@gmod/bam</code>, and JBrowse proper to\nuse these modules instead, and make the bable chain, which typically does not\nparse node_modules, to build these modules specifically (I didn't want to setup\nbabel toolchains for every single one of these modules, just one in the jbrowse\nmain codebase...). This was really a lot of work to support IE11 but now that\nworks so ...ya</p>\n<p>-  Fixed bug where some files were not being fetched properly when changing\nrefseqs. This was actually fixed when I changed out lru-cache for quick-lru and\nfixed a bug where the cache size was set to 0 due to a erroneous comment that\nsaid <code>50*1024 // 50MB</code>...of course it should have said <code>50*1024*1024 // 50MB</code> <a href=\"https://github.com/GMOD/jbrowse/commit/2025dc0aa0091b70\">https://github.com/GMOD/jbrowse/commit/2025dc0aa0091b70</a></p>\n<ul>\n<li>\n<p>Fixed issue where JBrowse would load the wrong area of the refseq on startup\nresulting in bad layouts and excessive data fetches. This was actually a\nheinous bug where jbrowse upon loading would just navigateTo the start of the\nreference sequence automatically and then to wherever was specified by the\nuser. This resulted in track data to start downloading immediately from the\nstart of the chromosome and resulted in for example 350 kilobases of\nreference sequence from all tracks to start downloading, which when I was\nimplementing view as pairs, was causing me to download over 100MB routinely.\nThis was terrible, and after fixing I only download about 10MB over even\nlarge regions for most BAM files. Additionally, this bug was causing the\ntrack heights to be calculated incorrectly because the track heights would\nactually be calculated based on distorted canvas\nbitmaps. <a href=\"https://github.com/gmod/jbrowse/issues/1187\">https://github.com/gmod/jbrowse/issues/1187</a></p>\n</li>\n<li>\n<p>JBrowse Desktop was not fetching remote files. This was a weird issue where\nremote file requests were considered a CORS requests to any external remote.\nThis was solved by changing the usage of the fetch API in JBrowse for\nnode-fetch which does not obey CORS. Note that electron-fetch was also\nconsidered, which uses Chromiums network stack instead of node's, but that\nhad specific assumptions about the context in which it was called.</p>\n</li>\n</ul>\n<p>-  Fixed issue where some parts of a CRAM file would not be displayed in\nJBrowse due to a CRAM index parsing issue. This was based on a sort of binary\nsearch that was implemented in JBrowse where the elements of the lists were\nnon-overlapping regions, and the query was a region, and the output should be a\nlist of the non-overlapping regions that overlap the query. Most algorithms for\nbinary search don't really tell you how to do searches on ranges so needed to\nroll up my sleeves and write a little custom code. An interval tree could have\nbeen used but this is too heavy-weight for non-overlapping regions from the\nindex <a href=\"https://github.com/GMOD/cram-js/pull/10\">https://github.com/GMOD/cram-js/pull/10</a></p>\n<p>-  Fixed an issue where BAM features were not lazily evaluating their tags.\nWhen a function <code>feature.get('blahblah')</code> is called on a BAM feature, it checks\nto see if it's part of a default list of things that are parsed like feature\nstart, end, id, but if not, it has to parse all the BAM tags to see if it is a\ntag. Since they are called \"lazy features\" the tag processing is deferred until\nit is absolutely needed. As it turned out, the incorporation of CRAM in 1.15\nwas calling a function to try to get the CRAM's version of CIGAR/MD on the BAM\nfeatures unnecessarily invoking the tag parsing on every feature up front and\ntherefore making the feature not really lazy anymore. This restored\nthe \"lazyness\" aspect of BAM.</p>\n<p>-  Fixed issue where CRAM layout and mouseover would be glitchy due to ID\ncollisions on features. In the 1.15 releases, CRAM was introduced, and we\nthought that the concept of taking CRC32 of the entire feature data days were\nover because there is the concept of a \"unique ID\" on the features. However,\nthis ID was only unique within the slices, so around the slice boundaries there\nwere a lot of bad feature layouts and mouseovers would fail because they would\nmap to multiple features, etc. I found a way to unique-ify this by giving it\nthe sliceHeader file offset. <a href=\"https://github.com/GMOD/cram-js/pull/10\">https://github.com/GMOD/cram-js/pull/10</a></p>\n<ul>\n<li>\n<p>We also had behind the scenes work by igv.js team member jrobinso who helped\non the CRAM codebase to incorporate a feature where for lossy read names, so\nthat a read and it's mate pair would consistently be assigned the same read\nname based on the unique ID mentioned above. There was also a rare issue\nwhere sometimes the mate pair's orientation was incorrectly reported based on\nthe CRAM flags, but the embedded BAM flags correctly reported it.</p>\n</li>\n<li>\n<p>Finally the paired reads feature. This was a feature that I really wanted to\nget right. It started when garrett and rbuels were going to san diego for the\nCIVIC hackathon, and we talked about doing something that matched a \"variant\nreview system\" that they had done for the IGV codebase, which involved\ndetailed inspection of reads. I thought it would probably be feasible for\njbrowse to do this, but I thought essentially at some point that enhancing\njbrowse's read visualizations with paired reads would be a big win. I had\nthought about this at the JBrowse hackathon also and my discussions then were\nthat this was very hard. Overall, I invented a compromise that I thought was\nreasonable which was that there can be a \"maxInsertSize\" for the pileup view\nbeyond which the pairing wouldn't be resolved. This allowed (a) a significant\nreduction in data fetches because I implemented a \"read redispatcher\" that\nwould actually literally resolve the read pairs in the separate chunks and\n(b) a cleaner view because the layout wouldn't be polluted by very long read\ninserts all the time and also, for example, if you scrolled to the right, and\nsuddenly a read was paired to the left side of your view, it would result in\na bad layout (but with max insert size, the window of all reads within\nmaxinsertsize are always resolved so this does not happen) and finally ( c)\nthe paired arc view was incorporated which does not use read redispatching\nand which can do very long reads. All of these things took time to think\nthrough and resolve, but it is now I think a pretty solid system and I look\nforward to user feedback!</p>\n</li>\n</ul>"},{"title":"Problems that I experienced with the HPCC","date":"2017-04-21","slug":"2017-04-21","html":"<p>Many of these issues may be due to me being stubborn with a weird build system.\nNonetheless, they were baffling, and I had very little interest in debugging\nthese issues. I just wanted to get my science done after all!</p>\n<h1>Module load completely barfs with incomprehensible error</h1>\n<pre><code>$ module spider bedtools\nUsing system spider cache file\n/opt/software/lmod/bin/lua: /opt/software/lmod/4.1.4icer5/libexec/Spider.lua:662: attempt to perform arithmetic on a nil value\nstack traceback:\n    /opt/software/lmod/4.1.4icer5/libexec/Spider.lua:662: in function 'Level1'\n    /opt/software/lmod/4.1.4icer5/libexec/Spider.lua:640: in function 'spiderSearch'\n    /opt/software/lmod/4.1.4icer5/libexec/lmod:967: in function 'cmd'\n    /opt/software/lmod/4.1.4icer5/libexec/lmod:1195: in function 'main'\n    /opt/software/lmod/4.1.4icer5/libexec/lmod:1222: in main chunk\n    [C]: ?\n</code></pre>\n<h1>Linuxbrew is terribly confused by things that depend on gcc</h1>\n<pre><code>brew install hello\n==> Installing dependencies for hello: glibc, xz, gmp, mpfr, libmpc, isl, gcc\n==> Installing hello dependency: glibc\nError: glibc cannot be built with any available compilers.\nInstall Clang or brew install gcc\n</code></pre>\n<p>Using module load Clang does not fix problem >_&#x3C;</p>\n<h1>Compiling things manually on software machine does not work on interactive machine</h1>\n<pre><code>$ mummer\nIllegal instruction (core dumped)\n</code></pre>\n<h1>Many modules have a secret dependency on loading other modules</h1>\n<pre><code>$ module load LASTZ\n\nLmod Warning: Did not find: LASTZ\n\nTry: \"module spider LASTZ\"\n$ module load GNU\n$ module load LASTZ\n$ lastz\nYou must specify a target file\nlastz-- Local Alignment Search Tool, blastZ-like\n  (version 1.03.02 released 20110719)\n...\n</code></pre>\n<p>Etc etc.</p>"},{"title":"How I learned to hate ORM (especially for data import scripts)","date":"2017-03-12","slug":"2017-03-12","html":"<p>When I was tasked with making a new application for our websites, I was\ngiven several CSV files with some expectation that these files could\nbasically be just loaded into a database and jumped into production really\nquickly. If you are using R and Shiny to make a data visualization dashboard,\nespecially if it is read only, this can actually be a reality for you: load\nthose CSVs and just pretend you're a full featured database. I had to actually\ncreate some read write functionality though. This was sort of experimental for\nme and I'm not that well versed in databases, but I wanted to share my\nexperience</p>\n<p>When I started, I chose grails/groovy/hibernate/GORM as a platform to\nuse. This quickly turned into pain when I tried to make a data importer\nusing grails also.</p>\n<p>Each CSV row from the source file would have to be turned into many\ndifferent rows in the database because it represented multiple\nrelationships, example:</p>\n<p><img src=\"/media/158300473458_0.png\" alt=\"\"></p>\n<p>Initially I made my data importer in grails, and was hardcoding column\nnames knowing full well this was really inflexible. At the same time I\nwas also trying to \"iterate\" on my database schema, and I'd want to\nre-import my data to test it out, but it was really really slow. I tried\nmany different approaches to try to speed this up such as cleanUpGorm,\nStatelessSessions, and other tricks, but it would take 10-20 minutes for\nimports on a 100KB input file.</p>\n<p>What I basically realised is that for bulk data import</p>\n<p><strong>1) Using the ORM is really painful for bulk import.</strong></p>\n<p><strong>2) If you can pre-process your data so that it is already in the\nformat the database expects, then you can use the CSV COPY command which\nis very fast</strong></p>\n<p><strong>3) If you can then abandon the ORM mentality and even ignore it as\na convenience factor, then you can embrace my database system itself</strong></p>\n<p>Overall, after all this work, it just seemed like ORM treats the\ndatabase as a danger and something to be heavily abstracted over, but I\nactually found joy in learning how to treat my database as a first class\ncitizen. Soon I started gaining appreciation of</p>\n<ul>\n<li>using plain SQL queries</li>\n<li>learning about full text search in postgres with ts_query</li>\n<li>learning about triggers to make a \"last updated\" field get updated\nautomatically</li>\n</ul>\n<p>I am pretty happy this way, and although I miss some things like\ncriteria queries which are very powerful, I am happy that I can interact\nwith my database as a friend</p>\n<p>At the very least, due to the fact that I now pre-process the data\nbefore database loading, I can now import large amounts of data super\nfast with the CSV COPY command</p>"},{"title":"Plotting a coordinate on the screen","date":"2017-02-16","slug":"2017-02-16","html":"<p>I always end up having to remember the math for plotting a coordinate on the\nscreen, for example an HTML5 canvas and end up stitching it together manually</p>\n<p>If you step through the math it becomes very simple though</p>\n<p>Say you have a coordinate range of 1000 to 2000 that you want to plot in a\nHTML5 canvas of size 100px</p>\n<p>Let's do a quick example and then generalize. Let's say you want to plot the\nvalue 1500, and put it into screen coordinates, so you take that and subtract\nthe minimum of the range</p>\n<pre><code>1500-1000\n</code></pre>\n<p>Second, you know your point is going to be halfway in the range, and in\ngeneral, to get this position, you divide now by the size of the interval you\nare plotting in, e.g. 2000-1000</p>\n<pre><code>(1500-1000)/(2000-1000) = 0.5\n</code></pre>\n<p>We get 0.5 as expected. Then you multiply this proportion times the width of\nbox you are rendering in, e.g. 100 pixels wide, and get that you put your pixel\nat position 50px</p>\n<p>To summarize, the general formula for plotting a point x in a range (x1,x2) on\na screen of width w is</p>\n<pre><code>w*(x -  x1) / (x2 - x1)\n</code></pre>\n<p>Of course same thing applies for y</p>\n<pre><code>h*(y - y1) / (y2 - y1)\n</code></pre>\n<p>This does not take into account small possible adjustments for closed vs open\nranges, which could be important to avoid subpixel rendering on a canvas, but\nthat can be a further exercise</p>"},{"title":"Creating a JBrowse plugin","date":"2016-11-10","slug":"2016-11-10","html":"<p>I have been very impressed with peoples creativity and willingness to\ndig into all the details of JBrowse to customize it's features. One\ngreat way to do this in a modular way is to create a \"JBrowse plugin\".\nThis concept sounds hard but you can set up a simple couple of files and\nrefresh your browser and it will \"just work\"!</p>\n<p><strong>Introduction to the plugin mindset</strong></p>\n<p>In a plugin, you can define new things like custom track types, custom\nadaptors to new file types, new track selectors, or something really\ndifferent. A key insight about the custom types of tracks and things\nthough is that you can put the name of your new custom class in the\njbrowse config file which will then find the code in your plugin and run\nit. Plugins can do other things, but the ability to just swap out track\ntypes or other components in the config file is a great benefit.</p>\n<p><strong>A scenario</strong></p>\n<p>One example that was talked about on the mailing list might involve\nadding new menu items for a given track. We might consider a plugin that\ndefines a custom track type to add that functionality.</p>\n<p>Basically, we can use object- oriented principles to \"inherit\" from some\nexisting track type like CanvasFeatures and then extend its\nfunctionality by overriding the functions.</p>\n<p>If you are not familiar with object-oriented javascript, dojo makes it\npretty easy (but, especially get a background on this if you need to,\nsee footnotes below).</p>\n<p>We can inherit a new track type by using the \"define\" function to\ninclude the dependencies needed in a file, and they are listed in an\narray at the top of your file.</p>\n<p>Then the \"declare\" function creates a new class. The first argument to\ndeclare is the is your parent class, like CanvasFeatures, and we type\n\"return declare\" because we are returning our new track class from the\nmodule.</p>\n<pre><code> define( [\"dojo/_base/declare\",\n \"JBrowse/View/Track/CanvasFeatures\"],\n     function(declare,CanvasFeatures) {\n     return declare(CanvasFeatures, {\n         _trackMenuOptions: function() {\n\n             var opts=this.inherited(arguments); //call the parent\n classes function\n\n             opts.push( // add an extra menu item to the array returned\n from parent class function\n                 {       \n                     label: \"Custom item\",\n                     type: 'dijit/CheckedMenuItem',\n                     onClick: function(event) {\n                         console.log('Clicked');\n                     },  \n                     iconClass: \"dijitIconPackage\"\n                 }   \n             );  \n             return opts;\n         }   \n     });\n     }   \n );\n</code></pre>\n<p>Code listing 1. an example custom track type, MyTrack.js, that adds an\nextra track menu item</p>\n<p><strong>Now how do we make this a plugin?</strong></p>\n<p>In the above section, we created a new track subclass with a custom menu\noption. How do we use this track? We want to turn it into part of afine\nthe boilerplate code from the <a href=\"http://gmod.org/wiki/JBrowse_Configuration_Guide#Writing_JBrowse_Plugins\">Writing\nplugins</a>\nguide.</p>\n<pre><code class=\"language-js\"> define([\n            'dojo/_base/declare',\n            'JBrowse/Plugin'\n        ],  \n        function(\n            declare,\n            JBrowsePlugin\n        ) {\n  \n return declare( JBrowsePlugin, // our plugin's main.js derives from\n the \"JBrowse/Plugin\" base class\n {\n     constructor: function( args ) {\n         /*don't necessarily have to do any initializing here, but you\n can get a handle to various jbrowse components if any initialization\n is needed from the args.browser variable*/\n     }   \n });\n });\n</code></pre>\n<p>Code listing 2. Our plugin's main.js</p>\n<p>After this, we create the plugin directory structure</p>\n<blockquote>\n<p>jbrowse/plugins/MyPlugin</p>\n<blockquote>\n<p>jbrowse/plugins/MyPlugin/js</p>\n<blockquote>\n<p>jbrowse/plugins/MyPlugin/js/main.js</p>\n<p>jbrowse/plugins/MyPlugin/js/MyTrack.js</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<p>Then we can add our new plugin to a config file like jbrowse_conf.json\nas \"plugins\": [\"MyPlugin\"]  and then make a track in trackList.json\nhave \"type\":  \"MyPlugin/MyTrack\" instead of for\nexample \"type\": \"CanvasFeatures\". That will tell jbrowse to load the\nMyTrack class from your plugin instead of the normal CanvasFeatures\nclass.</p>\n<p>That's about it!</p>\n<p>Note that the bin/new-plugin.pl script can automatically initialize some\nof this directory structure too. Try running \"bin/new-plugin.pl\nMyPlugin\" and see what happens.</p>\n<p>Footnotes:</p>\n<p>It is important to understand the module format (AMD) which is what the\n\"define\" function is about and the dojo way of definining classes using\nthe \"declare\" function. Together, this allows the dojo to create\nobject-oriented programs that are modular in javascript. See\n<a href=\"http://dojotoolkit.org/reference-guide/1.10/dojo/_base/declare.html\">http://dojotoolkit.org/reference-guide/1.10/dojo/_base/declare.html</a>\nand <a href=\"http://dojotoolkit.org/documentation/tutorials/1.9/modules/\">http://dojotoolkit.org/documentation/tutorials/1.9/modules/</a>\n(understanding this helps you understand the \"preamble\" for declaring a\njbrowse plugin)</p>"},{"title":"Installing clamav on OSX","date":"2016-06-20","slug":"2016-06-20","html":"<p>It is a common trope that OSX doesn't need anti-virus because everyone targets\nwindows. That is maybe comforting to some but I think it's pretty naive. It\nwould be better to have a system on your machine to tell you about viruses,\ntrojan horses, malware, or spying.  I have decided to employ a free open source\nscanner called clamAV <a href=\"https://www.clamav.net/\">https://www.clamav.net/</a>. I don't really know if it has\nany good features for Mac scanning but thought it could be fun to install</p>\n<p>ClamAV is the top choice for linux based OSs being free and open source (GPL)\nvirus scanner.</p>\n<p>To install we can use homebrew</p>\n<pre><code>    $ brew install clamav\n</code></pre>\n<p>Then there is s config file to setup. This is located\nin /usr/local/etc/clamav/freshclam.conf</p>\n<p>To setup, edit this file and delete the line that says \"Example\" and\nthen uncheck the desired settings. I would chose to enable logging to\n/var/log/clamav.log and also database directories in /var/lib/clamav</p>\n<p>Then run the \"freshclam\" program</p>\n<pre><code>    $ freshclam\n</code></pre>\n<p>This will download the virus scanner database (main) and daily scanning\nupdates</p>\n<p>Then you can run clamscan on a given directory (recursively, only print\ninfected files)</p>\n<pre><code>    $ clamscan -ri ~/\n</code></pre>\n<p>Or add this to a cronjob</p>\n<pre><code>    $ crontab -\n\n    @hourly clamscan -ri ~/ | mail -v -s \"clamscan results\" your.email@gmail.com  >/dev/null 2>&#x26;1\n\n</code></pre>"},{"title":"Querying InterMine databases using R","date":"2016-06-17","slug":"2016-06-17","html":"<p>In the past, I had found some ways to do simple queries on InterMine web\nservices using basic HTTP commands with R (see\n<a href=\"https://gist.github.com/cmdcolin/4758167bdd89e6c9c055\">https://gist.github.com/cmdcolin/4758167bdd89e6c9c055</a>)</p>\n<p>However, the InterMineR (<a href=\"https://github.com/intermine/intermineR\">https://github.com/intermine/intermineR</a>)\npackage automates some of these features and makes it easier to load the\ndata in R.</p>\n<p><strong>Installation</strong></p>\n<p>One way to install InterMineR is to install from github with\nhadley/devtools</p>\n<pre><code>install.packages(\"devtools\")\ndevtools::install_github(\"hadley/devtools\")\ndevtools::install_github(\"intermine/intermineR\")\n</code></pre>\n<p><strong>Usage</strong></p>\n<p>Basic usage includes loading the \"intermine URL\" using the initInterMine\nfunction. Then various functions can be called on this result.</p>\n<pre><code>library(InterMineR)\nmine=initInterMine(\"http://bovinegenome.org/bovinemine/\")\ngetVersion(mine) #18, intermine API version\ngetRelease(mine) #1.0, our data release version\ngetTemplates(mine) # lists all templates on interminer\n</code></pre>\n<p><strong>Run a template query</strong></p>\n<p>From the getTemplates function, if you see a template query that you\nwant to run, you can use the getTemplateQuery function with it's name,\nand run it with the runQuery function</p>\n<pre><code>getTemplateQuery(mine,\"TQ_protein_to_gene\") # see what template looks like\ntemplate=getTemplateQuery(mine,\"TQ_protein_to_gene\") # save template\nrunQuery(mine,template) # run the template query with default params, receive data.frame\n</code></pre>\n<p>This method is good, but some improvement could be added to change\ndefault parameters in the template query, etc.</p>\n<p><strong>Run query XML</strong></p>\n<p>Another option for running queries is to use the query XML that you can\ndownload from the InterMine query result pages.</p>\n<pre><code> # get all Ensembl genes on chr28 from bovinemine\n query='&#x3C;query model=\"genomic\" view=\"Gene.primaryIdentifier\n Gene.secondaryIdentifier Gene.symbol Gene.name Gene.source\n Gene.organism.shortName Gene.chromosome.primaryIdentifier\"\n sortOrder=\"Gene.primaryIdentifier ASC\" >&#x3C;constraint\n path=\"Gene.organism.shortName\" op=\"=\" value=\"B. taurus\"\n />&#x3C;constraint path=\"Gene.chromosome.primaryIdentifier\" op=\"=\"\n value=\"GK000028.2\" />&#x3C;/query>'\n\n results=runQuery(mine, query)\n\n head(results)\n</code></pre>\n<p><strong>Conclusion</strong></p>\n<p>The InterMineR package has a couple of nice features for getting\nInterMine data with a couple of functions for looking at templates. For\nmany use cases, copying the Query XML from a InterMine webpage and\npasting that into the runQuery function is sufficient and produces a\ndata frame that can be analyzed.</p>\n<p>PS it is not easy to post XML on tumblr after editing the post in\nmarkdown mode. You have to add the lt and gt shortcuts and even after\nthat it gets filtered?!</p>"},{"title":"How to make your resume.json or resume-cli look great","date":"2016-04-23","slug":"2016-04-23","html":"<p>There are a ton of themes for resume-cli that are not immediately\nobvious to find</p>\n<p>To see all the great themes on the command line, check out</p>\n<pre><code>    curl http://themes.jsonresume.org/themes.json |jq .\n</code></pre>\n<p>I tried a bunch of them</p>\n<pre><code>   4679  resume export site/resume/index.html -t modern-freeland\n   4680  resume export site/resume/index.html -t modern-freelance\n   4682  resume export site/resume/index.html -t modern-with-projects-section\n   4683  resume export site/resume/index.html -t dangerflat\n   4684  resume export site/resume/index.html -t striking\n   4685  resume export site/resume/index.html -t crisp\n   4686  resume export site/resume/index.html -t semantic-ui\n   4687  resume export site/resume/index.html -t material\n   4688  resume export site/resume/index.html -t modern-extended\n   4689  resume export site/resume/index.html -t paper\n   4690  resume export site/resume/index.html -t smart\n   4691  resume export site/resume/index.html -t flat\n\n</code></pre>\n<p>Note: resume.json is setup to use HTML themes, so even though it has a\nPDF output option, it is inherently converting HTML first and then to\nPDF. The PDF conversion is done by a automated cloud service, which\ncurrently can fail sometimes. It is probably better to just choose HTML\nand convert to PDF if you need to.\nSee <a href=\"https://github.com/jsonresume/resume-cli/issues/94\">https://github.com/jsonresume/resume-cli/issues/94</a></p>"},{"title":"Creating a testing framework for JBrowse plugins","date":"2016-04-19","slug":"2016-04-19","html":"<p>Testing client side apps requires a couple of tedious steps: Organizing\nthe git clone, the dependencies, wrangling up a web server, the test\nframework, etc.</p>\n<p>When testing a plugin for jbrowse, the dependency tree is interesting\nbecause the plugin \"depends\" on JBrowse to run, but we will use\ntravis-CI and bower inside the git repo for our plugin to accomplish\nthis.</p>\n<p>In this scenario, we will</p>\n<ol>\n<li>\n<p>Use bower to install jasmine and JBrowse (our platform that we write\nthe plugin for)</p>\n</li>\n<li>\n<p>Use nginx to launch a webserver on travis-CI</p>\n</li>\n<li>\n<p>Use the phantomjs run-jasmine.js script to check jasmine test\nresults</p>\n</li>\n</ol>\n<p>Without further ado</p>\n<p>Here is the .travis.yml</p>\n<pre><code>    sudo: false\n    addons:\n      apt:\n        packages:\n        - nginx\n    cache:\n      apt: true\n      directories:\n      - $HOME/.cache/bower\n    before_install:\n      - npm install -g jshint bower\n    install:\n      - bower install\n    before_script:\n      - cat test/travis.conf | envsubst > test/travis-envsubst.conf\n      - nginx -c `pwd`/test/travis-envsubst.conf\n    script:\n      - phantomjs test/run-jasmine.js http://localhost:9000/test/\n      - jshint js\n\n</code></pre>\n<p>Refer to\n<a href=\"http://searchvoidstar.tumblr.com/post/141858047213/running-nginx-on-containerised-travis-ci-pt-2\">http://searchvoidstar.tumblr.com/post/141858047213/running-nginx-on-containerised-travis-ci-pt-2</a>\nfor details on the nginx setup</p>\n<p>Here is the bower.json</p>\n<pre><code>    {\n      \"name\": \"sashimiplot\",\n      \"homepage\": \"https://github.com/cmdcolin/sashimiplot\",\n      \"description\": \"Sashimi track type for jbrowse\",\n      \"main\": \"js/main.js\",\n      \"keywords\": [\n        \"bioinformatics\",\n        \"jbrowse\"\n      ],\n      \"license\": \"MIT\",\n      \"ignore\": [\n        \"**/.*\",\n        \"node_modules\",\n        \"bower_components\",\n        \"src\",\n        \"test\",\n        \"tests\"\n      ],\n      \"devDependencies\": {\n        \"jasmine-core\": \"jasmine#^2.4.1\",\n        \"jbrowse\": \"git://github.com/GMOD/jbrowse.git#master\"\n      }\n    }\n</code></pre>\n<p>The key thing here is that it installs jasmine and JBrowse. I set\n.bowerrc to install both jasmine and JBrowse to the \"test\" directory</p>\n<pre><code>    {\n        \"directory\": \"test\"\n    }\n</code></pre>\n<p>With this setup, bower will make a \"flat dependency tree\" in the test\ndirectory, so it will look like this</p>\n<pre><code>    $ ls -1 test\n    FileSaver\n    dbind\n    dgrid\n    dijit\n    dojo\n    dojox\n    *index.html*\n    jDataView\n    jasmine-core\n    jbrowse\n    json-schema\n    jszlib\n    lazyload\n    put-selector\n    *run-jasmine.js*\n    *spec*\n    *travis.conf*\n    util\n    xstyle\n</code></pre>\n<p>Here the asterisks indicate things that are part of our app, other's are\nautomatically installed by bower (jbrowse, jasmine-core, the dojo\ndependencies, and other things)</p>\n<p>Then we can create the jasmine test/index.html to be something like this</p>\n<pre><code>    &#x3C;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"\n      \"http://www.w3.org/TR/html4/loose.dtd\">\n\n\n      &#x3C;meta>\n      Jasmine Spec Runner\n\n      &#x3C;link rel=\"stylesheet\" href=\"jasmine-core/lib/jasmine-core/jasmine.css\">&#x3C;script src=\"jasmine-core/lib/jasmine-core/jasmine.js\">&#x3C;/script>&#x3C;script src=\"jasmine-core/lib/jasmine-core/boot.js\">&#x3C;/script>&#x3C;script type=\"text/javascript\" src=\"dojo/dojo.js\" data-dojo-config=\"async: 1\">&#x3C;/script>&#x3C;script type=\"text/javascript\">\n        require( { baseUrl: '.',\n                   packages: [\n                       'dojo',\n                       'dijit',\n                       'dojox',\n                       'jszlib',\n                       { name: 'lazyload', location: 'lazyload', main: 'lazyload' },\n                       'dgrid',\n                       'xstyle',\n                       'put-selector',\n                       'FileSaver',\n                       { name: 'jDataView', location: 'jDataView/src', main: 'jdataview' },\n                       { name: 'JBrowse', location: 'jbrowse/src/JBrowse' },\n                       { name: 'SashimiPlot', location: '../js' }\n                   ]\n        });\n      &#x3C;/script>&#x3C;script type=\"text/javascript\" src=\"spec/SashimiPlot.spec.js\">&#x3C;/script>&#x3C;div id=\"sandbox\" style=\"overflow:hidden; height:1px;\">&#x3C;/div>\n</code></pre>\n<p>The \"packages\" in the require statement puts all these packages in the\nright \"namespace\" for the AMD includes, and the \"specs\" are defined like\n<code>&#x3C;script type=\"text/javascript\" src=\"spec/Projection.spec.js\">&#x3C;/script></code></p>\n<p>Finally, run-jasmine.js is used to check the results of the jasmine tests (it\nis run via phantomjs in the travis-CI script). It is a special version for the\nmost recent version of jasmine (2.4)\n<a href=\"https://gist.github.com/vmeln/b6cbb319d9a0efc816be\">https://gist.github.com/vmeln/b6cbb319d9a0efc816be</a></p>\n<p>For an example of the project using this, see\n<a href=\"https://github.com/cmdcolin/sashimiplot\">https://github.com/cmdcolin/sashimiplot</a></p>"},{"title":"Creating a docker image","date":"2016-04-17","slug":"2016-04-17","html":"<p>Example</p>\n<pre><code>    brew install docker boot2docker docker-machine\n    docker-machine create --driver virtualbox default\n    docker-machine env default # will output some variables\n    eval \"$(docker-machine env default)\" # use those variables\n    # make dockerfile\n    docker build -t nameof-yourimage .\n</code></pre>"},{"title":"Basic command line productivity tricks and learning experiences","date":"2016-04-06","slug":"2016-04-06","html":"<ul>\n<li>dd deletes line in vim</li>\n<li>Ctrl+d scrolls down in vim</li>\n<li>Learn to love your package manager. Homebrew, NPM, gem, cpanm, gvm/sdkman,\netc. these all do amazing things</li>\n<li>Once you learn bash, try zsh and oh-my-zsh, they have things like\ncase-insensitive tab completion</li>\n<li>Don't make scripts that hardcode paths, make reusable command line scripts.\nUse bash as your \"REPL\", not R.</li>\n<li>git log -p helps analyze your log files in full details (make sure\nautocoloring is turned on in your terminal)</li>\n<li>There are keys to jump forward and backwards on the command line text editor,\nlearn them...don't scroll one char at a time</li>\n<li>Learn how \"PATH\" works. Generally it is just a set of directories connected\nby \":\" separators. You can add things to the path by saying \"export\nPATH=$PATH:/new/directory/to/add\" and you can add this to ~/.bashrc for\nexample</li>\n<li>When your install process for a command line tool seems like nonsense, try\nhomebrew instead. barring that, learn PATH, and how to run \"make install\",\netc. Most of your headbashing from installing programs is 90% can be\nexplained by not understanding how the developer is intending it to be used,\n10% of the tool's install process being wrong</li>\n<li>Get a static analyzer and basic tests going on your codebase and run it on\ntravis-ci. Getting started with travis-ci is kind of a learning curve, but it\nis worth it</li>\n<li>Use cpanm instead of cpan for package management</li>\n<li>Vocabulary learning curve: catalina is the same thing as tomcat.\nCATALINA_HOME is the same thing as the tomcat folder</li>\n<li>alias ll=\"ls -l\", because I type \"ll\" hundreds of times a day.</li>\n<li>For irc productivity, run irssi on a server in a \"screen\" e.g. \"screen irssi\"\nand then you can come back to conversations later by just logging into the\nserver with ssh</li>\n<li>Edit ~/.ssh/config to include your hostnames so you don't have to type out\nlong ssh\ncommands <a href=\"http://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/\">http://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/</a></li>\n<li>Use spaces instead of tabs in your source code (>:( yes I think this is the\none true way)</li>\n<li>Try out nodejs and browserify in your spare time to make a \"npm\" based app in\nthe browser. it's fun.</li>\n<li>Similarly, try making a simple \"api\" endpoint on the server side with\nexpress.js or similar. can get started very quickly.</li>\n<li>Learn how to get a mindset of writing tests. You can write tests proactively\n(i.e. Test driven development), but you can also write them \"reactively\" too\n(i.e. if have a bug that you fix, you can make a test to make sure this\ndoesn't happen anymore)</li>\n<li>Similar to above, tests in this sense are more \"sanity checks\" than they are\nformal proofs. Take \"assert\" logic and \"debugging\" code out of main codebase\nand put them in tests</li>\n<li>Minimize comments in your code, and also don't comment out code and leave it\npresent. Find a way to delete it and move on!</li>\n<li>When you have a bunch of .orig files after doing a git merge, just use git\nclean -f to get rid of them. Similarly, to get rid of everythng, including\nthings in your gitignore file (i.e. a super clean) use git clean -fdx. It has\na --exclude argument too</li>\n</ul>"},{"title":"Running nginx on containerised travis-CI pt 2","date":"2016-03-28","slug":"2016-03-28","html":"<p>There are several guides out there about how to setup nginx on travis-CI\nbut I still found it to be a challenge, especially finding a modern one\nthat works with the containerized builds. I was frustrated that things\nlike <code>SimpleHTTPServer</code> from python and http-server from npm did not have\nfully enough features to run our app either (a complex \"static-site\ngenerator\" type thing you might say), and I was also too lazy to setup\n\"sauce labs\" (which I have not used, but presume has some better ability\nto run functional/browser tests).</p>\n<p>Essentially, the problem with running nginx under the containerized\nbuild is that it \"likes to be sudo\", with many logfiles by default going\nto different places that only sudo has access to.</p>\n<p>This link is probably the most similar to the technique I use here, but\nit is now gone (?) and must be accessed through the internet archive!</p>\n<p><a href=\"https://web.archive.org/web/20150919050719/http://www.doublesignal.com/running-nginx-on-containerised-travis-ci\">http://www.doublesignal.com/running-nginx-on-containerised-travis-ci</a></p>\n<p>My technique is very similar, however I use an extra trick to set the\nfile root to the current directory (instead of /tmp/nowhere as in the\nlink) by using \"envsubst\" to replace variables in the nginx config file.</p>\n<p>Without further ado, the .travis.yml can look like this</p>\n<pre><code>    sudo: false\n    addons:\n      apt:\n        packages:\n        - nginx\n    install:\n      - cat tests/travis.conf | envsubst > tests/travis-envsubst.conf\n      - nginx -c `pwd`/tests/travis-envsubst.conf\n    script:\n      - wget http://localhost:9000/yourfiles\n</code></pre>\n<p>Then your nginx config file can look like this</p>\n<pre><code>    worker_processes 10;\n    pid /tmp/nginx.pid;\n\n    error_log /tmp/error.log;\n\n    events {\n        worker_connections 768;\n    }\n\n    http {\n        client_body_temp_path /tmp/nginx_client_body;\n        fastcgi_temp_path     /tmp/nginx_fastcgi_temp;\n        proxy_temp_path       /tmp/nginx_proxy_temp;\n        scgi_temp_path        /tmp/nginx_scgi_temp;\n        uwsgi_temp_path       /tmp/nginx_uwsgi_temp;\n\n        server {\n            listen 9000 default_server;\n\n            server_name localhost;\n            location / {\n                root $TRAVIS_BUILD_DIR;\n                index  index.html index.htm;\n            }\n            error_log /tmp/error.log;\n            access_log /tmp/access.log;\n        }\n    }\n\n</code></pre>\n<p>Then, when travis-CI is run, it uses envsubst to replace\n<code>$TRAVIS_BUILD_DIR</code> in the tests/travis.conf file, and then boots up\nnginx</p>"},{"title":"On over-reproducibility","date":"2016-03-05","slug":"2016-03-05","html":"<p>Recently, some posts were made by <a href=\"https://twitter.com/arjunrajlab%C2%A0about\">https://twitter.com/arjunrajlab about</a> how\nperhaps we are aiming at \"over-reproducibility\". I think this is interesting,\nand would generally agree that not everyone needs to achieve total automation\nof their whole pipeline, but I think the post does a lot of \"blaming your\ntools\" and disparaging good development practices with regards to version\ncontrol and figure generation.</p>\n<p>I think that the complaint that version control and automated figures are not\nfor everyone is probably true, but it is overgeneralizing a different problem.\nFor example, students are not \"trained\" to work with Git, and they are not\n\"trained\" to do software engineering. In fact, even computer science students\nare not generally \"trained\" to do any of those things (computer science !=\nsoftware engineering). But that doesn't mean that your lab needs to forego\nusing all those tools. Software development can be incredibly complex and\nsophisticated, but it's important to make sure things are \"done right\"!\nHigh-quality and easy-to-reproduce software is really about process, and\nengineering. But that is also why there is no one-true-way for\nreproducibility. Maybe Arjun doesn't have a reproducible workflow right now,\nbut what about 5 years down the road, where he suddenly has a great framework\nfor such things? This happens all the time in software development (for\nexample, how long ago was it that \"push to deploy\" did not exist? how often\nwould you just edit your files live on your site? now that is seen as bad\npractice!), but that said, processes for software quality can evolve pretty\norganically, so even though some best practices exist, people can grow their\nown quality environment.</p>\n<p>Even if we agree that software development+version control=good, there are\nstill a lot of complaints about it in the blogpost. For example, the complaint\nthat git is too hard is pretty silly, and the xkcd comic about calling over\ngraph theorist doesn't really help. As a software developer at work, I think\nthat version control simply helps define a disciplined way of working. Version\ncontrol makes you analyze your progress, summarize it as a commit message,\nformat the code properly, make sure it passes tests, and then talk to your\ncollaborators about accepting it. Dropbox might accomplish some of those\nthings, but I would really doubt that it is covering that full scope. Arjun\nseems to agree with using version control for some of his labs software\ndevelopment, so again, there is a spectrum of needs being met. Nevertheless,\nthere are some weird comments about whether commit messages are like a \"lab\nnotebook\", but hint: they are not, write documentation for your project or keep\na separate journal or blog or wiki. Commit messages in my opinion should be\nabout one line, and the changes should be very self explanatory. But another\nbig argument in the blogpost is whether version control works for something\nlike paper writing, and I believe that this underscores something else: that\npaper writing is really a pretty messy procedure.</p>\n<p>I think that perhaps the \"google docs\" mode of writing is probably pretty ok\nfor many things, but it still needs a gatekeeper to incorporate the comments\nfrom coauthors and reviewers into the document in an organized way. In my\nexperience as a \"gatekeeper\" with writing my senior thesis, I organized my\npaper using knitr, and I automated figures being generated by R wherever\npossible, and then I would convert the paper to .docx to share with my\nadvisors. Then I would take their comments on the .docx and incorporate it back\ninto my paper. This could be seen as burdensome (\"why not just use google\ndocs\"), but I felt that it was a good way to incorporate review into a\nreproducible workflow.</p>\n<p>Now, my pipeline precludes your PI from having to learn git to make a pull\nrequest on your paper. That's a good thing... and we still have\nreproducibility.  But what about the figures themselves? I said I had knitr for\nreproducible figures, but what about everyone else? I think figures have high\nvalue, and so people might want to have more reproducibility invested in them.\nIn the blog post, it was claimed that making \"complex\" pub-quality figures was\ndifficult (i.e. the plea for Adobe Illustrator), but look at the annotation\nfunctions from ggplot2, and multifaceted images. I found these annotation\nfunctions to be very easy to pick up. There is also the on-going debate about\nggplot2 vs base graphics on the simplystatistics blog, which covers making\npublication quality figures, and last I checked, I think the ggplot2′ers were\nwinning. I don't know how it works in high profile journals like Nature,\nbecause it looks like they just re-do all the figures to make them have some\nconsistent style, but that doesn't mean your original figure should be\nirreproducible.</p>\n<p>The debate about reproducible figures is pretty tangible too in things like\nmicroscopy images. Simply look at the large amount of discussion from pubpeer\nabout image fraud and possible duplications. The pubpeer community obviously\nhas some pretty sophisticated tools for hunting out possibly manipulated\nmicroscopy images. These types of things also lead to investigations, and you\ncan see in the high-profile retraction case over STAP cells that it looks like\nthe investigating committee were simply asking how some figures were made, and\nupon finding that lab members don't know, a paper was retracted. The\nRetractionWatch blog covers these\ninvestigations <a href=\"http://retractionwatch.com/2016/02/26/stap-stem-cell-researcher-obokata-loses-another-paper/\">http://retractionwatch.com/2016/02/26/stap-stem-cell-researcher-obokata-loses-another-paper/</a></p>\n<p>You can't depend on other people to back your figure up, so you need to take\nresponsibility for making sure your papers and your work are reproducible (and,\nthere is a spectrum for reproducibility, but I believe that version control is\na great example of highly disciplined work). I also think that just having\nfolders on some hard drive is not a good way to do things either. There is a\nsaying in software development that is \"if it's not in version control, it\ndoesn't exist\". That's not to say that version control is for everything, big\ndata obviously has trouble with being stored in git. But that shouldn't block\nyou from creating reproducible analyses.</p>\n<p>Another example from the over-reproducibility blogpost says that if you have\n\"analysis1\" and \"analysis2\", then version control advocates would tell you to\ndelete analysis1 and just remember that it is in your history. I think that\nthis is just a different issue. If you actually care about both analyses, just\nmake them separate repositories, with basic README.md files explaining each\nthem, and stop worrying about it. Having one repository containing too many\nmiscellaneous scripts is actually an anti-pattern. Stop making repositories\ncalled \"bioinfo-scripts\" that just contain a mish-mash of analysis scripts!\nMake your work purpose driven and do tasks. Also, this is an argument against\nREPL tools: your R REPL history is not a reproducible script. Make your code\ninto a script that generates well defined outputs. Windows users: you might not\nunderstand this because the command line on windows is crippled, but you have\nto make things run on the command line.</p>\n<p>Now I wish I could say that I live by my words, but having been involved in\ncoauthoring several papers, I will just have to admit that it is really a messy\nprocedure despite my best intentions as an editor and coauthor. I wish things\nwould be better!</p>\n<p>On over-reproducibility: there is no such thing! There are pretty good\narguments to really automate most of a process, especially if it is done\nrepeatedly, to remove human errors, because meat-machines genuinely do things\nwrong all the time.</p>\n<p>And, as my parents would say around the dinner table: \"you can always have\nmore, but you can never have less\"...so, you're not going to get to a point of\nover-reproducibility. We shouldn't cargo cult it as the only way to do science\nbut it's not a bad thing to have.</p>"},{"title":"Cheating in your computer science class by copying from stackoverflow","date":"2015-12-17","slug":"2015-12-17","html":"<p>I would like to tell a story about how I provided some personal tutoring help\nfor a friend in a computer science class, and talk about a nagging feeling that\nreally felt wrong for me.</p>\n<p>So, a long time ago, in a land far far away, a friend took an intermediate\nclass on C++. I was first updated on his progress when he emailed me to get\nsome help with some compiler errors. I was happy to help the young padawan.\nHere was the error:</p>\n<pre><code>         test.cpp:42:43: error: non-ASCII characters are not allowed outside of literals\n                and identifiers\n              for (startScan = 0; startScan &#x3C; (size − 1); startScan++)\n                                                    ^~\n</code></pre>\n<p>Now, what does this say to you? For me, it was actually very clear what the\nerror meant. It simply meant that this code was taken from somewhere, and\ncopied and pasted into the compiler. I know that because if they had typed it\nthemself, they definitely would not get this error, because it is the error\nthat implies something was automatically converted to a unicode dash, mostly\nsomething done during copying and pasting. At this point, I just kind of\nlaughed, and helped him fix that. I showed how the compiler is actually pretty\nsmart and can help fix these errors and then I said \"l8r dude\".</p>\n<p>The next week, I had another skype meeting with him, and this time I wanted to\nhelp a little more. It was pretty clear when we started that he was using code\nthat was copied and pasted again. I said, \"uh, ok,....I'm not sure we need that\nnow, but let's just keep going\", and then I sat down and started helping. I\nwanted to help get all the details of the program working, so I helped guide\nthe solution. Each time we needed to test the program, it required repeating\nsome input lines via <code>cin >></code>, which is really annoying (obviously, you should\ntest your code with unit tests, but universities don't teach that, a rant for\nanother day). Anyways, it took awhile, because coding really does just take\ntime, but in the end he finally got it fixed and I said great job, and he\nturned it in!</p>\n<p>Now, on my friends last assignment, I got another call for help, and when we\nstarted skype, I found yet again that he had copied code from somewhere, which\nincluded a C++ class and a main function for doing binary trees. I just simply\nsaid \"dude, delete that, we don't need it\" and so he deleted it, but I think\nmaybe he had worked on this copied code for awhile, and maybe felt it was kind\nof his, so was apprehensive. I insisted though. Then we walked through the\nassignment again, very slowly. I spent probably 2-3 hours helping him out that\nnight. During those hours, I saw him continually making many programming\nmistakes such as just not knowing how to declare variable or a function\nproperly, or just not knowing what to do next. This was kind of frustrating!!!\nBut I wanted to absolutely teach him how to make it right! I was patient\nthough, and I wanted to teach a fun lesson, so I showed how you can do some\n\"unit tests\" which avoids having to constantly re-enter your data via `cin</p>\n<blockquote>\n<blockquote>\n<p>`....</p>\n</blockquote>\n</blockquote>\n<p>Now, the padawan completed his C++ class, and then we all were happy ever\nafter....but a disturbance in the force was sensed...\n<img src=\"http://zelcs.com/wp-content/uploads/2013/02/stackoverflow-logo-dumpster.jpg\" alt=\"image\">\nImage from <a href=\"http://zelcs.com/this-is-why-stackoverflow-sucks/\">http://zelcs.com/this-is-why-stackoverflow-sucks/</a></p>\n<p>I was reminded about all this due to seeing that <a href=\"http://meta.stackexchange.com/questions/271080/the-mit-license-clarity-on-using-code-on-stack-overflow-and-on-the-stack-excha\">StackOverflow is now changing\ntheir \"license\" over all the little snippets of\ncode</a>\nthat are posted on their site. It just makes me reflect on literally HOW OFTEN\nPEOPLE JUST COPY AND PASTE FROM THERE. They might understand what they are\ndoing, or they seriously might not!!! I think it is a real problem that people\nsometimes do not understand, but I cannot deny that it can be helpful too.</p>\n<p>If I reflect on education in general, I recall when I took a University level\nphysics class... it was really hard! We had to enter our validated solutions\nfor the math problems into a computerized website homework portal, and that\ninvolved being 100% correct about things. Now, what if there was just a\nphysicsoverflow, where they not only had Q&#x26;A, but they had \"programs\" that gave\nyou all the right answers to your homework problems that you could just copy\nand paste and use as solutions to your homework? This isn't even in the realm\nof asking for \"homework help\" anymore, this is just pure cheating if you can\ncopy your answers from somewhere. It is disappointing though because this is\nwhat people are doing in computer science!! These students are missing out on\nbasic understanding of code. !!thisIsNotOk();</p>\n<p>Now, at least when I was being a tutor for my friend, I felt like my advice\nhelped my friend learn some things, not just give answers. But what if I was\nnot there? I guess there is a certain \"impersonal quality\" that makes asking\nGoogle/StackOverflow for answers less like conventional \"cheating\", but that is\nstill wrong. I think it would be good if more expert knowledge was available\nfor all people, and not just copy and paste snippets. As a start, I thought\nthat <a href=\"http://cacm.acm.org/magazines/2015/10/192385-life-after-moocs/fulltext\">this post by Philip Compeau and Pavel\nPevzner</a>\n(who teach a Bioinformatics Algorithms MOOC on Coursera) was very interesting,\nand I really liked their quote:</p>\n<p>\"Online education should move toward replicating the experience of receiving\none-on-one tutoring.\"</p>\n<p>That sounds great, but how can this be acheived? And how can it be done right?\nI think it really requires the student to \"learn how to learn\"</p>\n<p>If I think back to a long time ago, I remember being in 4th or 5th grade and I\ndid a book report on World War 1, and I went to the library. I remember\ndesperately flipping through pages of a 100 page book to try to find some\nsnippets of information to support some basic idea that I wanted to talk about.\nMaybe I wanted to know something specific, but the problem was that I wasn't\nREALLY READING THE BOOK! I probably could have had a better understanding of\nthe topic if I had just read it, or even a part of it, and asked for help, but\ninstead I just picked and chose snippets from the book to \"sound smart\". I am\nvery guilty of this type of error in many instances throughout my school\ncareer, so I am no saint! I even have a phrase to describe this style of\nlearning...I call it \"predatory learning\" and it is probably the worst kind of\nlearning style. Predatory learners often pick and choose from scraps of info,\nbut they never get a full meal!</p>"},{"title":"Killing postgres the hard way","date":"2015-10-22","slug":"2015-10-22","html":"<p>So today, I finally decided to do something about a query that we saw\nhad been running for 25 DAYS on our server</p>\n<p>Note: If you find this post and you need to follow the hard way, backup\nyour data first if possible.</p>\n<p>First I could obviously see the culprit: each postgress query runs it's\nown process so I could see in \"htop\" that there was this process that\nhad been running for 600 hours, or about 25 days</p>\n<p>Next, I opened a psql console and ran this query:</p>\n<pre><code>> SELECT datname,procpid,current_query FROM pg_stat_activity WHERE\n> datname='database_name' ORDER BY procpid ;\n</code></pre>\n<p>This returns which actual queries are being run on the database at any\ngiven time.  I could easily see the one problematic query being run,\nwhich was a badly constructed intermine template query that resulted in\na weird \"recursion\" essentially.</p>\n<p>I wanted to try just terminating this query itself, so I ran this</p>\n<pre><code>> SELECT pg_cancel_backend(29033);\n</code></pre>\n<p>Each time I ran it, it would say it returned one result but it did\nnothing.</p>\n<p>I also read that you can try to nicely \"kill\" it from the command line\n(no kill -9) so I ran</p>\n<pre><code>> kill 29033\n</code></pre>\n<p>This also did not work!</p>\n<p>I thought perhaps all these problems were because tomcat was still\nactive, so we shut down tomcat, and retried killing the specific query,\nbut to no avail</p>\n<p>At this point, I just wanted to restart the whole database server. Kind\nof a risky move... but I am sort of a risky kind of guy...(that is not a\ngood thing with databases). If you are doing this, make backups! I\ndidn't. Luckily I suffered no data loss but what follows is kind of\nintense.</p>\n<p>So first, I try and stop the database service</p>\n<pre><code>> service postgresql-9.1 stop\n</code></pre>\n<p>Unfortunately, this <code>[FAILED]</code> ! And of course, even though it failed,\nthe database is now unusable. No logging into it anymore, we have to go\nwith the hard way now...</p>\n<p>Looking at /etc/init.d/postgres-9.1 told me that the service stop\ncommand was effectively using something like this:</p>\n<pre><code>> pg_ctl -D /db/postgres/data -m fast stop\n</code></pre>\n<p>After some reading, I learned that you can try using a slightly\ndifferent flag to restart it</p>\n<pre><code>> pg_ctl -D /db/postgres/data -m immediate  stop\n</code></pre>\n<p>I ran this and to my horror/surprise, it actually worked! At this point\nI decided to start postgresql back up again!</p>\n<pre><code>> service postgresql-9.1 start\n</code></pre>\n<p>The service start quickly returned a SUCCESS, which was great, but then\nI tried to start a psql console and the console froze on me! I could not\neven ctrl+c it!</p>\n<p>I got really worried at this point and I looked at the process manager,\nand saw that there was one postmaster process running but it was not\nclear what it was doing. I actually tried to shutdown the server again\nin a panic mode but at this point it said</p>\n<pre><code>> /usr/pgsql-9.1/bin/pg_ctl stop -D /db/postgres/data/ -m immediate\n> waiting for server to shut\n> down...............................................................\n> failed\n</code></pre>\n<p>It was probably good that it didn't shut down, because I would quickly\nfind out that it was in recovery mode.  I looked at the postgresql logs\nand I saw this, reproduced here for full detail (from before the\nshutdown to the restart)</p>\n<pre><code>> WARNING:  pgstat wait timeout\n> WARNING:  pgstat wait timeout\n> WARNING:  pgstat wait timeout\n> WARNING:  pgstat wait timeout\n> WARNING:  pgstat wait timeout\n> WARNING:  pgstat wait timeout\n> WARNING:  pgstat wait timeout\n> WARNING:  pgstat wait timeout\n>\n> ERROR:  canceling statement due to user request\n> STATEMENT:  CREATE TABLE precomp_90519 AS SELECT DISTINCT a1_.id AS\n> a1_id, a2_.id AS a2_id, a3_.id AS a3_id, a4_.id AS a4_id,\n> a5_.id AS a5_id, a6_.id AS a6_id, a12_.id AS a12_id, a10_.id AS\n> a10_id, a1_.id AS a13_, a1_.primaryIdentifier AS a14_,\n> a1_.secondaryIdentifier AS a15_, a2_.type AS a16_, a3_.name AS\n> a17_, a4_.primaryIdentifier AS a18_, a5_.primaryIdentifier AS\n> a19_, a6_.shortName AS a20_, a12_.identifier AS a21_, a10_.code\n> AS a22_ FROM Gene AS a1_, Homologue AS a2_, Organism AS a3_, Gene\n> AS a4_, Gene AS a5_, Organism AS a6_, GOAnnotation AS a7_,\n> GOEvidence AS a8_, OntologyTerm AS a9_, GOEvidenceCode AS a10_,\n> OntologyAnnotation AS a11_, OntologyTerm AS a12_, GeneGoAnnotation\n> AS indirect0, EvidenceGOAnnotation AS indirect1 WHERE a1_.id =\n> a2_.geneId AND a1_.organismId = a3_.id AND a2_.geneId = a4_.id\n> AND a2_.homologueId = a5_.id AND a5_.organismId = a6_.id AND\n> a1_.id = indirect0.Gene AND indirect0.GoAnnotation = a7_.id AND\n> a7_.id = indirect1.GOAnnotation AND indirect1.Evidence = a8_.id AND\n> a7_.ontologyTermId = a9_.id AND a8_.codeId = a10_.id AND a9_.id =\n> a11_.ontologyTermId AND a11_.ontologyTermId = a12_.id ORDER BY\n> a1_.primaryIdentifier, a1_.secondaryIdentifier, a2_.type,\n> a3_.name, a4_.primaryIdentifier, a5_.primaryIdentifier,\n> a6_.shortName, a12_.identifier, a10_.code, a1_.id, a2_.id,\n> a3_.id, a4_.id, a5_.id, a6_.id, a12_.id, a10_.id\n> LOG:  could not send data to client: Broken pipe\n> STATEMENT:  CREATE TABLE precomp_90519 AS SELECT DISTINCT a1_.id AS\n> a1_id, a2_.id AS a2_id, a3_.id AS a3_id, a4_.id AS a4_id,\n> a5_.id AS a5_id, a6_.id AS a6_id, a12_.id AS a12_id, a10_.id AS\n> a10_id, a1_.id AS a13_, a1_.primaryIdentifier AS a14_,\n> a1_.secondaryIdentifier AS a15_, a2_.type AS a16_, a3_.name AS\n> a17_, a4_.primaryIdentifier AS a18_, a5_.primaryIdentifier AS\n> a19_, a6_.shortName AS a20_, a12_.identifier AS a21_, a10_.code\n> AS a22_ FROM Gene AS a1_, Homologue AS a2_, Organism AS a3_, Gene\n> AS a4_, Gene AS a5_, Organism AS a6_, GOAnnotation AS a7_,\n> GOEvidence AS a8_, OntologyTerm AS a9_, GOEvidenceCode AS a10_,\n> OntologyAnnotation AS a11_, OntologyTerm AS a12_, GeneGoAnnotation\n> AS indirect0, EvidenceGOAnnotation AS indirect1 WHERE a1_.id =\n> a2_.geneId AND a1_.organismId = a3_.id AND a2_.geneId = a4_.id\n> AND a2_.homologueId = a5_.id AND a5_.organismId = a6_.id AND\n> a1_.id = indirect0.Gene AND indirect0.GoAnnotation = a7_.id AND\n> a7_.id = indirect1.GOAnnotation AND indirect1.Evidence = a8_.id AND\n> a7_.ontologyTermId = a9_.id AND a8_.codeId = a10_.id AND a9_.id =\n> a11_.ontologyTermId AND a11_.ontologyTermId = a12_.id ORDER BY\n> a1_.primaryIdentifier, a1_.secondaryIdentifier, a2_.type,\n> a3_.name, a4_.primaryIdentifier, a5_.primaryIdentifier,\n> a6_.shortName, a12_.identifier, a10_.code, a1_.id, a2_.id,\n> a3_.id, a4_.id, a5_.id, a6_.id, a12_.id, a10_.id\n> LOG:  unexpected EOF on client connection\n> LOG:  unexpected EOF on client connection\n> LOG:  unexpected EOF on client connection\n> LOG:  unexpected EOF on client connection\n> LOG:  received fast shutdown request\n> LOG:  aborting any active transactions\n> LOG:  autovacuum launcher shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> FATAL:  the database system is shutting down\n> LOG:  received immediate shutdown request\n> WARNING:  terminating connection because of crash of another server\n> process\n> DETAIL:  The postmaster has commanded this server process to roll back\n> the current transaction and exit, because another server process\n> exited abnormally and possibly corrupted shared memory.\n> HINT:  In a moment you should be able to reconnect to the database and\n> repeat your command.\n> LOG:  received fast shutdown request\n> LOG:  database system was interrupted; last known up at 2015-10-22\n> 15:47:43 CDT\n> LOG:  received immediate shutdown request\n> LOG:  database system was interrupted; last known up at 2015-10-22\n> 15:47:43 CDT\n> LOG:  database system was not properly shut down; automatic recovery\n> in progress\n> LOG:  record with zero length at BBD/1CC2F0C0\n> ...\n</code></pre>\n<p>You can see all the weird activity that was done here</p>\n<ul>\n<li>first the attempt to \"canceling statement due to user request\" did not work</li>\n<li>then the database stop using -m fast</li>\n<li>then the database stop using -m immediate</li>\n<li>the restart (with the HINT, should be ready soon)</li>\n<li>the panic mode where i tried to shut it again anyways</li>\n</ul>\n<p>During the recovery period, I was still very concerned about the\ndatabase was doing, so I used \"strace\" to look at the main postmaster\nprocess.</p>\n<p>I was pleasantly surprised to see that the postmaster process was just\ncleaning up files in /db/postgres/data/base/pgsql_tmp/, I could see the\nfile system \"unlink\" command with successful status codes.</p>\n<p>There were about 150 large files in /db/postgres/data/base/pgsql_tmp/,\nand I waited about an hour for them to be deleted, and after that, the\npostgresql log file said it was ready, and indeed, it was perfect :)</p>\n<pre><code>> LOG:  redo is not required\n> LOG:  database system is ready to accept connections\n> LOG:  autovacuum launcher started\n</code></pre>\n<p>What a relief!</p>\n<p>I hope this might help any wayward stragglers to see how the postgresql\nrestart process works. Sometimes things don't shut down cleanly, but I\nthink it is still good to know some alternative steps to kill -9</p>"},{"title":"Tomcat memory debugging","date":"2015-10-15","slug":"2015-10-15","html":"<p>In my previous posts, I speculated about the issues that were causing\nserver lag and CPU usage spiking with\ntomcat: <a href=\"https://cmdcolin.github.io/posts/2015-09-16\">https://cmdcolin.github.io/posts/2015-09-16</a></p>\n<p>Unfortunately, I was completely wrong in my speculations, but we\nincreased tomcat memory limits so that the entire Lucene search index\ncould fit in memory, which was able to fix the spiky CPU problems.</p>\n<p>Luckily, fixing the memory issues had very good implications for our\nwebapp:</p>\n<p>I have a cron job uses a simple curl command to grab different pages on\nthe website, and then it logs the time taken to a output file. I charted\nthese output times, before and after we increased the memory limits of\ntomcat, and it turned out that the response time of the webapp was\ndramatically improved by this change.</p>\n<p><img src=\"/media/131229569383_0.png\" alt=\"\"></p>\n<p>Figure 1. The webapp response time was extremely variable before the\nredeploy on Oct 2nd where we increased tomcat's memory allocation, which\nthereafter dramatically improved the response time.</p>\n<p>Clearly, the webapp response time was being severely compromised by the\nmemory issues.</p>\n<p>In response to all of these issues, I also added GC logging to the\ntomcat configuration so that I can see if the GC is correlated with\nthese webapp response time. Figure 2 shows how high GC activity is\ncorrelated with longer webapp response times, but note that this figure\nwas made after the other memory allocation problems were fixed, so it is\nstill much better than the problems we had in the past.</p>\n<p><img src=\"/media/131229569383_1.png\" alt=\"\"></p>\n<p>Figure 2. After increasing the memory, you can see webapp response time\nis much better, except if the GC activity becomes very high, and then\nthis increases the response time.</p>\n<p>Edit: Bonus screenshot, seemingly each friday we get a majoy activity\nburst that triggers GC activity!</p>\n<p><img src=\"/media/131229569383_2.png\" alt=\"\"></p>\n<p>Figure 3. Crazy Java GC activity on a friday night, but the app seems to\nrecover from it</p>\n<h2>Conclusion</h2>\n<p>Increasing the memory allocation to java and tomcat allows the entire\nsystem to perform much better. If you can afford to get more memory to\nallocate to tomcat, then it's probably a good idea.</p>\n<p>Also, tracking your webapp response times will help you see if your\nchanges are having a good effect. I made this a script for graphing log\noutputs here <a href=\"https://github.com/cmdcolin/loggraph\">https://github.com/cmdcolin/loggraph</a></p>\n<p>PS:</p>\n<p>If your tomcat is running as the tomcat user, then it can be difficult\nto debug the memory problems simply with the \"get heap dump\" from\njvisualvm, because the permissions will be wrong. To fix this, try using\na privileged user to run the jmap command:</p>\n<pre><code>runuser -l tomcat -c \"/usr/java/latest/bin/jmap-dump:format=b,file=/db/tomcat/tomcat.dump 25543\"\n</code></pre>"},{"title":"Fixing spiky CPU issues and unresponsiveness with Tomcat","date":"2015-09-16","slug":"2015-09-16","html":"<p>The symptoms of spiking and the server was lagging after light usage of the\napplications, the CPU usage would start spiking and rapidly cycle from many CPU\ncores (e.g. 2000% CPU usage) back to 0% CPU for no apparent reason.</p>\n<p>We now know this was due to memory issues and garbage collection, but it was\nconfusing because it wasn't strictly showing up as GC usage in JVisualVm (the\nGC usage, blue spikes on the left in fig 1, are small, but the orange spikes\nare large, even though the memory issues are the problem)</p>\n<p>Here is what it looked like during spiking (obviously, pushing the memory\nlimits here, a linked in article suggests having 6GB of \"newgen\" memory, so on\ntop of the old gen, tomcat needs a bunch more for the newgen to make things\nhappy.</p>\n<p><img src=\"/media/129241954103_0.png\" alt=\"\"></p>\n<p>Here is what it looks like when it is not spiking</p>\n<p><img src=\"/media/129241954103_1.png\" alt=\"\"></p>\n<p>Edit: See this follow up post for showing that increasing memory helps\n<a href=\"https://cmdcolin.github.io/posts/2015-10-15\">https://cmdcolin.github.io/posts/2015-10-15</a></p>"},{"title":"Weekend project - graphing tumblr reblogs using cytoscape.js","date":"2015-08-30","slug":"2015-08-30","html":"<p>In the past, I made an app that used RStudio's Shiny platform to plot\nnetwork graphs with RGraphviz. This worked, and gave some nice results,\nbut when I found out about cytoscape.js, I really wanted to try that\nout.</p>\n<p>The app is designed to plot tumblr reblogs, so it has a tree structure,\nbut simply plotting things as a tree is not very space efficient (as in,\nthe visualization takes up too much space). Therefore, using different\ntypes of layouts can really help.</p>\n<p>In my first app with graphviz\n<a href=\"https://colindiesh.shinyapps.io/tumblrgraph\">https://colindiesh.shinyapps.io/tumblrgraph</a>, there are several\nbuilt-in graph layouts including \"neato\" \"twopi\", \"circo\", and \"dot\"</p>\n<p>I made all of these available for users to try in the Shiny app. The\nnames of the layouts don't lend much to their behavior, but they are\nbuilt-in functions in Graphviz. There are both \"tree\" and\n\"force-directed\" style graph views. As I mentioned, the \"tree\" style\nview make a lot of sense for the tumblr reblogs, but the force directed\ngraphs are also a lot more compact, so offering both styles is useful.</p>\n<p><img src=\"../../media/128000908903_0.png\" alt=\"\"></p>\n<p>Figure 1. My default example graph from graphviz using the twopi layout.</p>\n<p>I wanted to replicate all the features that I had in the Graphviz app in\nCytoscape.js. Here is the breakdown of the basic components that needed\nreplicating:</p>\n<ol>\n<li>\n<p>Build the \"graph\" representation of reblogs in memory</p>\n</li>\n<li>\n<p>Add user forms and configurability</p>\n</li>\n<li>\n<p>Add color for distance from root using a breadth first search</p>\n</li>\n<li>\n<p>Draw the graph</p>\n</li>\n</ol>\n<p>As I went along, I was happy to learn that the concepts mapped very\neasily to javascript and cytoscape.js. The implementations are a little\ndifferent, but it worked out very nicely.</p>\n<p><img src=\"../../media/128000908903_1.png\" alt=\"\"></p>\n<p>Figure 2. Same data plotted in Cytoscape.js with the springy layout.</p>\n<p>In the new app, we enabled several different layouts similar to the\nGraphviz app too. In cytoscape.js, the layouts that are offered\ninclude \"arbor\", \"springy\", \"cola\", \"cose\", and \"dagre\". I like \"cola\"\nbecause it really looks like bubbles moving around in a soda. Others are\nworth experimenting with too.</p>\n<p><img src=\"../../media/128000908903_2.png\" alt=\"\"></p>\n<p>Figure 3. A Cytoscape.js springy layout for a larger tumblr reblog graph</p>\n<p>The new cytoscape.js app also has a nice animation feature. The old\ngraphviz app offered animation too (using Yihui's animation library for\nR) but the new version can automatically encode HTML5 video on the\nclient side from individual picture frames in the browser using\n<a href=\"https://github.com/antimatter15/whammy\">\"Whammy\"</a>! This quite\nimpressive!</p>\n<p>So to animate the graph, what is done is</p>\n<ol>\n<li>\n<p>Add nodes/edges and layout the graph (the simulation time is\nconfigurable, because allowing the user to interact with the graph while\nthe simulation is running is useful)</p>\n</li>\n<li>\n<p>Once layout is complete, the user can save the graph as an\nanimation, which first hides all nodes by adding visibility: hidden to\nthe CSS.</p>\n</li>\n<li>\n<p>Then the nodes are re-shown one-by-one, preserving the layout, and a\nframe is saved by the renderer at each step (takes a snapshot of the\ncanvas).</p>\n</li>\n</ol>\n<p>This strategy for the animation is actually better than the original\ngraphviz version that I had because the layout is only done once, which\nis time saving and it is also more consistent (the layout changes a lot\nif you re run it on different sets of nodes).</p>\n<p>Check out the app here <a href=\"http://cmdcolin.github.io/tumblrgraph2/\">http://cmdcolin.github.io/tumblrgraph2/</a></p>\n<p>Future goals:</p>\n<ul>\n<li>Test out super large graphs (I have tested up to about 500 reblogs\nbut after this, around 1000 reblogs, it slows down a lot and produces\nbad layouts. Needs fixing)</li>\n<li>Test out ability to place importance on certain nodes by increasing\nnode size based on it's degree</li>\n</ul>\n<p>Check out an example of the HTML5 video here</p>"},{"title":"Creating high-resolution screenshots (of jbrowse) with phantomJS","date":"2015-03-02","slug":"2015-03-02","html":"<p>Generating screenshots that are of high quality can be a great benefit\nfor things like science publications. PhantomJS is great for automating\nthis in a reproducible way. While many HTML pages can be rendered in\nhigh resolution without modification, HTML5 canvas apps need special\nconsiderations (see this <a href=\"http://searchvoidstar.tumblr.com/post/86542847038/high-dpi-rendering-on-html5-canvas-some-problems\">previous post on the\ntopic</a>).</p>\n<p>One of the key things that we noticed when we developed the high\nresolution canvas rendering (see above link) is that the\n\"devicePixelRatio\" can increase based on the browser's zoom level, and\nit can also take fractional values. This was a difficult problem, to\nmake rendering 100% consistent under all devicePixelRatio values, so we\ncreated a config parameter called highResolutionMode to accept arbitrary\nresolutions.</p>\n<p>Later, we learned about PhantomJS and how it can be used for creating\nscreenshots, it was clear that our design for the settings arbitrary\nscaling factors for the HTML5 canvas was very helpful, as we can set\nhighResolutionMode=4 along with the phantomJS variable\npage.zoomFactor=4, which matches the resolutions and creates high-res\ncanvas screenshots.</p>\n<p>One of the reasons that this is important is that it doesn't look like\nPhantomJS allows \"devicePixelRatio\" to be emulated, so the\npage.zoomFactor doesn't necessarily set the devicePixelRatio to a higher\nnumber, so being able to set the the arbitrary high resolution canvas\nscalings ourselves is a good solution. Reference: issue open Jan 2013\n<a href=\"https://github.com/ariya/phantomjs/issues/10964\">https://github.com/ariya/phantomjs/issues/10964</a> and we are now in Aug\n2015</p>\n<p>Here are some examples of the rendering process.</p>\n<h2>Examples</h2>\n<ol>\n<li>\n<p>Rendering screenshots to PNG</p>\n<p>phantomjs rasterize.js\n\"<a href=\"http://localhost/jbrowse/?data=sample_data/json/volvox&#x26;tracklist=0\">http://localhost/jbrowse/?data=sample_data/json/volvox&#x26;tracklist=0</a>\"\noutput.png \"3800px*1600px\" 2</p>\n<p><a href=\"http://i.imgur.com/ABLo6WJ.png\"><img src=\"http://i.imgur.com/ABLo6WJ.png\" alt=\"\"></a></p>\n<p>Figure 1. A basic image output from phantomJS. It uses a\nzoomFactor=2 on the command line to match highResolutionMode=2 in\nthe config file. `</p>\n</li>\n<li>\n<p>Rendering screenshots to PDF. In JBrowse, this requires PhantomJS\n2.0. Also see footnote.</p>\n<p>phantomjs rasterize.js\n\"<a href=\"http://localhost/jbrowse/?data=sample_data/json/volvox&#x26;tracklist=0\">http://localhost/jbrowse/?data=sample_data/json/volvox&#x26;tracklist=0</a>\"\noutput.pdf \"16in*8in\"</p>\n<p><a href=\"https://www.dropbox.com/s/7pceo4o406dys8s/output.pdf?dl=0\">Dropbox PDF\n906kb</a></p>\n<p>Figure 2. Outputted PDF from phantomJS. This still requires setting\nthe configuration such as highResolutionMode=2 too</p>\n</li>\n</ol>\n<h2>Conclusion</h2>\n<p>In the future, we want to consider adding highResolutionMode to be\nspecified via the URL so that it doesn't need to be changed\nmanually, although, setting highResolutionMode=2 by default is not a\nbad strategy.</p>\n<p><strong>Footnote</strong></p>\n<p>I used the following patch for rasterize.js to help \"fill out\" the\npage space in PDF renderings (otherwise, it is a square page, not\nsuper pretty for a widescreen app). I guess rasterize.js is really\njust a template and not meant to be super multi-purposed, so this\ncustom modification helps for our case.</p>\n<pre><code>\n        diff --git a/examples/rasterize.js b/examples/rasterize.js\n        index b0e0f67..3b0b6e4 100644\n        --- a/examples/rasterize.js\n        +++ b/examples/rasterize.js\n        _@@ -14,6 +14,7 @@ if (system.args.length &#x3C; 3 || system.args.length > 5) {\n            page.viewportSize = { width: 600, height: 600 };\n            if (system.args.length > 3 &#x26;&#x26; system.args[2].substr(-4) === \".pdf\") {\n                size = system.args[3].split('_');\n\n        +       page.viewportSize.width *= parseInt(size[0])/parseInt(size[1]);\n                page.paperSize = size.length === 2 ? { width: size[0], height: size[1], margin: '0px' }\n\n</code></pre>\n<p><strong>Reference</strong></p>\n<p><a href=\"https://gmod.org/wiki/JBrowse_Configuration_Guide#Rendering_high_resolution_screenshots_using_PhantomJS\">https://gmod.org/wiki/JBrowse_Configuration_Guide#Rendering_high_resolution_screenshots_using_PhantomJS</a></p>\n<p><strong>Comparison</strong></p>\n<p><img src=\"/media/112494997473_0.png\" alt=\"image\"></p>\n<p>Big improvement on font rendering</p>"},{"title":"Post graduation survey","date":"2015-02-01","slug":"2015-02-01","html":"<p>I recently received some post-graduation survey results from my class of\n2013 about salaries, job satisfaction, and other things. I thought I'd\ntry to visualize the data using R and ggplot2 as an exercise.</p>\n<p><a href=\"http://i.imgur.com/5rVnQHC.png\"></a></p>\n<p><img src=\"/media/109823235838_0.png\" alt=\"\"></p>\n<p>Figure 1. The fancy ggplot2 graph of salaries with standard deviation\nbars comparing salaries of BS/MS grads (red) with BS grads (blue).</p>\n<p>As a CS grad, I suppose I'm happy to see that we have the a highest\naverage salary right out of the gate. CS also has a high standard\ndeviation which I thought was interesting. Perhaps CS majors work in a\nmyriad of fields that demand computational skills where other\nengineering majors may be more focused on certain types of fields,\ngiving less deviation.</p>\n<p>In the process of making this graph, I was looking for how to do the\nside-by-side bar charts in ggplot and ended up supplying a \"correction\"\nto a answer on crossvalidated, a stackexchange site. The correction\nentailed how the syntax for using reshape2 vs reshape has changed\nslightly, so hopefully that helps other people searching for the same\nissue.</p>\n<p>Here is the code for processing</p>\n<pre><code class=\"language-R\"> library(xlsx)\n library(ggplot2)\n library(reshape2)\n\n salaries=read.xlsx(\"workbook.xlsx\",1)\n df=melt(salaries,measure.vars = c(\"BS.MS.annual.salary\",\n \"BS.annual.salary\"))\n #awkward step to merge standard deviations\n df[df$variable==\"BS.MS.annual.salary\",\"stdev\"]=df[df$variable==\"BS.MS.annual.salary\",\"stdev.1\"]\n ggplot(df, aes(NA., value, fill=variable)) +\n      geom_bar(position=\"dodge\",stat=\"identity\") +\n      geom_errorbar(aes(ymin=value-stdev, ymax=value+stdev),\n position=position_dodge(width=0.9)) +\n      ggtitle(\"Salary for 2013 class of Engineering (2014 survey)\") +\n      xlab(\"Major\") +\n      ylab(\"Salary w/ stddev\")\n</code></pre>\n<p>Table pictured</p>\n<p><img src=\"/media/109823235838_1.png\" alt=\"\"></p>"},{"title":"High DPI rendering on HTML5 canvas - some problems and solutions","date":"2014-05-22","slug":"2014-05-22","html":"<p>Recently our code has been moving towards the use of HTML5 canvas, as it has\nmany benefits. I felt that if we were going to keep this going towards canvas,\nthe rendering needed to match the quality of regular HTML based tracks.\nUnfortunately, the HTML5 canvas by default looks very \"fuzzy\" on a high\nresolution display (Figure 1).</p>\n<p><img src=\"/media/86542847038_0.jpg\" alt=\"\"></p>\n<p><em>Figure 1.</em> An example of really bad font rendering before and after enabling\nhigh resolution on the HTML5 canvas.</p>\n<p>**Background **</p>\n<p>Major credit goes to the tutorial at\n<a href=\"http://www.html5rocks.com/en/tutorials/canvas/hidpi/\">http://www.html5rocks.com/en/tutorials/canvas/hidpi/</a> for pioneering this!\n The html5rocks tutorial, written in 2010 it still remains relevant. The major\nthing it introduces is these browser variables called devicePixelRatio and\nbackingStoreRatio that can be used to adjust your canvas drawing. In my\ninterpretation, these two variables have the following purpose:</p>\n<p><em>devicePixelRatio</em></p>\n<p>On high DPI displays, screen pixels are actually abstracted away from the\nphysical pixels, so, when you create some HTML element with width 100, height\n100, that element actually takes up a larger number of pixels than 100x100. The\nactual ratio of the pixels that it takes up is 100<em>devicePixelRatio x\n100</em>devicePixelRatio. On a high DPI platform like Retina, the devicePixelRatio\nis normally 2 at 100% zoom.</p>\n<p><em>backingStoreRatio</em></p>\n<p>The backing store ratio doesn't seem to change as much from platform to\nplatform, but my interpretation of this value is that it essentially gives the\nsize of the memory buffer for the canvas. On my platform, the backingStoreRatio\nis \"1\". I think this value had more historical use, but it may not really be\nused anymore (update aug 7th, 2015 deprecated?\n<a href=\"http://stackoverflow.com/questions/24332639/why-context2d-backingstorepixelratio-deprecated\">http://stackoverflow.com/questions/24332639/why-context2d-backingstorepixelratio-deprecated</a>)</p>\n<p>So, what are the consequences of the backing store ratio and the device pixel\nratio? If the backing store ratio equals the device pixel ratio, then no\nscaling takes place, but what we often see is that they are not equal, so the\nimage is up-scaled from the backing store to the screen, and then it is\nstretched and blurred.</p>\n<p><strong>So, how do you enable the high DPI mode?</strong></p>\n<p>The solution to properly scale your HTML5 canvas content involves a couple of\nsteps that are described in the tutorial here\n<a href=\"http://www.html5rocks.com/en/tutorials/canvas/hidpi/\">http://www.html5rocks.com/en/tutorials/canvas/hidpi/</a>, but here is the\nessence:</p>\n<ol>\n<li>\n<p>Use the canvas.scale method, which tells the canvas's drawing area to become\nbigger, but keeps drawing operations consistent.</p>\n</li>\n<li>\n<p>The scaling factor for the canvas.scale method is\ndevicePixelRatio/backingStoreRatio. This will be 2 for instance on a Retina\nscreen at a typical 100% zoom level. The zoom level is relevant which will be\ndiscussed later in this post...</p>\n</li>\n<li>\n<p>Multiply the width and height attributes of the canvas by\ndevicePixelRatio/backingStoreRatio, so that the \"canvas object\" is as big as\nthe scaled size.</p>\n</li>\n<li>\n<p>Here's the tricky part: set the CSS width and height attributes to be the\nUNSCALED size that you want.</p>\n</li>\n</ol>\n<p>Note: you can also set CSS width:100% or something and then the canvas will be\nsized appropriately. Normally though, what you will have is something like\n<code>&#x3C;canvas width=640 height=480 style=\"width:320px;height:240px\"></code> so you can see\nthat the canvas size is larger than what the CSS actually resizes it to be.</p>\n<p>**Issues: Browser zoom and fractional devicePixelRatios **</p>\n<p>When I first started this project, the benefit of this high resolution\nrendering seemed limited to the fancy people who had Retina or other High DPI\nscreens. However, what I didn't even realize is that the devicePixelRatio value\nchanges depending on browser zoom settings, so even people with a regular\nscreen can have improved rendering of the HTML5 canvas. (Update: we even saw\nthat if you have customized canvas renderings, then you an generate good\nscreenshots of the canvas with PhantomJS too. See <a href=\"http://searchvoidstar.tumblr.com/post/112494997473/creating-high-resolution-screenshots-of-jbrowse\">my other more recent\narticle</a>)</p>\n<p>The issue with these zoom settings though is that when you change the zoom\nlevel, especially on chrome and firefox browsers, the devicePixelRatio can end\nup being a fractional value e.g. 2.223277 which can result in sub-pixel\nrendering problems.</p>\n<p>Remember that when we scaled the canvas, it also scales the drawing functions\nto be consistent, so that essentially if you draw a 1 pixel width line on a\nscaled canvas, it might draw a 2.223277 pixel width line. Hence, we can get\nfuzzy rendering issues.</p>\n<p>This issue is very noticeable if you draw many 1px wide lines right next to\neach other. In this case, there will be noticeable gaps between the lines due\nto the imperfect rendering (see green box below).</p>\n<p><img src=\"/media/86542847038_1.png\" alt=\"\"></p>\n<p><em>Figure 2.</em> Examples of 1px wide lines rendered next to each other when there\nis fractional devicePixelRatio.</p>\n<p>Bottom Green box: 1px wide lines drawn 1px apart. (note: bad rendering! tiny\ngaps)  Middle Blue box: 1px wide line rendered every 2 px (intentional gaps for\ndemonstration).  Top Red box: 1.3px wide lines (a fudge factor is used to make\neliminate the tiny gaps).</p>\n<p>**My solution: The Red Box -- add a fudge factor **</p>\n<p>As you can see in the above figure, my solution to the sub-pixel rendering is\nto add a \"fudge factor\" to the line width to make it render lines that are\n1.3px wide instead of 1px wide when the devicePixelRatio is not a whole number,\nwhich effectively eliminates any gaps due to the sub-pixel rendering problem.</p>\n<p>I heuristically determined the value 1.3px to be sufficient, as testing values\nlike 1.1px, 1.2px and even 1.25px were too small. I'd love to see a proof of\ndetermining this value empirically, or even better, something that isn't this\nbig of a hack, but for now that's what I have.</p>\n<p>You can see the effect of the fudge factor (red box) vs the bad rendering\n(green box) in Figure 2. You can also try this out yourself here\n<a href=\"http://jsfiddle.net/4xe4d/\">http://jsfiddle.net/4xe4d/</a>, just zoom your browser and then refresh (zooming\nand not refreshing doesn't modify device pixel ratio) to test out different\nvalues of devicePixelRatio.</p>\n<p><strong>Conclusion</strong></p>\n<p>In conclusion...we now have high resolution rendering on canvas! The solution\nfor drawing lots of lines right next to each other is sort of suboptimal, so\nthe question continues...what shall be done in this case?</p>\n<p>Maybe someone could implement some sort of library that replaces the\ncanvas.scale method to do better layout and obtain more pixel perfect\nrendering. Alternatively, you could force the scaling factor to always round to\na whole number. This is actually not a bad solution, because the canvas is\nalready being resized, and then you can control your rendering better.</p>\n<p>Thanks for reading</p>"}]},"__N_SSG":true}