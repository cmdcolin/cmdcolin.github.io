<!DOCTYPE html><html lang="en"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="shortcut icon" href="favicon.ico"/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><meta name="description" content="Blogging for the future"/><meta name="next-head-count" content="5"/><link rel="preload" href="/_next/static/css/895e0128ed383e47.css" as="style"/><link rel="stylesheet" href="/_next/static/css/895e0128ed383e47.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-514908bffb652963.js" defer=""></script><script src="/_next/static/chunks/framework-91d7f78b5b4003c8.js" defer=""></script><script src="/_next/static/chunks/main-7c3bf82eed00c281.js" defer=""></script><script src="/_next/static/chunks/pages/_app-76cd57bf65f05d70.js" defer=""></script><script src="/_next/static/chunks/162-34cdd443a23a768b.js" defer=""></script><script src="/_next/static/chunks/pages/archive-2e29d0681a35186e.js" defer=""></script><script src="/_next/static/wRE5shyr7ZdzrAZY-3qqN/_buildManifest.js" defer=""></script><script src="/_next/static/wRE5shyr7ZdzrAZY-3qqN/_ssgManifest.js" defer=""></script><script src="/_next/static/wRE5shyr7ZdzrAZY-3qqN/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="min-h-screen"><main><div class="container mx-auto px-5"><div class="text-2xl md:text-4xl font-bold py-14 border-b border-accent-2 flex flex-col lg:flex-row items-center"><a class="hover:underline" href="/">Misc scribbles</a></div><h2>Blog archive</h2><section><div class="py-14"><h1>Posts</h1><div><div><a class="hover:underline" href="/posts/2021-10-30-spooky">2021-10-30<!-- --> - <!-- -->A spooky error when you have a string bigger than 512MB in Chrome</a></div><div><a class="hover:underline" href="/posts/2021-10-05-jest">2021-10-05<!-- --> - <!-- -->Jest parallelization, globals, mocks, and squawkless tests</a></div><div><a class="hover:underline" href="/posts/2021-09-05-typescript">2021-09-05<!-- --> - <!-- -->Decrease your idle CPU usage when developing typescript apps with this one weird environment variable</a></div><div><a class="hover:underline" href="/posts/2021-08-15-map-limit">2021-08-15<!-- --> - <!-- -->An amazing error message if you put more than 2^24 items in a JS Map object</a></div><div><a class="hover:underline" href="/posts/2021-07-27-npm-dependencies">2021-07-27<!-- --> - <!-- -->Do you understand your NPM dependencies?</a></div><div><a class="hover:underline" href="/posts/2020-12-26-pt2">2020-12-26<!-- --> - <!-- -->Making a HTTPS accessible S3 powered static site with CloudFront+route 53</a></div><div><a class="hover:underline" href="/posts/2020-12-26">2020-12-26<!-- --> - <!-- -->Making a serverless website for photo and video upload pt. 2</a></div><div><a class="hover:underline" href="/posts/2020-12-24">2020-12-24<!-- --> - <!-- -->Making a serverless website for photo upload pt. 1</a></div><div><a class="hover:underline" href="/posts/2020-07-04">2020-07-04<!-- --> - <!-- -->Challenges I have faced learning React</a></div><div><a class="hover:underline" href="/posts/2020-06-03">2020-06-03<!-- --> - <!-- -->Misconceptions your team might have during The Big Rewrite</a></div><div><a class="hover:underline" href="/posts/2018-12-17">2018-12-17<!-- --> - <!-- -->Behind the release - the story of the bugs and features in JBrowse 1.16.0</a></div><div><a class="hover:underline" href="/posts/2017-04-21">2017-04-21<!-- --> - <!-- -->Problems that I experienced with the HPCC</a></div><div><a class="hover:underline" href="/posts/2017-03-12">2017-03-12<!-- --> - <!-- -->How I learned to hate ORM (especially for data import scripts)</a></div><div><a class="hover:underline" href="/posts/2017-02-16">2017-02-16<!-- --> - <!-- -->Plotting a coordinate on the screen</a></div><div><a class="hover:underline" href="/posts/2016-11-10">2016-11-10<!-- --> - <!-- -->Creating a JBrowse plugin</a></div><div><a class="hover:underline" href="/posts/2015-09-16">2016-09-16<!-- --> - <!-- -->Fixing spiky CPU issues with Tomcat</a></div><div><a class="hover:underline" href="/posts/2016-06-20">2016-06-20<!-- --> - <!-- -->Installing clamav on OSX</a></div><div><a class="hover:underline" href="/posts/2016-06-17">2016-06-17<!-- --> - <!-- -->Querying InterMine databases using R</a></div><div><a class="hover:underline" href="/posts/2016-04-23">2016-04-23<!-- --> - <!-- -->How to make your resume.json or resume-cli look great</a></div><div><a class="hover:underline" href="/posts/2016-04-19">2016-04-19<!-- --> - <!-- -->Creating a testing framework for JBrowse plugins</a></div><div><a class="hover:underline" href="/posts/2016-04-17">2016-04-17<!-- --> - <!-- -->Creating a docker image</a></div><div><a class="hover:underline" href="/posts/2016-04-06">2016-04-06<!-- --> - <!-- -->Basic command line productivity tricks and learning experiences</a></div><div><a class="hover:underline" href="/posts/2016-03-28">2016-03-28<!-- --> - <!-- -->Running nginx on containerised travis-CI pt 2</a></div><div><a class="hover:underline" href="/posts/2016-03-05">2016-03-05<!-- --> - <!-- -->On over-reproducibility</a></div><div><a class="hover:underline" href="/posts/2015-12-17">2015-12-17<!-- --> - <!-- -->Cheating in your computer science class by copying from stackoverflow</a></div><div><a class="hover:underline" href="/posts/2015-10-22">2015-10-22<!-- --> - <!-- -->Killing postgres the hard way</a></div><div><a class="hover:underline" href="/posts/2015-10-15">2015-10-15<!-- --> - <!-- -->Tomcat memory debugging</a></div><div><a class="hover:underline" href="/posts/2015-08-30">2015-08-30<!-- --> - <!-- -->Weekend project - graphing tumblr reblogs using cytoscape.js</a></div><div><a class="hover:underline" href="/posts/2015-03-02">2015-03-02<!-- --> - <!-- -->Creating high-resolution screenshots (of jbrowse) with phantomJS</a></div><div><a class="hover:underline" href="/posts/2015-02-01">2015-02-01<!-- --> - <!-- -->Post graduation survey</a></div><div><a class="hover:underline" href="/posts/2014-05-22">2014-05-22<!-- --> - <!-- -->High DPI rendering on HTML5 canvas - some problems and solutions</a></div></div></div></section></div></main></div><footer class="bg-accent-1 border-t border-accent-2"><div class="container mx-auto px-5"><div class="py-14 flex flex-col lg:flex-row items-center"><div class="m-4"><a class="hover:underline" href="/">Home</a></div><div class="m-4"><a class="hover:underline" href="https://github.com/cmdcolin">Github</a></div><div class="m-4"><a class="hover:underline" href="https://twitter.com/cmdcolin">Twitter</a></div><div class="m-4"><a class="hover:underline" href="/projects">Projects</a></div><div class="m-4"><a class="hover:underline" href="/rss.xml">RSS</a></div></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"allPosts":[{"title":"A spooky error when you have a string bigger than 512MB in Chrome","date":"2021-10-30","slug":"2021-10-30-spooky","content":"\nNow gather round for a spooky story\n\nLate one night... in the haunted office space castle (hindenbugs cackling in\nthe background amongst the dusty technical books) the midnight candles were\nburning bright and we entered data for a user file\n\nA simple 52MB gzipped datafile that we want to process in the browser. We unzip\nit, decode it, and ...an error\n\nERROR: data not found\n\n![](/media/pumpkin-dark.jpg)\n\nBut... our code is so simple (we of course abide by the religion of writing \"simple code\" you know)...what could be happening?\n\nThe code looks like this\n\n```js\nconst buf = unzip(file)\nconst str = new TextDecoder().decode(buf)\n```\n\nWe trace it back and run a console.log(str)\n\nIt looks empty. We try running console.log(str.length) ... it prints out 0\n\nBut if we console.log(buffer.length) we get 546,483,710 bytes...\n\nWhat could be happening?\n\nWe see in the TextDecoder documentation that it has a note called \"fatal\". We try\n\n```js\nconst buf = unzip(file)\nconst str = new TextDecoder('utf8', { fatal: true }).decode(buf)\n```\n\nThis doesn't change the results though\n\nThen it dawns on us while the lightning hits and the thunderclap booms and the\nwind blows through the rattly windows\n\nWe have hit...the maximum string length in Chrome\n\nBWAHAHAHAHA\n\nThe maximum string length!!! Nooooooo\n\nIt is 512MB on the dot... 536,870,888 bytes. We test this to be sure\n\n```\nconst len = 536_870_888;\nconst buf = new Uint8Array(len);\nfor (let i = 0; i \u003c len; i++) {\n  buf[i] = \"a\".charCodeAt(0);\n}\nconst str = new TextDecoder().decode(buf);\nconsole.log(str.length);\n\n```\n\nThis is correct, outputs 536,870,888\n\nWith anything, even one byte more, it fails and outputs 0\n\nhappy halloween!!\n\npumpkin photo source: http://mountainbikerak.blogspot.com/2010/11/google-chrome-pumpkin.html\n\nchrome 95 tested\n\nnodejs 15 - at 512MB+1 bytes it prints an error message `Error: Cannot create a string longer than 0x1fffffe8 characters` for significantly greater than 512MB\ne.g. 600MB it actually prints a different error `TypeError [ERR_ENCODING_INVALID_ENCODED_DATA]: The encoded data was not valid for encoding utf-8`)\n\nfirefox 93 - goes up to ~1GB but then gives Exception { name: \"NS_ERROR_OUT_OF_MEMORY\", message: \"\", result: 2147942414\n\nmidori 6 (safari-alike/webkit) - goes up to ~2GB fine! will have to test more\n"},{"title":"Jest parallelization, globals, mocks, and squawkless tests","date":"2021-10-05","slug":"2021-10-05-jest","content":"\nI found that there is a little bit of confusion and misunderstanding around how\nthings like parallelization work in jest, which sometimes leads to additional\nhacking around problems that may not exist or speculating incorrectly about\ntest failure. This is also of course a point of concern when you have code that\nfor some reason or another uses global variables. Here are a short summary of\nthings that may cause confusion.\n\n## Tests in a single file are NOT run in parallel\n\nSimple example, the global variable r is included in the test condition, but it\nis accurately run in all cases because the tests are not run in parallel.\n\n```js\nlet r = 0\n\nfunction timeout(ms) {\n  return new Promise(resolve =\u003e setTimeout(resolve, ms))\n}\n\ndescribe('tests', () =\u003e {\n  it('t1', async () =\u003e {\n    await timeout(1000)\n    expect(r).toBe(0)\n    r++\n  })\n  it('t2', async () =\u003e {\n    await timeout(1000)\n    expect(r).toBe(1)\n    r++\n  })\n  it('t3', async () =\u003e {\n    await timeout(1000)\n    expect(r).toBe(2)\n    r++\n  })\n})\n```\n\nThis test will take 3 seconds, and will accurately count the global variable.\nIf it was in parallel, it may only take 1 second, and would inaccurately count\nthe global variable due to race conditions\n\n## Tests in different files ARE run in parallel\n\nLet's take another example where we use a global variable, and then two\ndifferent tests use the global variable.\n\nfile_using_some_globals.js\n\n```js\nlet myGlobal = 0\n\nexport function doStuff() {\n  myGlobal++\n  return myGlobal\n}\n\nexport function resetMyGlobal() {\n  myGlobal = 0\n}\n\nexport function timeout(ms) {\n  return new Promise(resolve =\u003e setTimeout(resolve, ms))\n}\n```\n\ntest_global_vars1.test.js\n\n```js\nimport { doStuff, timeout } from './dostuff'\ntest('file1', async () =\u003e {\n  doStuff()\n  await timeout(1000)\n  expect(doStuff()).toEqual(2)\n})\n```\n\ntest_global_vars2.test.js\n\n```js\nimport { doStuff, timeout } from './dostuff'\n\ntest('file1', async () =\u003e {\n  await timeout(1000)\n  expect(doStuff()).toEqual(1)\n})\n```\n\nThis test completes in less than 2 seconds, and these tests are run in\nparallel. They use different instances of the global state, and therefore have\nno worries with colliding their state.\n\n## Does a mock from one test affect another test?\n\nWhile seeking the fabled \"squawk-less\" test, it is often useful to mock console\nso that tests that produce an expected error don't actually print an error\nmessage. However, if not done carefully, you will remove errors across tests\n\nSo, could a mock from one test affect another test? If it's in the same file,\nyes!\n\nmock_console.test.js\n\n```\ntest(\"test1\", () =\u003e {\n  console.error = jest.fn();\n  console.error(\"wow\");\n  expect(console.error).toHaveBeenCalled();\n});\n\ntest(\"test2\", () =\u003e {\n  // this console.error will not appear because test1 mocked away console.error\n  // without restoring it\n  console.error(\"Help I can't see!\");\n});\n\n\n```\n\nTo properly mock these, you should restore the console mock at the end of your\nfunction\n\n```\ntest(\"test1\", () =\u003e {\n  const orig = console.error;\n  console.error = jest.fn();\n  console.error(\"I should not see this!\");\n  expect(console.error).toHaveBeenCalled();\n  console.error = orig;\n});\n\ntest(\"test2\", () =\u003e {\n  const consoleMock = jest.spyOn(console, \"error\").mockImplementation();\n  console.error(\"I should not see this!\");\n  consoleMock.mockRestore();\n});\n\ntest(\"test3\", () =\u003e {\n  console.error(\"I should see this error!\");\n});\n```\n\n## Add-on: Achieve squawkless tests!\n\nYour test output should just be a big list of PASS statements, not interleaved\nwith console.error outputs from when you are testing error conditions of your\ncode\n\n\"Squawkless tests\" is a term I made up, but it means that if you have code\nunder test that prints some errors to the console, then mock the console.error\nfunction, as in the previous section. Don't stand for having a bunch of verbose\nerrors in your CI logs! However, I also suggest only mocking out console.error\nfor tests that are **expected** to have errors, lest you paper over unexpected\nerrors.\n\n![](/media/squawkless_tests.png)\n\nFigure: a nice clean test suite without a bunch of crazy console.error outputs\n\n## Conclusion\n\nGetting better at testing requires exercise, and understanding the basics of\nyour tools can help! Hopefully this helps you achieve a better understanding\nand write cleaner jest tests.\n"},{"title":"Decrease your idle CPU usage when developing typescript apps with this one weird environment variable","date":"2021-09-05","slug":"2021-09-05-typescript","content":"\nTL;DR:\n\nadd this to your bashrc\n\n```\nexport TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling\n```\n\n\u003chr/\u003e\n\nBy default, the typescript watcher configuration e.g. tsc --watch or whatever\nis run internally to a create-react-app typescript app (I see it in the process\nmanager as fork-ts-checker-webpack-plugin cpu usage) can have high idling\n(doing nothing...) CPU usage\n\nThis is because the default configuration polls for file changes (constantly\nasks the computer if there are changes every 250ms or so). There is an\nalternative configuration for this to change it to a file watcher so it\nreceives file system notifications on file change. There is discussion here on\nthis.\n\nThe main summary is that a env variable set to\nTSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling allows this\n\nhttps://github.com/microsoft/TypeScript/issues/31048\n\nThe issue thread shows that it can go from roughly ~7% idle CPU usage to 0.2%.\nThis corresponds with what I see too after applying this! Detailed docs for\ntypescript discuss some of the reasoning behing not making this the default\n\nhttps://github.com/microsoft/TypeScript-Handbook/blob/master/pages/Configuring%20Watch.md#background\n\nIt claims that some OS specific behaviors of file watching could be harmful to\nmaking it the default. For example, that (maybe?) on linux, it may use a large\nnumber of file watchers which can exceed notify handles (this is a setting I\ncommonly have to increase in linux, guide here\nhttps://dev.to/rubiin/ubuntu-increase-inotify-watcher-file-watch-limit-kf4)\n\nPS: if you have a package.json of a `create-react-app --template typescript` or\nsomething like this then you can edit the package.json to apply this\nautomatically\n\n```\n-\"start\": \"react-scripts start\"\n+\"start\": \"cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling react-scripts start\"\n```\n\nPhew. I can already feel my laptop running cooler...or at least I can sleep\nmore soundly knowing that my readers adopt this and save some CPU cycles for\nplanet earth...and hopefully don't run into any of the caveats\n\nEdit: It may be worth it to note, the 'UseFsEvents' part of this uses the\nnode.js fs.watch API and the polling based API is based on fs.watchFile\n\nFun table of how the watchers are implemented on different OSs\n[[1](https://github.com/microsoft/TypeScript/issues/31048#issuecomment-495483957)]\n\n```\nOn Linux systems, this uses inotify(7).\nOn BSD systems, this uses kqueue(2).\nOn macOS, this uses kqueue(2) for files and FSEvents for directories.\nOn SunOS systems (including Solaris and SmartOS), this uses event ports.\nOn Windows systems, this feature depends on ReadDirectoryChangesW.\nOn Aix systems, this feature depends on AHAFS, which must be enabled.\n```\n\nAnd in general, these should all respond more or less the same, but there are\nsmall corner cases that are discussed\nhttps://nodejs.org/docs/latest/api/fs.html#fs_availability\n\nDisclaimer: it may be worth reading the reasons that typescript does not have\nthis enabled by default before pushing this into your dev environment and all\nyour teammates, but as far as I could tell, it seems ok!\n"},{"title":"An amazing error message if you put more than 2^24 items in a JS Map object","date":"2021-08-15","slug":"2021-08-15-map-limit","content":"\nOne of the fun things about working with big data is that you can often hit\nweird limits with a system.\n\nI was personally trying to load every 'common' single nucleotide polymorphism\nfor the human genome into memory (dbSNP), of which there are over 37 million\nentries (there are many more uncommon ones) for the purposes of making a custom\nsearch index for them [1].\n\nTurns out, you may run into some hard limits. Note that these are all V8-isms\nand may not apply to all browsers or engines (I was using node.js for this)\n\n```\nconst myObject = new Map();\nfor (let i = 0; i \u003c= 50_000_000; i++) {\n  myObject.set(i,i);\n  if(i%100000==0) { console.log(i) }\n}\n```\n\nThis will crash after adding approx 16.7M elements and say\n\n```\n0\n100000\n200000\n...\n16400000\n16500000\n16600000\n16700000\n\nUncaught RangeError: Value undefined out of range for undefined options\nproperty undefined\n```\n\nThat is a very weird error message. It says “undefined” three times! Much\nbetter than your usual “TypeError: Can’t find property ‘lol’ of undefined”. See\nhttps://bugs.chromium.org/p/v8/issues/detail?id=11852 for a bug filed to help\nimprove the error message perhaps.\n\nNow, also interestingly enough, if you use an Object instead of a Map\n\n```js\nconst myObject = {};\nfor (let i = 0; i \u003c= 50_000_000; i++) {\n  myObject['myobj_’+i]=i;\n  if(i%100000==0) { console.log(i) }\n}\n```\n\nThen it will print….\n\n```\n0\n100000\n200000\n...\n8000000\n8100000\n8200000\n8300000\n```\n\nAnd it will actually just hang there…frozen…no error message though! And it is\nfailing at ~8.3M elements. Weird right? This is roughly half the amount of\nelements as the 16.7M case\n\nTurns out there is a precise hard limit for the Map case\n\nFor the Map: 2^24=16,777,216\n\nFor the Object it is around 2^23=8,388,608 HOWEVER, I can actually add more\nthan this, e.g. I can add 8,388,609 or 8,388,610 or even more, but the\noperations start taking forever to run, e.g. 8,388,999 was taking many minutes\n\nVery weird stuff! If you expected me to dig into this and explain it in deep\ntechnical detail, well, you’d be wrong. I am lazy. However, this helpful post\non stackoverflow by a V8 js engine developer clarifies the Map case!!\nhttps://stackoverflow.com/questions/54452896/maximum-number-of-entries-in-node-js-map\n\n```\nV8 developer here. I can confirm that 2^24 is the maximum number of entries in\na Map. That’s not a bug, it’s just the implementation-defined limit.\n\nThe limit is determined by:\n\nThe FixedArray backing store of the Map has a maximum size of 1GB (independent\nof the overall heap size limit) On a 64-bit system that means 1GB / 8B = 2^30 /\n2^3 = 2^27 ~= 134M maximum elements per FixedArray A Map needs 3 elements per\nentry (key, value, next bucket link), and has a maximum load factor of 50% (to\navoid the slowdown caused by many bucket collisions), and its capacity must be\na power of 2. 2^27 / (3 * 2) rounded down to the next power of 2 is 2^24, which\nis the limit you observe.  FWIW, there are limits to everything: besides the\nmaximum heap size, there’s a maximum String length, a maximum Array length, a\nmaximum ArrayBuffer length, a maximum BigInt size, a maximum stack size, etc.\nAny one of those limits is potentially debatable, and sometimes it makes sense\nto raise them, but the limits as such will remain. Off the top of my head I\ndon’t know what it would take to bump this particular limit by, say, a factor\nof two – and I also don’t know whether a factor of two would be enough to\nsatisfy your expectations.\n\n```\n\nGreat details there. It would also be good to know what the behavior is for the\nObject, which has those 100% CPU stalls after ~8.3M, but not the same error\nmessage...\n\nAnother fun note: if I modify the Object code to use only “integer IDs” the\ncode actually works fine, does not hit any errors, and is “blazingly fast” as\nthe kids call it\n\n```js\nconst myObject = {}\nfor (let i = 0; i \u003c= 50_000_000; i++) {\n  myObject[i] = i\n  if (i % 100000 == 0) {\n    console.log(i)\n  }\n}\n```\n\nI presume that this code works because it detects that I’m using it like an\narray and it decides to transform how it is working internally and not use a\nhash-map-style data structure, so does not hit a limit. There is a slightly\nhigher limit though, e.g. 1 billion elements gives “Uncaught RangeError:\nInvalid array length”\n\n```js\nconst myObject = {}\nfor (let i = 0; i \u003c= 1_000_000_000; i++) {\n  myObject[i] = i\n  if (i % 100000 == 0) {\n    console.log(i)\n  }\n}\n```\n\nThis has been another episode of ....the twilight zone (other episodes\ncatalogued here) https://github.com/cmdcolin/technical_oddities/\n\n[1] The final product of this adventure was this, to create a search index for\na large number of elements https://github.com/GMOD/ixixx-js\n"},{"title":"Do you understand your NPM dependencies?","date":"2021-07-27","slug":"2021-07-27-npm-dependencies","content":"\nYou are writing a library...or your writing an app and you want to publish some\nof the components of it as a library...\n\nHere are some questions in the form of comments\n\n- Did you realize that your yarn.lock will be ignored for anyone who installs\n  your libraries?\n\n- Did you realize this means that your perfectly running test suite with your\n  yarn.lock could be a failing case for consumers of your app unless you don’t\n  use semver strings like ^1.0.0 and just hardcode it to 1.0.0?\n\n- Did you realize the default of ^1.0.0 automatically gets minor version bumps\n  which are often fairly substantial changes, e.g. even breaking possibly?\n\n- Did you know that larger libraries like @material-ui/core don’t like to bump\n  their major version all the time for example so large changes are often made\n  to the minor version?\n\n- Did you know if you run `yarn upgrade`, it may update what is in your yarn.lock file but will not update what is in your package.json?\n\n- Did you realize that this means that if you depend on the results of running `yarn upgrade` e.g. it gave you a bugfix, you will be shipping buggy code to consumers of your library?\n\nJust something to be aware of! You can always ride the dragon and accept these\nminor breakages from semver bumps, but it can introduce some issues for your\nconsumers\n\nRandom fun thing: Adding a yarn package can even downgrade some other packages.\nFor example if you have ^6.0.0 in your package.json, you yarn upgrade it up to\n^6.1.0 but then later install another library that requires a hard 6.0.1, yarn\nwill decide to downgrade you to 6.0.1\n"},{"title":"Making a HTTPS accessible S3 powered static site with CloudFront+route 53","date":"2020-12-26","slug":"2020-12-26-pt2","content":"\nThis is not a very authoritative post because I stumbled though this but\nI think I got it working now on my website :)\n\n## Setup your S3 bucket\n\nFirst setup your S3 bucket, your bucket must be named yourdomain.com\ne.g. named after your domain\n\nThen if you have a create-react-app setup I add a script in package.json\nthat runs\n\n```\n \"predeploy\": \"npm run build\",\n \"deploy\": \"aws sync --delete build s3://yourdomain.com\"\n```\n\nThen we can run \"yarn deploy\" and it will automatically upload our\ncreate-react-app website to our S3 static site bucket.\n\nThen make sure your bucket has public permissions enabled\n\u003chttps://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2\u003eThen\nmake sure your bucket has \"static site hosting\" enabled too\n\n## Setup route 53, and make your NS entries in domains.google.com\n\nI bought a domain with domains.google.com\n\nGoogle then emailed me to validate my ownership\n\nThen I went to aws.amazon.com route 53 and I created a hosted zone\n\nThis generated 4 name server entries and I added those to the\ndomains.google.com site\n\n![](/media/638618421776515072_0.png)\n\nScreenshot shows copying the NS values from route 53 to the name servers\narea of domains.google.com\n\n## Setup your Amazon certificate for making SSL work on CloudFront\n\nTo properly setup However, this does not work so you need to go to\nAmazon Certificates-\u003eProvision certificates\n\nWe request the certificate for\n\n[www.yourdomain.com](http://www.yourdomain.com)\nyourdomain.com\n\nThen it generates some codes for a CNAME value for each of those two\nentries, and has a button to autoimport those CNAME values to route53\n\nThen it will say \"Pending validation\"...I waited like an hour and then\nit changed to \"Success\".\n\n![](/media/638618421776515072_1.png)\n\nScreenshot shows the now successful Amazon Certificate. After you get\nthis, you can proceed to finishing your cloudfront\n\n## Create a CloudFront distribution and add \"Alternative CNAME\" entries for your domain\n\nThen we can update our CloudFront distribution and add these to\nthe \"Alternative CNAME\" input box\n\nyourdomain.com\n[www.yourdomain.com](http://www.yourdomain.com)\n\nNote also that I first generated my certificate in us-east-2 but the\n\"Import certificate form\" in cloudfront said I had to create it in\nus-east-1\n\n![](/media/638618421776515072_2.png)\n\n## Add a default object index.html to the CloudFront setting\n\nMake your CloudFront \"default object\" is index.html\n\nYou have to manually type this in :)\n\n## Add the CloudFront distribution to your Route 53\n\nAdd a Route 53 \"A\" record that points to the CloudFront domain name e.g.\nd897d897d87d98dd.cloudfront.net\n\n## Summary of steps needed\n\nThe general hindsight 20/20 procedure is\n\n1.  Upload your static content to an S3 bucket called yoursite.com (must\n    be your domain name)\n2.  Make your S3 bucket have the \"static website\" setting on in the\n    properties menu and add a permissions policy that supports getObject\n    e.g. \u003chttps://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2\u003e\n3.  Create a CloudFront distribution for your website\n4.  Make the CloudFront default object index.html\n5.  Create your domain with domains.google.com or similar\n6.  Point the google domain's name server to Route 53 NS list from AWS\n7.  Add Route 53 A records that point to the CloudFront domain name e.g.\n    d897d897d87d98dd.cloudfront.net\n8.  Create Amazon issued certificate for yourdomain.com, which can\n    auto-import a validation CNAME to your Route 53\n9.  Make your CloudFront domain support your Alternative CNAME's e.g.\n    yourdomain.com which requires importing (e.g. selecting from a list\n    that they auto-populate) your Amazon-issued-certificate\n\n## Troubleshooting and notes\n\nProblem: Your website gives 403 CloudFlare error\nSolution: You have to get the Alternateive CNAME configuration setup\n(pre-step involves the certificate request and validation)\n\nProblem: Your website gives an object not found error\nSolution: Set the CloudFront \"default object\" to index.html\n\n## Random comment\n\nThis is one of those processes (creating the cloudfront/route 53) that\nprobably could have done with the aws-sam CLI and it would have possibly\nbeen easier, it is quite fiddly doing all these steps in the web\ninterface\n"},{"title":"Making a serverless website for photo and video upload pt. 2","date":"2020-12-26","slug":"2020-12-26","content":"\nThis post follows\non \u003chttps://cmdcolin.github.io/2020-12-24.html\u003e\n\nIt is possible I zoomed ahead too fast to make this a continuous\ntutorial, but overall I just wanted to post an update\n\nIn pt. 1 I learned how to use the `aws-sam` CLI tool. This was a great\ninsight for me about automating deployments. I can now simply run `sam deploy` and it will create new dynamodb tables, lambda functions, etc.\n\nAfter writing pt 1. I converted the existing vue-js app that was in the\naws tutorial and converted it to react. Then I extended the app to allow\n\n- Posting comments on photos\n- Uploading multiple files\n- Uploading videos\n  etc.\n\nIt will be hard to summarize all the changes since now the app has taken\noff a little bit but it looks like this:\n\nRepo structure\n\n```\n ./frontend # created using npx create-react-app frontend --template\n typescript\n ./frontend/src/App.tsx # main frontend app code in react\n ./lambdas/\n ./lambdas/postFile # post a file to the lambda, this uploads a row to\n dynamodb and returns a pre-signed URL for uploading (note that if the\n client failed it's upload, that row in the lambda DB might be in a bad\n state...)\n ./lambdas/getFiles # get all files that were ever posted\n ./lambdas/postComment # post a comment on a picture with POST\n request\n ./lambdas/getComments?file=filename.jpg # get comments on a\n picture/video with GET request\n```\n\nHere is a detailed code for uploading the file. We upload one file at a\ntime, but the client code post to the lambda endpoint individually for\neach file\n\nThis generates a pre-signed URL to allow the client-side JS (not the\nlambda itself) to directly upload to S3, and also posts a row in the S3\nto the filename that will. It is very similar code in\nto \u003chttps://cmdcolin.github.io/2020-12-24.html\u003e\n\n./lambdas/postFile/app.js\n\n```js\n'use strict'\n\nconst AWS = require('aws-sdk')\nconst multipart = require('./multipart')\nAWS.config.update({ region: process.env.AWS_REGION })\nconst s3 = new AWS.S3()\n\n// Change this value to adjust the signed URL's expiration\nconst URL_EXPIRATION_SECONDS = 300\n\n// Main Lambda entry point\nexports.handler = async event =\u003e {\n  return await getUploadURL(event)\n}\n\nconst { AWS_REGION: region } = process.env\n\nconst dynamodb = new AWS.DynamoDB({ apiVersion: '2012-08-10', region })\n\nasync function uploadPic({\n  timestamp,\n  filename,\n  message,\n  user,\n  date,\n  contentType,\n}) {\n  const params = {\n    Item: {\n      timestamp: {\n        N: `${timestamp}`,\n      },\n      filename: {\n        S: filename,\n      },\n      message: {\n        S: message,\n      },\n      user: {\n        S: user,\n      },\n      date: {\n        S: date,\n      },\n      contentType: {\n        S: contentType,\n      },\n    },\n    TableName: 'files',\n  }\n  return dynamodb.putItem(params).promise()\n}\n\nconst getUploadURL = async function (event) {\n  try {\n    const data = multipart.parse(event)\n    const { filename, contentType, user, message, date } = data\n    const timestamp = +Date.now()\n    const Key = `${timestamp}-${filename}` // Get signed URL from S3\n\n    const s3Params = {\n      Bucket: process.env.UploadBucket,\n      Key,\n      Expires: URL_EXPIRATION_SECONDS,\n      ContentType: contentType, // This ACL makes the uploaded object publicly readable. You must also uncomment // the extra permission for the Lambda function in the SAM template.\n\n      ACL: 'public-read',\n    }\n\n    const uploadURL = await s3.getSignedUrlPromise('putObject', s3Params)\n\n    await uploadPic({\n      timestamp,\n      filename: Key,\n      message,\n      user,\n      date,\n      contentType,\n    })\n\n    return JSON.stringify({\n      uploadURL,\n      Key,\n    })\n  } catch (e) {\n    const response = {\n      statusCode: 500,\n      body: JSON.stringify({ message: `${e}` }),\n    }\n    return response\n  }\n}\n```\n\n./lambdas/getFiles/app.js\n\n```js\n// eslint-disable-next-line import/no-unresolved\nconst AWS = require('aws-sdk')\n\nconst { AWS_REGION: region } = process.env\n\nconst docClient = new AWS.DynamoDB.DocumentClient()\n\nconst getItems = function () {\n  const params = {\n    TableName: 'files',\n  }\n\n  return docClient.scan(params).promise()\n}\n\nexports.handler = async event =\u003e {\n  try {\n    const result = await getItems()\n    return {\n      statusCode: 200,\n      body: JSON.stringify(result),\n    }\n  } catch (e) {\n    return {\n      statusCode: 400,\n      body: JSON.stringify({ message: `${e}` }),\n    }\n  }\n}\n```\n\n./frontend/src/App.tsx (excerpt)\n\n```tsx\nasync function myfetch(params: string, opts?: any) {\n  const response = await fetch(params, opts)\n  if (!response.ok) {\n    throw new Error(`HTTP ${response.status}\n ${response.statusText}`)\n  }\n  return response.json()\n}\n\nfunction UploadDialog({\n  open,\n  onClose,\n}: {\n  open: boolean\n  onClose: () =\u003e void\n}) {\n  const [images, setImages] = useState\u003cFileList\u003e()\n  const [error, setError] = useState\u003cError\u003e()\n  const [loading, setLoading] = useState(false)\n  const [total, setTotal] = useState(0)\n  const [completed, setCompleted] = useState(0)\n  const [user, setUser] = useState('')\n  const [message, setMessage] = useState('')\n  const classes = useStyles()\n\n  const handleClose = () =\u003e {\n    setError(undefined)\n    setLoading(false)\n    setImages(undefined)\n    setCompleted(0)\n    setTotal(0)\n    setMessage('')\n    onClose()\n  }\n\n  return (\n    \u003cDialog onClose={handleClose} open={open}\u003e\n           \u003cDialogTitle\u003eupload a file (supports picture or video)\u003c/DialogTitle\u003e \n         \u003cDialogContent\u003e\n               \u003clabel htmlFor=\"user\"\u003ename (optional) \u003c/label\u003e\n               \u003cinput\n          type=\"text\"\n          value={user}\n          onChange={event =\u003e setUser(event.target.value)}\n          id=\"user\"\n        /\u003e\n               \u003cbr /\u003e       \u003clabel htmlFor=\"user\"\u003emessage (optional) \u003c/label\u003e\n               \n        \u003cinput\n          type=\"text\"\n          value={message}\n          onChange={event =\u003e setMessage(event.target.value)}\n          id=\"message\"\n        /\u003e\n               \u003cbr /\u003e\n               \n        \u003cinput\n          multiple\n          type=\"file\"\n          onChange={e =\u003e {\n            let files = e.target.files\n            if (files \u0026\u0026 files.length) {\n              setImages(files)\n            }\n          }}\n        /\u003e\n               {error ? (\n          \u003cdiv className={classes.error}\u003e{`${error}`}\u003c/div\u003e\n        ) : loading ? (\n          `Uploading...${completed}/${total}`\n        ) : completed ? (\n          \u003ch2\u003eUploaded \u003c/h2\u003e\n        ) : null}       \n        \u003cDialogActions\u003e\n                   \n          \u003cButton\n            style={{ textTransform: 'none' }}\n            onClick={async () =\u003e {\n              try {\n                if (images) {\n                  setLoading(true)\n                  setError(undefined)\n                  setCompleted(0)\n                  setTotal(images.length)\n                  await Promise.all(\n                    Array.from(images).map(async image =\u003e {\n                      const data = new FormData()\n                      data.append('message', message)\n                      data.append('user', user)\n                      data.append('date', new Date().toLocaleString())\n                      data.append('filename', image.name)\n                      data.append('contentType', image.type)\n                      const res = await myfetch(API_ENDPOINT + '/postFile', {\n                        method: 'POST',\n                        body: data,\n                      })\n\n                      await myfetch(res.uploadURL, {\n                        method: 'PUT',\n                        body: image,\n                      })\n\n                      setCompleted(completed =\u003e completed + 1)\n                    }),\n                  )\n                  setTimeout(() =\u003e {\n                    handleClose()\n                  }, 500)\n                }\n              } catch (e) {\n                setError(e)\n              }\n            }}\n            color=\"primary\"\n          \u003e\n                       upload          \n          \u003c/Button\u003e\n                   \u003cButton\n            onClick={handleClose}\n            color=\"primary\"\n            style={{ textTransform: 'none' }}\n          \u003e\n                       cancel          \n          \u003c/Button\u003e       \n        \u003c/DialogActions\u003e\n             \n      \u003c/DialogContent\u003e   \n    \u003c/Dialog\u003e\n  )\n}\n```\n\ntemplate.yaml for AWS\n\n```\n AWSTemplateFormatVersion: 2010-09-09\n Transform: AWS::Serverless-2016-10-31\n Description: S3 Uploader\n\n Resources:\n  filesDynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      AttributeDefinitions:\n        - AttributeName: \"timestamp\"\n          AttributeType: \"N\"\n      KeySchema:\n        - AttributeName: \"timestamp\"\n          KeyType: \"HASH\"\n      ProvisionedThroughput:\n        ReadCapacityUnits: \"5\"\n        WriteCapacityUnits: \"5\"\n      TableName: \"files\"\n\n  # HTTP API\n  MyApi:\n    Type: AWS::Serverless::HttpApi\n    Properties:\n      # CORS configuration - this is open for development only and\n should be restricted in prod.\n      # See\n \u003chttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-property-httpapi-httpapicorsconfiguration.html\u003e\n      CorsConfiguration:\n        AllowMethods:\n          - GET\n          - POST\n          - DELETE\n          - OPTIONS\n        AllowHeaders:\n          - \"*\"\n        AllowOrigins:\n          - \"*\"\n\n  UploadRequestFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: lambdas/postFile/\n      Handler: app.handler\n      Runtime: nodejs12.x\n      Timeout: 3\n      MemorySize: 128\n      Environment:\n        Variables:\n          UploadBucket: !Ref S3UploadBucket\n      Policies:\n        - AmazonDynamoDBFullAccess\n        - S3WritePolicy:\n            BucketName: !Ref S3UploadBucket\n        - Statement:\n            - Effect: Allow\n              Resource: !Sub \"arn:aws:s3:::${S3UploadBucket}/\"\n              Action:\n                - s3:putObjectAcl\n      Events:\n        UploadAssetAPI:\n          Type: HttpApi\n          Properties:\n            Path: /postFile\n            Method: post\n            ApiId: !Ref MyApi\n\n\n  FileReadFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: lambdas/getFiles/\n      Handler: app.handler\n      Runtime: nodejs12.x\n      Timeout: 3\n      MemorySize: 128\n      Policies:\n        - AmazonDynamoDBFullAccess\n      Events:\n        UploadAssetAPI:\n          Type: HttpApi\n          Properties:\n            Path: /getFiles\n            Method: get\n            ApiId: !Ref MyApi\n\n  ## S3 bucket\n  S3UploadBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      CorsConfiguration:\n        CorsRules:\n          - AllowedHeaders:\n              - \"*\"\n            AllowedMethods:\n              - GET\n              - PUT\n              - HEAD\n            AllowedOrigins:\n              - \"*\"\n\n\n ## Take a note of the outputs for deploying the workflow templates\n in this sample application\n Outputs:\n  APIendpoint:\n    Description: \"HTTP API endpoint URL\"\n    Value: !Sub\n \"https://${MyApi}.execute-api.${AWS::Region}.amazonaws.com\"\n  S3UploadBucketName:\n    Description: \"S3 bucket for application uploads\"\n    Value: !Ref \"S3UploadBucket\"\n\n```\n\nTo display all the pictures I use a switch from video or img tag based\non contentType.startsWith('video'). I also use the \"figcaption\" HTML tag\nto have a little caption on the pics/videos\n\n./frontend/src/App.tsx\n\n```\n function Media({\n  file,\n  style,\n  onClick,\n  children,\n }: {\n  file: File;\n  onClick?: Function;\n  style?: React.CSSProperties;\n  children?: React.ReactNode;\n }) {\n  const { filename, contentType } = file;\n  const src = `${BUCKET}/${filename}`;\n  return (\n    \u003cfigure style={{ display: \"inline-block\" }}\u003e\n      \u003cpicture\u003e\n        {contentType.startsWith(\"video\") ? (\n          \u003cvideo style={style} src={src} controls onClick={onClick as\n any} /\u003e\n        ) : (\n          \u003cimg style={style} src={src} onClick={onClick as any} /\u003e\n        )}\n      \u003c/picture\u003e\n      \u003cfigcaption\u003e{children}\u003c/figcaption\u003e\n    \u003c/figure\u003e\n  );\n }\n```\n\nNow the really fun part: if you get an image of a picture frame\nlike \u003chttps://www.amazon.com/Paintings-Frames-Antique-Shatterproof-Osafs2-Gld-A3/dp/B06XNQ8W9T\u003e\n\nYou can make it a border for any image or video using border-image CSS\n\n```\n     style = {\n         border: \"30px solid\",\n         borderImage: `url(borders/${border}) 30 round`\n     }\n```\n\n![](/media/638602799897329664_0.png)\n\nSummary\n\nThe template.yaml automatically deploys the lambdas for postFile/getFile\nand the files table in dynamoDB\n\nThe React app uses postFile for each file in an `\u003cinput type=\"file\"/\u003e`,\nthe code uses React hooks and functional components but is hopefully not\ntoo complex\n\nI also added commenting on photos. The code is not shown here but you\ncan look in the source code for details\n\n![](/media/638602799897329664_1.png)\n\nOverall this has been a good experience learning to develop this app and\nlearning to automate the cloud deployment is really good for ensuring\nreliability and fast iteration.\n\nAlso quick note on serverless CLI vs aws-sam. I had tried a serverless\nCLI tutorial from another user but it didn't click with me, while the\naws-sam tutorial from\n\u003chttps://searchvoidstar.tumblr.com/post/638408397901987840/making-a-serverless-website-for-photo-upload-pt-1\u003e was\na great kick start for me. I am sure the serverless CLI is great too and\nit ensures a bit less vendor lock in, but then is also a little bit\nremoved from the native aws config schemas. Probably fine though\n\nSource code \u003chttps://github.com/cmdcolin/aws_photo_gallery/\u003e\n"},{"title":"Making a serverless website for photo upload pt. 1","date":"2020-12-24","slug":"2020-12-24","content":"\nI set out to make a serverless website for photo uploads. Our dearly\ndeparted dixie dog needed a place to have photo uploads.\n\nI didn't want to get charged dollars per month for a running ec2\ninstance, so I wanted something that was lightweight e.g. serverless,\nand easy\n\nI decided to follow this tutorial\n\n\u003chttps://aws.amazon.com/blogs/compute/uploading-to-amazon-s3-directly-from-a-web-or-mobile-application/\u003e\n\nI really liked the command line deployment (aws-sam) because fiddling\naround with the AWS web based control panel is ridiculously complicated\n\nFor example I also tried following this tutorial which uses the web\nbased UI (\u003chttps://www.youtube.com/watch?v=mw_-0iCVpUc\u003e) and it just did\nnot work for me....I couldn't stay focused (blame ADHD or just my CLI\nobsession?) and certain things like \"Execution role\" that they say to\nmodify are not there in the web UI anymore, so I just gave up (I did try\nthough!)\n\nTo install aws-sam I used homebrew\n\n```\n brew tap aws/tap\n brew install aws-sam-cli\n brew install aws-sam-cli # I had to run the install command twice ref https://github.com/aws/aws-sam-cli/issues/2320#issuecomment-721414971\n\n git clone https://github.com/aws-samples/amazon-s3-presigned-urls-aws-sam\n cd amazon-s3-presigned-urls-aws-sam\n sam deploy --guided\n\n # proceeeds with a guided installation, I used all defaults except I\n made \"UploadRequestFunction may not have authorization defined, Is\n this okay? [y/N]: y\"\n```\n\n![](/media/638408397901987840_0.png)\n\nThey then in the tutorial describe trying to use postman to test\n\nI test with `curl` instead\n\n```\ncurl 'https://fjgbqj5436.execute-api.us-east-2.amazonaws.com/uploads' {\"uploadURL\":\"https://sam-app-s3uploadbucket-1653634.s3.us-east-2.amazonaws.com/112162.jpg?Content-Type=image%2Fjpeg\u0026X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=ASIAU6CQBER6YBNCDDMJ%2F20201224%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20201224T174804Z\u0026X-Amz-Expires=300\u0026X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDIaCXVzLWVhc3QtMiJGMEQCIH65IvgJsofUpIX46lTaG3Pi5WC85ti1lukM3iICh%2BB%2BAiAJEyynPNPhZN8%2Bg1ylO7wthqud9cBcNIChIp2H%2F%2BR7mCryAQjb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDMzOTQ3MDI2MzQyMSIMLqPo1IYyH7udCGZuKsYBSEF3c50YXkmPeSWcLsEzq%2BFBTpeOIrwZTyCUjbJ7fgJUakhM1YRX40jExstN8eJcMXqw00Xd5lYHvZDbU9ajwWPLRAxcEN5BQ0utqn0NGTLyJhibzJUj8cjgm5RguIEKe9GUtMVWa9mi7C5%2FlFpS0i9jK5BSVf74JyPSLETV5mzMMzy5kHBQMGjw1dR66E3MG8PjIqfgKjhVtZmlaicf5OmeqNI2%2F8T5ye%2FICRsH4d7KNEmj4FELa8buW8U%2Fn97ThfH3P7XmMNOok%2F8FOuEBDj1EHluCT4DfZ1jIXjvrJsVv1WtV4POQDn2Dah%2BWosBn%2BFNTtQtw841ACDarYR1ZVbuwcpTjfBPlGuSOncPsbzOhzDy7wYyumsPKsXoPdxTncMWbx4BQkbU5SeF9hjpfIKRMSOqkJBN7%2BtgHXwuW1rfYMDN2OAlQZpTj7uWMPWojUMbvMzyHvI2pfgcRAlrBdGGYDigyjWl9QXP%2Bdi6WiR7XCSXbWcIAJDZh%2Beb%2BIH1asmMJtpAK6nMP8gWczaYh7PMeYyVOIs2B20xQBy%2Bz7oe%2BYQ2GfdEr2hgqPH3jd%2B7c\u0026X-Amz-Signature=11b8cd524c25ef51193e3b3fc4816760ebcde8bfc74bd52f3f91d8bf409620f5\u0026X-Amz-SignedHeaders=host\",\"Key\":\"112162.jpg\"}% \n\n```\n\nThe premise of this is you make a request, and then the response from\nthe API is a pre-signed URL that then allows you to upload directly to\nS3. You can use `curl \u003curl\u003e --upload-file yourfile.jpg`. This\nautomatically does a PUT request to the s3 bucket (yes, this is talking\ndirectly to s3 now, not the lambda! the lambda is just for generating\nthe \"pre-signed URL\" to let you upload). Careful to copy it exactly as\nis\n\n```\n curl \"https://sam-app-s3uploadbucket-1653634.s3.us-east-2.amazonaws.com/112162.jpg?Content-Type=image%2Fjpeg\u0026X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=ASIAU6CQBER6YBNCDDMJ%2F20201224%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20201224T174804Z\u0026X-Amz-Expires=300\u0026X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDIaCXVzLWVhc3QtMiJGMEQCIH65IvgJsofUpIX46lTaG3Pi5WC85ti1lukM3iICh%2BB%2BAiAJEyynPNPhZN8%2Bg1ylO7wthqud9cBcNIChIp2H%2F%2BR7mCryAQjb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDMzOTQ3MDI2MzQyMSIMLqPo1IYyH7udCGZuKsYBSEF3c50YXkmPeSWcLsEzq%2BFBTpeOIrwZTyCUjbJ7fgJUakhM1YRX40jExstN8eJcMXqw00Xd5lYHvZDbU9ajwWPLRAxcEN5BQ0utqn0NGTLyJhibzJUj8cjgm5RguIEKe9GUtMVWa9mi7C5%2FlFpS0i9jK5BSVf74JyPSLETV5mzMMzy5kHBQMGjw1dR66E3MG8PjIqfgKjhVtZmlaicf5OmeqNI2%2F8T5ye%2FICRsH4d7KNEmj4FELa8buW8U%2Fn97ThfH3P7XmMNOok%2F8FOuEBDj1EHluCT4DfZ1jIXjvrJsVv1WtV4POQDn2Dah%2BWosBn%2BFNTtQtw841ACDarYR1ZVbuwcpTjfBPlGuSOncPsbzOhzDy7wYyumsPKsXoPdxTncMWbx4BQkbU5SeF9hjpfIKRMSOqkJBN7%2BtgHXwuW1rfYMDN2OAlQZpTj7uWMPWojUMbvMzyHvI2pfgcRAlrBdGGYDigyjWl9QXP%2Bdi6WiR7XCSXbWcIAJDZh%2Beb%2BIH1asmMJtpAK6nMP8gWczaYh7PMeYyVOIs2B20xQBy%2Bz7oe%2BYQ2GfdEr2hgqPH3jd%2B7c\u0026X-Amz-Signature=11b8cd524c25ef51193e3b3fc4816760ebcde8bfc74bd52f3f91d8bf409620f5\u0026X-Amz-SignedHeaders=host\" --upload-file test.jpg\n```\n\nThere is no response, but I can then check the s3 console and see the\nfile upload is successful (all files are renamed)\n\n![](/media/638408397901987840_1.png)\n\nFigure shows that the file upload is successful :)\n\nThen we can edit the file frontend/index.html from the repo we cloned to\ncontain the lambda with the /uploads/ suffix\n\n![](/media/638408397901987840_2.png)\n\nFigure shows editing the index.html with the lambda endpoint\n\nThen we manually upload this file to another s3 bucket or test it\nlocally\n\n```\n aws s3 cp index.html s3://mybucket/\n\n\n# then ...visit that in the browser\n```\n\nAt this point the files are getting uploaded but not publically\naccessible. To make them publically accessible we uncomment the\nACL: 'public-read' in the getSignedURL/app.js folder in the github repo\n\n![](/media/638408397901987840_3.png)\n\nFigure showing the public-read uncommented\n\n![](/media/638408397901987840_4.png)\n\nFigure showing the lines that need uncommenting in template.yaml in the\nroot of the github repo that allows putObject in s3 with the public-read\nACL\n\nRe-run `sam deploy --guided`, same thing as at the start\n\nNow the objects are publicly accessible!\n"},{"title":"Challenges I have faced learning React","date":"2020-07-04","slug":"2020-07-04","content":"\nLearning React was a big challenge for me. I started learning React in earnest\nin 2019. It was a difficult experience overall, but I wanted to go over my\nlearning experience, and maybe find some lessons in the mix. This goes mostly\ninto personal details and doesn't really get too technical, however, I review\nthe commit logs and try and backtrace my feelings that I remember at the time.\n\nIf I were to take away anything from this, it's probably that pair programming\nwas really useful especially as a remote worker, I had nothing before that\nexcept weekly standups where I felt really depressed. Also stay patient, stay\nthankful, and try to focus while you learn\n\n**Introduction to me**\n\nI am maybe what you'd call a front-end engineer. I have done web development\nfor about 7 years now. I worked on various fast-becoming-legacy projects and\ngreenfield that were made in Ruby, PHP, Perl CGI, Java servlets, etc.\n\n**Early dabbles with React circa 2016**\n\nI had a random `\u003cform\u003e` that I was tasked with making and I wanted to code and\nwanted to try using React. I tried importing React via a CDN  and gave it a\nshot, and it seemed simple enough, but I kept getting really confused about how\nto even read and initialize the value of a textbox for example properly. TLDR:\nI was not aware of what a _controlled component_ was.\n\nThe idea of controlled components (not a word in my vocabulary at the time) was\nquite unintuitive and instead, I kept googling weird things like \"two way data\nbinding react\" and variants of this. I had never used Angular but I heard of\ntwo-way data binding from Angular, and I just felt like it was what I needed.\nI even posted about my frustrations about this on the React subreddit and was\ndownvoted. Felt bad. I was just really confused. I abandoned the project in\nReact and just used our normal jqueryish thing.\n\n**New job in 2018**\n\nWhen I got a call about a new job in 2018, I was really happy and started in\nJune 2018. They decided they are going to do \"the big rewrite\" and are going to\nuse React. My coworker started building the new React app prototype. My\ncoworker keeps asking me \"what state management library should we use\". I just\nhad no idea about React still, I had not ever looked into state management, and\nbasically just was like \"I dunno!\". I had no way to form an opinion. I was also\nworking on some misc stuff sort of unrelated to the rewrite and remained pretty\nout of the loop. We would have weekly meetings but I just wouldn't really\nunderstand the goings ons. The project started using mobx-state-tree and I saw\nthem start to write fresh code for the project but things like prop-types just\nwere confusing to me, e.g. there were the mobx-state-tree model types, and\nsuddenly and the React prop-types and it was still the days of class-based\nReact components. I couldn't get any clear idea of what was happening\n\n**I am floundering...not understanding what's going on with the rewrite**\n\nIt's December 2018, I go home for Christmas and I have an honest talk with my\nparents and tell them \"I don't get what is happening in the new codebase, I'm\nhonestly unhappy, and it just does all this 'React' stuff\" but I can't explain\nReact to them I just say the code is automatically reacting to other things. My\nparents say \"well if you are unhappy you might have to leave your job\" and they\nare not like, cheering for me to leave, but they tell me that. At this point,\nit really hit me that I do like this job and I decided to try to focus on work.\n\n**I try and make an honest attempt to get involved in the project, start pair\nprogramming**\n\nOn January 10th 2019 I make my first commit to the rewrite by doing some\nmonkey-see monkey-do type coding. I copy a bunch of files and just put them in\nthe right place, tweak some lines, and start to figure out how to make things\nrun. By the end of January 2019 I get my first code change merged.\n\nI also suggested that we start doing **pair-programming sessions**. Once I\nstarted these it made a huge difference for me in learning how to code. The\npair programming often still way over my head due to how my coworker presented\nstuff or how much he assumed I understood. Nevertheless, these were extremely\nhelpful for me to help get caught up.\n\n**I start to reading \"Learning React\"**\n\nIn March 2019, I got the book \"Learning React\" (O'Reilly2017\n\u003chttps://www.oreilly.com/library/view/learning-react/9781491954614/\u003e) for my\nkindle.  Reading this book was a big help I felt, and provided a needed \"brain\nreset\" for me. The book worked well for me, I read it each night on my kindle,\nand the function component concepts were super enlightening. To me it was so\nmuch better reading a book than, say, an internet tutorial. With the book, I\ncould focus, not have distractions, etc. My eyes would just glaze over every\ntime I clicked on internet tutorials and stuff before this.\n\nSo anyways, March 2019 goes on, and I'm learning, but our codebase still feels\npretty complicated and alien. We use mobx-state-tree and the glue for\nmobx-state-tree to React e.g. the mobx-react doesn't really make sense to me. I\nremember asking my coworkers why my component was not updating and they\neventually find out it's because I keep not using the observe() wrapper around\nmy components.\n\n**I start to experiment with Typescript**\n\nIn April 2019 I start to experiment with typescript and release a typescript\nversion of some data parsing code. I start by explicity specifying a lot of\ntypes but I eventually start getting into the zen of \"type inference\" and I\nturn off the @typescript-eslint/explicit-function-return-type so I get implied\nreturn types.\n\n**I start using React hooks**\n\nIn May 2019 I try out my first React hook, a useState. It worked well. I\ncouldn't really figure out why I would use it instead of the mobx state\nmanagement we used elsewhere, but the example was that it was a click and drag\nand it made sense to keep that click and drag state local to the component\nrather than the \"app\"\n\n**I start using react-testing-library**\n\nIn June 2019, I create \"integration test\" level tests for our app. I had used\nreact-testing-library for some components before this, but this was using\nreact-testing-library to render the entire \"app level\" component. I was happy\nto pioneer this and was happy to try this out instead of doing true browser\ntests, and I think this has worked out well.\n\nSome caveats: I got very caught up with trying to do canvas tests initially. I\nreally wanted to use jest-mock-canvas but we were using offscreencanvas via a\npretty complicated string of things, so I don't make progress here, and I also\ngot confused about the relationship between node-canvas and jest-mock-canvas\n(they are basically totally different approaches). Later on, I find using\njest-image-snapshot of the canvas contents works nice (ref\n\u003chttps://stackoverflow.com/questions/33269093/how-to-add-canvas-support-to-my-tests-in-jest\u003e)\n\nOther random note: when building out the integration tests, we got a lot\nof \"act warnings\" which were confusing. These were fixed in React 16.9\n(released August 2019), but we had to ignore them and they basically just\nconfused me a lot and made it feel like I was battling a very complex system\nrather than a nice simple one.\n\n**Conclusions**\n\nOverall, I just wanted to write up my whole experience. It felt really\ndifficult for me to make these changes. I also went through a breakup during\nthis time, had a bad living situation, etc. so things were a struggle. If\nanyone else has had struggles learning React, tell your story, and let me know.\nI'd like to also thank everyone who helped me along the way. I feel like a much\nbetter coder now, yet, I should always keep growing. The feeling of\nuncomfortableness could be a growing experience.\n"},{"title":"Misconceptions your team might have during The Big Rewrite","date":"2020-06-03","slug":"2020-06-03","content":"\nDisclaimer: I enjoy the project I am working on and this is still a work\nin progress. I just had to rant about the stuff I go through in my job\nhere, but it does not reflect the opinions of my emplorer, and my\npersonal opinion is despite these troubles we are coming along nicely\n\nI joined a team that was doing the big rewrite in 2018. I was involved\nin the project before then and knew it's ins and outs, and frankly think\nit's still a great system. In order to break it's \"limitations\" a grand\nv2 gets started. I think my team has been good. My tech lead is really\ngood at architecture. Where I really resist kind of \"writing new\narchitecture that is not already there\", he can pull up entirely new\nconcepts and abstractions that are all pretty good. Myself, I don't much\nenjoy writing \"new architecture\" if there is something already there\nthat I can use, and I'll try to refer to the existence of an existing\nthing instead of creating new exotic stuff.\n\nNow, what happened during the big rewrite so far. 4 people on the team,\n2 years in\n\nPersistent confusion about sources of slowness in our app\n\n- it's only slow because devtools is open (maybe it is! but this is\n  definitely a red herring. the code should work with devtools open.\n  reason that's been stated: devtools adds a \"bunch of instrumentation to\n  the promises that slows it down\"...stated without any evidence during a\n  3 hour long planning call...)\n   - it's only slow because we're using a development build of react, try\n  a production build (the production build makes some stuff faster, but it\n  is NOT going to save your butt if you are constantly rerending all your\n  components unnecessarily every millisecond during user scroll, which is\n  something we suffered from, and it creeps back in if you are not careful\n  because you can't write tests against this so often one day I'll be\n  looking at my devtools and suddenly things are rendering twice per frame\n  (signature of calling an unnecessary setState), tons of unnecessary\n  components rendering in every frame (signature of\n  componentShouldUpdate/bad functional react memoizing, etc))\n   - it's slow because we are hogging the main thread all the time, our\n  killer new feature in v2 is an intense webworker framework. now main\n  thread contention is a concern, but really our app needs to just be\n  performant all around, webworkers just offloads that cpu spinning to\n  another core. what we have done in v2 is we went whole hog and made our\n  code rely on OffscreenCanvas which 0 browsers support. also, our\n  webworker bundles (worker-loader webpack build) are huge webpack things\n  that pretty much contain all the code that is on the main thread so it's\n  just massive. that makes it slow at loading time, and makes it harder to\n  think about our worker threads in a lighter-weight way, and the worker\n  concept is now very deeply entrenched in a lot of the code (all code has\n  to think of things in terms of rpc calls)\n   - it's slow because there are processes that haven't been aborted\n  spinning in the background, so we must build out an intensive\n  AbortController thing that touches the entirety of all our code\n  including sending abort signals across the RPC boundary in hopes that a\n  locked up webworker will respond to this (note: our first version of the\n  software had zero aborting, did not from my perspective suffer.\n  arguments with the team have gotten accusatory where I just claim that\n  there is no evidence that the aborting is helping us, pointing to the\n  fact that our old code works fine, and that if our new code suffers\n  without aborting, that means something else is wrong. I have not really\n  been given a proper response for this, and so the curse of passing\n  AbortSignals onto every function via an extra function parameter drags\n  on\n   - it's slow because we are not multithreading..., so we put two views\n  of the same data into different webworkers (but now each webworker\n  separately downloads the same data, which leads to more resource spent,\n  more network IO, more slowness)\n\nconfusion about what our old users needs are\n\n- tracks not having per-track scroll (problem: leads to many scrolls\n  within-scrolls, still unresolved problem)\n   - the name indexing was always a big problem (yes it is slow but is it\n  really THE critical problem we face? likely not: bioinformatics people\n  run a data pipeline, it takes a couple days, so what). use elasticsearch\n  if it sucks so bad\n   - our users are \"stupid\" so they need to have every single thing GUI\n  editable (interesting endeavor, but our design for this has been\n  difficult, and has not yet delivered on simplifying the system for\n  users)\n   - our users \"do not like modal popups\" so we design everything into a\n  tiny sidedrawer that barely can contain the relevant data that they want\n  to see\n\n- having interest in catering to obscure or not very clear \"user\n  stories\" like displaying the same exact region twice on the screen at\n  once saying \"someone will want to do this\", but causing a ton of extra\n  logical weirdness from this\n- not catering to emerging areas of user needs such as breaking our\n  large app into components that can be re-used, and instead just going\n  full hog on a large monolith project and treating our monolith as a\n  giant hammer that will solve everyones problems, when in reality, our\n  users are also programmers that could benefit from using smaller\n  componentized versions of our code\n- confusion about \"what our competitors have\". sometimes my team one day\n  was like \"alright we just do that and then we have everything product X\n  has?\" and I just had to be clear and be like, no! the competitor has a\n  reall pretty intricate complex system that we could never hope to\n  replicate. but does that matter? probably not, but even still, we likely\n  don't have even 20% of the full set of functions of a competitor.\n  luckily we have our own strengths that make us compelling besides that\n  20%\n- making it so our product requires a server side component to run,\n  where our first version was much more amenable to running as a static\n  site\n\n- etc...\n\nbut what does all this imply?\n\nthere are persistent confusion about what the challenges we face are,\nwhat the architectural needs are, what our user stores are, what our new\nv2 design goals are, and more. It's really crazy\n"},{"title":"Behind the release - the story of the bugs and features in JBrowse 1.16.0","date":"2018-12-17","slug":"2018-12-17","content":"\nEvery once in awhile, you might see that your favorite program, JBrowse,  has a\nnew release. There are a ton of little snippets in the release notes, you might\nas well just go ahead and upgrade, but what went into all those little fixes?\nGoing to the blog post has links to the github\nissues, \u003chttp://jbrowse.org/blog/2018/12/13/jbrowse-1-16-0.html\u003e but I felt\nlike maybe I'd add a little more context for some of them:\n\nPS This is sort of motivated by @zcbenz blog on Electron\n(\u003chttps://twitter.com/zcbenz\u003e \u003chttp://cheng.guru/\u003e) which tells the software in\nterms of actual commit messages and such.\n\n- The webpack build doing a production build by default. This seems pretty\n  straightforward, but was also difficult because I use WSL and the UglifyJs\n  plugin had trouble on WSL using the parallel: 4 option to use multiple\n  processors. This was really annoying and resulted in the webpack build just\n  hanging for no reason and only careful google-fu really uncovered other\n  people having this issue. I removed the parallelism as the speed gain wasn't\n  even really justifiable https://github.com/gmod/jbrowse/pull/1223\n\n- The incorporation of the `@gmod/bam` module. This was an almost 2 months\n  process after my first module, `@gmod/indexedfasta`. It required really\n  getting down to the binary level for BAM and was pretty tough. The module\n  has already itself had 12 releases [here](https://github.com/GMOD/bam-js/blob/master/CHANGELOG.md)\n\n- Added support for indexing arbitrary fields from GFF3Tabix files. This was\nfairly straightforward but required making design decisions about this.\nPreviously flatfile-to-json.pl files would have a command line flag to index\narbitrary fields. Since gff3tabix files are specified via config, I allowed\nspecifying arbitrary fields via config.\n\n- Added ability to render non-coding transcript types to the default Gene\nglyph. This one was a nice feature and enables you to see non-coding types, but\nrequired some weird design decisions because I could not override\nthe `box-\u003estyle-\u003ecolor` from a higher level type simply using the\n`_defaultConfig` function, so I needed to override the `getStyle` callback\nthat was passed down to the lower levels, so that it was able to use the\ndefault lower level style and also our non-coding transcript style. See this\npart of the code for\ndetails \u003chttps://github.com/GMOD/jbrowse/commit/ec638ea1cc62c8727#diff-a14e88322d8f4e8e940f995417277878R22\u003e\n\n- Added `hideImproperPairs` filter. This was fairly straightforward but it is one\nof these bugs that went unnoticed for years...the `hideMissingMatepairs` flag\nwould hide things didn't have the sam 0x02 flag for \"read mapped in proper\npair\", but reads with this flag could still be paired. Doing the 1.16 release\nthat focused on paired reads helped focus on this issue and now\nhideMissingMatepairs filters on \"mate unmapped\" and `hideImproperPairs` is\nthe \"read mapped in proper pair\"\n\n- Added `useTS` flag. This one is fairly straightforward, it is similar to\n  `useXS` which colors reads based on their alignment in canonical splice site\n  orientations. I figured I could just copy the `useXS` to the `useTS` since I\n  figured they are the same, but I went ahead and manually generated RNA-seq\n  alignments with minimap2 and found that the useTS is actually flipped the\n  opposite of `useXS`, so it was valuable to get actual test data here.\n\n- Fixed issue where some `generate-names` setups would fail to index features.\nThis was a bad bug that was brought to light by a user. I was kind of mind\nboggled when I saw it. In JBrowse 1.13-JBrowse 1.15 a change was introduced to\nname indexing with a memory leak. In JBrowse 1.15 that was removed. But, there\nwas another change where refseqs could return empty name records, because they\nwere handled separately. But if the initial fill up of the name buffer of 50000\nwas exceeded by the reference sequence, then there would be empty name records\nafter this point and cause the name indexing to stop. Therefore this bug would\nonly happen when the reference sequence indexing buffer exceeded 50000 items\nwhich could happen even when there are less than 50000 refseqs due to\nautocompletions\n\n-  Fixed issue with getting feature density from BAM files via the index stats\nestimation. This involved parsing the \"dummy bin\" from index files, and I found\nit was failing on certain 1000 genomes files. I actually don't really know what\nthe story behind this was, but our tabix code was better at parsing the dummy\nbins than my bam code, and it was the same concept, so I took a note from their\ncodebase to use it in bam-js code. Commit\nhere https://github.com/GMOD/bam-js/commit/d5796dfc8750378ac8b875615ae0a7e81371af76\n\n-  Fixed issue with some GFF3Tabix tracks having some inconsistent layout of\nfeatures. This is a persistently annoying fact in tabix files where we cannot\nreally get a unique ID of a feature based on it's file offset. Therefore this\ntakes the full crc32 of a line as it's unique ID.\n\n- Fixed CRAM store not renaming reference sequences in the same way as other\n  stores. This one was interesting because rbuels made a fix but it caused\n  features from one chromosome to show up on the wrong ones, so chr1 reads\n  where showing up on chrMT. This happened because it was falling back to the\n  refseq index if it chrMT wasn't in the embedded \"sam header\" in the CRAM\n  file, but it should only fallback to refseq index if there is not any\n  embedded \"sam header\" in the CRAM file.\n\n-  Fixed bug where older browsers e.g. IE11 were not being properly supported\nvia babel. This was a absolutely terrible bug that I found over thanksgiving\nbreak. It was a regression from 1.15 branch of JBrowse. Previous versions from\n1.13 when webpack was up until 1.15 used `@babel/env`. It was changed to\nbabel-preset-2015 but it was not being run correctly. Then I found that even if\nI did get it running correctly, it was unable to properly babel-ify the\nlru-cache module because it used something called\n`Object.defineProperty('length', ...)` to change how the length property was\nintepreted which was illegal in IE11. The 'util.promisify' NPM module also did\nthis in some contexts. I found that I could use the quick-lru module and the\nes6-promisify module instead of lru-cache and util.promisify as a workaround.\nThen I had to update all `@gmod/tabix`, `@gmod/vcf`, `@gmod/bgzf-filehandle`,\n`@gmod/indexedfasta`, `@gmod/tribble-index`, `@gmod/bam`, and JBrowse proper to\nuse these modules instead, and make the bable chain, which typically does not\nparse node_modules, to build these modules specifically (I didn't want to setup\nbabel toolchains for every single one of these modules, just one in the jbrowse\nmain codebase...). This was really a lot of work to support IE11 but now that\nworks so ...ya\n\n-  Fixed bug where some files were not being fetched properly when changing\nrefseqs. This was actually fixed when I changed out lru-cache for quick-lru and\nfixed a bug where the cache size was set to 0 due to a erroneous comment that\nsaid `50*1024 // 50MB`...of course it should have said `50*1024*1024 // 50MB` https://github.com/GMOD/jbrowse/commit/2025dc0aa0091b70\n\n- Fixed issue where JBrowse would load the wrong area of the refseq on startup\n  resulting in bad layouts and excessive data fetches. This was actually a\n  heinous bug where jbrowse upon loading would just navigateTo the start of the\n  reference sequence automatically and then to wherever was specified by the\n  user. This resulted in track data to start downloading immediately from the\n  start of the chromosome and resulted in for example 350 kilobases of\n  reference sequence from all tracks to start downloading, which when I was\n  implementing view as pairs, was causing me to download over 100MB routinely.\n  This was terrible, and after fixing I only download about 10MB over even\n  large regions for most BAM files. Additionally, this bug was causing the\n  track heights to be calculated incorrectly because the track heights would\n  actually be calculated based on distorted canvas\n  bitmaps. https://github.com/gmod/jbrowse/issues/1187\n\n- JBrowse Desktop was not fetching remote files. This was a weird issue where\n  remote file requests were considered a CORS requests to any external remote.\n  This was solved by changing the usage of the fetch API in JBrowse for\n  node-fetch which does not obey CORS. Note that electron-fetch was also\n  considered, which uses Chromiums network stack instead of node's, but that\n  had specific assumptions about the context in which it was called.\n\n-  Fixed issue where some parts of a CRAM file would not be displayed in\nJBrowse due to a CRAM index parsing issue. This was based on a sort of binary\nsearch that was implemented in JBrowse where the elements of the lists were\nnon-overlapping regions, and the query was a region, and the output should be a\nlist of the non-overlapping regions that overlap the query. Most algorithms for\nbinary search don't really tell you how to do searches on ranges so needed to\nroll up my sleeves and write a little custom code. An interval tree could have\nbeen used but this is too heavy-weight for non-overlapping regions from the\nindex https://github.com/GMOD/cram-js/pull/10\n\n-  Fixed an issue where BAM features were not lazily evaluating their tags.\nWhen a function `feature.get('blahblah')` is called on a BAM feature, it checks\nto see if it's part of a default list of things that are parsed like feature\nstart, end, id, but if not, it has to parse all the BAM tags to see if it is a\ntag. Since they are called \"lazy features\" the tag processing is deferred until\nit is absolutely needed. As it turned out, the incorporation of CRAM in 1.15\nwas calling a function to try to get the CRAM's version of CIGAR/MD on the BAM\nfeatures unnecessarily invoking the tag parsing on every feature up front and\ntherefore making the feature not really lazy anymore. This restored\nthe \"lazyness\" aspect of BAM.\n\n-  Fixed issue where CRAM layout and mouseover would be glitchy due to ID\ncollisions on features. In the 1.15 releases, CRAM was introduced, and we\nthought that the concept of taking CRC32 of the entire feature data days were\nover because there is the concept of a \"unique ID\" on the features. However,\nthis ID was only unique within the slices, so around the slice boundaries there\nwere a lot of bad feature layouts and mouseovers would fail because they would\nmap to multiple features, etc. I found a way to unique-ify this by giving it\nthe sliceHeader file offset. \u003chttps://github.com/GMOD/cram-js/pull/10\u003e\n\n- We also had behind the scenes work by igv.js team member jrobinso who helped\n  on the CRAM codebase to incorporate a feature where for lossy read names, so\n  that a read and it's mate pair would consistently be assigned the same read\n  name based on the unique ID mentioned above. There was also a rare issue\n  where sometimes the mate pair's orientation was incorrectly reported based on\n  the CRAM flags, but the embedded BAM flags correctly reported it.\n\n- Finally the paired reads feature. This was a feature that I really wanted to\n  get right. It started when garrett and rbuels were going to san diego for the\n  CIVIC hackathon, and we talked about doing something that matched a \"variant\n  review system\" that they had done for the IGV codebase, which involved\n  detailed inspection of reads. I thought it would probably be feasible for\n  jbrowse to do this, but I thought essentially at some point that enhancing\n  jbrowse's read visualizations with paired reads would be a big win. I had\n  thought about this at the JBrowse hackathon also and my discussions then were\n  that this was very hard. Overall, I invented a compromise that I thought was\n  reasonable which was that there can be a \"maxInsertSize\" for the pileup view\n  beyond which the pairing wouldn't be resolved. This allowed (a) a significant\n  reduction in data fetches because I implemented a \"read redispatcher\" that\n  would actually literally resolve the read pairs in the separate chunks and\n  (b) a cleaner view because the layout wouldn't be polluted by very long read\n  inserts all the time and also, for example, if you scrolled to the right, and\n  suddenly a read was paired to the left side of your view, it would result in\n  a bad layout (but with max insert size, the window of all reads within\n  maxinsertsize are always resolved so this does not happen) and finally ( c)\n  the paired arc view was incorporated which does not use read redispatching\n  and which can do very long reads. All of these things took time to think\n  through and resolve, but it is now I think a pretty solid system and I look\n  forward to user feedback!\n"},{"title":"Problems that I experienced with the HPCC","date":"2017-04-21","slug":"2017-04-21","content":"\nMany of these issues may be due to me being stubborn with a weird build system.\nNonetheless, they were baffling, and I had very little interest in debugging\nthese issues. I just wanted to get my science done after all!\n\n# Module load completely barfs with incomprehensible error\n\n    $ module spider bedtools\n    Using system spider cache file\n    /opt/software/lmod/bin/lua: /opt/software/lmod/4.1.4icer5/libexec/Spider.lua:662: attempt to perform arithmetic on a nil value\n    stack traceback:\n        /opt/software/lmod/4.1.4icer5/libexec/Spider.lua:662: in function 'Level1'\n        /opt/software/lmod/4.1.4icer5/libexec/Spider.lua:640: in function 'spiderSearch'\n        /opt/software/lmod/4.1.4icer5/libexec/lmod:967: in function 'cmd'\n        /opt/software/lmod/4.1.4icer5/libexec/lmod:1195: in function 'main'\n        /opt/software/lmod/4.1.4icer5/libexec/lmod:1222: in main chunk\n        [C]: ?\n\n# Linuxbrew is terribly confused by things that depend on gcc\n\n    brew install hello\n    ==\u003e Installing dependencies for hello: glibc, xz, gmp, mpfr, libmpc, isl, gcc\n    ==\u003e Installing hello dependency: glibc\n    Error: glibc cannot be built with any available compilers.\n    Install Clang or brew install gcc\n\nUsing module load Clang does not fix problem \\\u003e\\_\\\u003c\n\n# Compiling things manually on software machine does not work on interactive machine\n\n    $ mummer\n    Illegal instruction (core dumped)\n\n# Many modules have a secret dependency on loading other modules\n\n    $ module load LASTZ\n\n    Lmod Warning: Did not find: LASTZ\n\n    Try: \"module spider LASTZ\"\n    $ module load GNU\n    $ module load LASTZ\n    $ lastz\n    You must specify a target file\n    lastz-- Local Alignment Search Tool, blastZ-like\n      (version 1.03.02 released 20110719)\n    ...\n\nEtc etc.\n"},{"title":"How I learned to hate ORM (especially for data import scripts)","date":"2017-03-12","slug":"2017-03-12","content":"\nWhen I was tasked with making a new application for our websites, I was\ngiven several CSV files with some expectation that these files could\nbasically be just loaded into a database and jumped into production really\nquickly. If you are using R and Shiny to make a data visualization dashboard,\nespecially if it is read only, this can actually be a reality for you: load\nthose CSVs and just pretend you're a full featured database. I had to actually\ncreate some read write functionality though. This was sort of experimental for\nme and I'm not that well versed in databases, but I wanted to share my\nexperience\n\nWhen I started, I chose grails/groovy/hibernate/GORM as a platform to\nuse. This quickly turned into pain when I tried to make a data importer\nusing grails also.\n\nEach CSV row from the source file would have to be turned into many\ndifferent rows in the database because it represented multiple\nrelationships, example:\n\n![](/media/158300473458_0.png)\n\nInitially I made my data importer in grails, and was hardcoding column\nnames knowing full well this was really inflexible. At the same time I\nwas also trying to \"iterate\" on my database schema, and I'd want to\nre-import my data to test it out, but it was really really slow. I tried\nmany different approaches to try to speed this up such as cleanUpGorm,\nStatelessSessions, and other tricks, but it would take 10-20 minutes for\nimports on a 100KB input file.\n\nWhat I basically realised is that for bulk data import\n\n**1) Using the ORM is really painful for bulk import.**\n\n**2) If you can pre-process your data so that it is already in the\nformat the database expects, then you can use the CSV COPY command which\nis very fast**\n\n**3) If you can then abandon the ORM mentality and even ignore it as\na convenience factor, then you can embrace my database system itself**\n\nOverall, after all this work, it just seemed like ORM treats the\ndatabase as a danger and something to be heavily abstracted over, but I\nactually found joy in learning how to treat my database as a first class\ncitizen. Soon I started gaining appreciation of\n\n- using plain SQL queries\n- learning about full text search in postgres with ts_query\n- learning about triggers to make a \"last updated\" field get updated\n  automatically\n\nI am pretty happy this way, and although I miss some things like\ncriteria queries which are very powerful, I am happy that I can interact\nwith my database as a friend\n\nAt the very least, due to the fact that I now pre-process the data\nbefore database loading, I can now import large amounts of data super\nfast with the CSV COPY command\n"},{"title":"Plotting a coordinate on the screen","date":"2017-02-16","slug":"2017-02-16","content":"\nI always end up having to remember the math for plotting a coordinate on the\nscreen, for example an HTML5 canvas and end up stitching it together manually\n\nIf you step through the math it becomes very simple though\n\nSay you have a coordinate range of 1000 to 2000 that you want to plot in a\nHTML5 canvas of size 100px\n\nLet's do a quick example and then generalize. Let's say you want to plot the\nvalue 1500, and put it into screen coordinates, so you take that and subtract\nthe minimum of the range\n\n```\n1500-1000\n```\n\nSecond, you know your point is going to be halfway in the range, and in\ngeneral, to get this position, you divide now by the size of the interval you\nare plotting in, e.g. 2000-1000\n\n```\n(1500-1000)/(2000-1000) = 0.5\n```\n\nWe get 0.5 as expected. Then you multiply this proportion times the width of\nbox you are rendering in, e.g. 100 pixels wide, and get that you put your pixel\nat position 50px\n\nTo summarize, the general formula for plotting a point x in a range (x1,x2) on\na screen of width w is\n\n```\nw*(x -  x1) / (x2 - x1)\n```\n\nOf course same thing applies for y\n\n```\nh*(y - y1) / (y2 - y1)\n```\n\nThis does not take into account small possible adjustments for closed vs open\nranges, which could be important to avoid subpixel rendering on a canvas, but\nthat can be a further exercise\n"},{"title":"Creating a JBrowse plugin","date":"2016-11-10","slug":"2016-11-10","content":"\nI have been very impressed with peoples creativity and willingness to\ndig into all the details of JBrowse to customize it's features. One\ngreat way to do this in a modular way is to create a \"JBrowse plugin\".\nThis concept sounds hard but you can set up a simple couple of files and\nrefresh your browser and it will \"just work\"!\n\n**Introduction to the plugin mindset**\n\nIn a plugin, you can define new things like custom track types, custom\nadaptors to new file types, new track selectors, or something really\ndifferent. A key insight about the custom types of tracks and things\nthough is that you can put the name of your new custom class in the\njbrowse config file which will then find the code in your plugin and run\nit. Plugins can do other things, but the ability to just swap out track\ntypes or other components in the config file is a great benefit.\n\n**A scenario**\n\nOne example that was talked about on the mailing list might involve\nadding new menu items for a given track. We might consider a plugin that\ndefines a custom track type to add that functionality.\n\nBasically, we can use object- oriented principles to \"inherit\" from some\nexisting track type like CanvasFeatures and then extend its\nfunctionality by overriding the functions.\n\nIf you are not familiar with object-oriented javascript, dojo makes it\npretty easy (but, especially get a background on this if you need to,\nsee footnotes below).\n\nWe can inherit a new track type by using the \"define\" function to\ninclude the dependencies needed in a file, and they are listed in an\narray at the top of your file.\n\nThen the \"declare\" function creates a new class. The first argument to\ndeclare is the is your parent class, like CanvasFeatures, and we type\n\"return declare\" because we are returning our new track class from the\nmodule.\n\n```\n define( [\"dojo/_base/declare\",\n \"JBrowse/View/Track/CanvasFeatures\"],\n     function(declare,CanvasFeatures) {\n     return declare(CanvasFeatures, {\n         _trackMenuOptions: function() {\n\n             var opts=this.inherited(arguments); //call the parent\n classes function\n\n             opts.push( // add an extra menu item to the array returned\n from parent class function\n                 {       \n                     label: \"Custom item\",\n                     type: 'dijit/CheckedMenuItem',\n                     onClick: function(event) {\n                         console.log('Clicked');\n                     },  \n                     iconClass: \"dijitIconPackage\"\n                 }   \n             );  \n             return opts;\n         }   \n     });\n     }   \n );\n```\n\nCode listing 1. an example custom track type, MyTrack.js, that adds an\nextra track menu item\n\n**Now how do we make this a plugin?**\n\nIn the above section, we created a new track subclass with a custom menu\noption. How do we use this track? We want to turn it into part of afine\nthe boilerplate code from the [Writing\nplugins](http://gmod.org/wiki/JBrowse_Configuration_Guide#Writing_JBrowse_Plugins)\nguide.\n\n```js\n define([\n            'dojo/_base/declare',\n            'JBrowse/Plugin'\n        ],  \n        function(\n            declare,\n            JBrowsePlugin\n        ) {\n  \n return declare( JBrowsePlugin, // our plugin's main.js derives from\n the \"JBrowse/Plugin\" base class\n {\n     constructor: function( args ) {\n         /*don't necessarily have to do any initializing here, but you\n can get a handle to various jbrowse components if any initialization\n is needed from the args.browser variable*/\n     }   \n });\n });\n```\n\nCode listing 2. Our plugin's main.js\n\nAfter this, we create the plugin directory structure\n\n\u003e jbrowse/plugins/MyPlugin\n\u003e\n\u003e \u003e jbrowse/plugins/MyPlugin/js\n\u003e \u003e\n\u003e \u003e \u003e jbrowse/plugins/MyPlugin/js/main.js\n\u003e \u003e \u003e\n\u003e \u003e \u003e jbrowse/plugins/MyPlugin/js/MyTrack.js\n\nThen we can add our new plugin to a config file like jbrowse_conf.json\nas \"plugins\": [\"MyPlugin\"]  and then make a track in trackList.json\nhave \"type\":  \"MyPlugin/MyTrack\" instead of for\nexample \"type\": \"CanvasFeatures\". That will tell jbrowse to load the\nMyTrack class from your plugin instead of the normal CanvasFeatures\nclass.\n\nThat's about it!\n\nNote that the bin/new-plugin.pl script can automatically initialize some\nof this directory structure too. Try running \"bin/new-plugin.pl\nMyPlugin\" and see what happens.\n\nFootnotes:\n\nIt is important to understand the module format (AMD) which is what the\n\"define\" function is about and the dojo way of definining classes using\nthe \"declare\" function. Together, this allows the dojo to create\nobject-oriented programs that are modular in javascript. See\n\u003chttp://dojotoolkit.org/reference-guide/1.10/dojo/_base/declare.html\u003e\nand \u003chttp://dojotoolkit.org/documentation/tutorials/1.9/modules/\u003e\n(understanding this helps you understand the \"preamble\" for declaring a\njbrowse plugin)\n"},{"title":"Fixing spiky CPU issues with Tomcat","date":"2016-09-16","slug":"2015-09-16","content":"\nThe symptoms of spiking that we saw were simply that after light usage\nof the applications, the CPU usage would start spiking and rapidly cycle\nfrom many CPU cores (e.g. 2000% CPU usage) back to 0% CPU for no\napparent reason.\n\nWe now know this was due to memory issues and garbage collection, but it\nwas confusing because it wasn't strictly showing up as GC usage in\nJVisualVm (the GC usage, blue spikes on the left in fig 1, are small,\nbut the orange spikes are large, even though the memory issues are the\nproblem)\n\nHere is what it looked like during spiking (obviously, pushing the\nmemory limits here, a linked in article suggests having 6GB of \"newgen\"\nmemory, so on top of the old gen, tomcat needs a bunch more for the\nnewgen to make things happy.\n\n![](/media/129241954103_0.png)\n\nHere is what it looks like when it is not spiking\n\n![](/media/129241954103_1.png)\n\nEdit: See this follow up post for showing that increasing memory helps\n\u003chttp://searchvoidstar.tumblr.com/post/131229569383/tomcat-memory-debugging\u003e\n\n::: {#footer}\n[ September 16th, 2015 6:37pm ]{#timestamp} [tomcat]{.tag} [java]{.tag}\n[programming]{.tag} [coding]{.tag} [troubleshooting]{.tag}\n[intermine]{.tag} [bioinformatics]{.tag}\n:::\n\nexport default ({ children }) =\u003e \u003cLayout\u003e{children}\u003c/Layout\u003e\n"},{"title":"Installing clamav on OSX","date":"2016-06-20","slug":"2016-06-20","content":"\nIt is a common trope that OSX doesn't need anti-virus because everyone targets\nwindows. That is maybe comforting to some but I think it's pretty naive. It\nwould be better to have a system on your machine to tell you about viruses,\ntrojan horses, malware, or spying.  I have decided to employ a free open source\nscanner called clamAV \u003chttps://www.clamav.net/\u003e. I don't really know if it has\nany good features for Mac scanning but thought it could be fun to install\n\nClamAV is the top choice for linux based OSs being free and open source (GPL)\nvirus scanner.\n\nTo install we can use homebrew\n\n```\n    $ brew install clamav\n```\n\nThen there is s config file to setup. This is located\nin /usr/local/etc/clamav/freshclam.conf\n\nTo setup, edit this file and delete the line that says \"Example\" and\nthen uncheck the desired settings. I would chose to enable logging to\n/var/log/clamav.log and also database directories in /var/lib/clamav\n\nThen run the \"freshclam\" program\n\n```\n    $ freshclam\n```\n\nThis will download the virus scanner database (main) and daily scanning\nupdates\n\nThen you can run clamscan on a given directory (recursively, only print\ninfected files)\n\n```\n    $ clamscan -ri ~/\n```\n\nOr add this to a cronjob\n\n```\n    $ crontab -\n\n    @hourly clamscan -ri ~/ | mail -v -s \"clamscan results\" your.email@gmail.com  \u003e/dev/null 2\u003e\u00261\n\n```\n"},{"title":"Querying InterMine databases using R","date":"2016-06-17","slug":"2016-06-17","content":"\nIn the past, I had found some ways to do simple queries on InterMine web\nservices using basic HTTP commands with R (see\n\u003chttps://gist.github.com/cmdcolin/4758167bdd89e6c9c055\u003e)\n\nHowever, the InterMineR (\u003chttps://github.com/intermine/intermineR\u003e)\npackage automates some of these features and makes it easier to load the\ndata in R.\n\n**Installation**\n\nOne way to install InterMineR is to install from github with\nhadley/devtools\n\n    install.packages(\"devtools\")\n    devtools::install_github(\"hadley/devtools\")\n    devtools::install_github(\"intermine/intermineR\")\n\n**Usage**\n\nBasic usage includes loading the \"intermine URL\" using the initInterMine\nfunction. Then various functions can be called on this result.\n\n    library(InterMineR)\n    mine=initInterMine(\"http://bovinegenome.org/bovinemine/\")\n    getVersion(mine) #18, intermine API version\n    getRelease(mine) #1.0, our data release version\n    getTemplates(mine) # lists all templates on interminer\n\n**Run a template query**\n\nFrom the getTemplates function, if you see a template query that you\nwant to run, you can use the getTemplateQuery function with it's name,\nand run it with the runQuery function\n\n    getTemplateQuery(mine,\"TQ_protein_to_gene\") # see what template looks like\n    template=getTemplateQuery(mine,\"TQ_protein_to_gene\") # save template\n    runQuery(mine,template) # run the template query with default params, receive data.frame\n\nThis method is good, but some improvement could be added to change\ndefault parameters in the template query, etc.\n\n**Run query XML**\n\nAnother option for running queries is to use the query XML that you can\ndownload from the InterMine query result pages.\n\n```\n # get all Ensembl genes on chr28 from bovinemine\n query='\u003cquery model=\"genomic\" view=\"Gene.primaryIdentifier\n Gene.secondaryIdentifier Gene.symbol Gene.name Gene.source\n Gene.organism.shortName Gene.chromosome.primaryIdentifier\"\n sortOrder=\"Gene.primaryIdentifier ASC\" \u003e\u003cconstraint\n path=\"Gene.organism.shortName\" op=\"=\" value=\"B. taurus\"\n /\u003e\u003cconstraint path=\"Gene.chromosome.primaryIdentifier\" op=\"=\"\n value=\"GK000028.2\" /\u003e\u003c/query\u003e'\n\n results=runQuery(mine, query)\n\n head(results)\n```\n\n**Conclusion**\n\nThe InterMineR package has a couple of nice features for getting\nInterMine data with a couple of functions for looking at templates. For\nmany use cases, copying the Query XML from a InterMine webpage and\npasting that into the runQuery function is sufficient and produces a\ndata frame that can be analyzed.\n\nPS it is not easy to post XML on tumblr after editing the post in\nmarkdown mode. You have to add the lt and gt shortcuts and even after\nthat it gets filtered?!\n"},{"title":"How to make your resume.json or resume-cli look great","date":"2016-04-23","slug":"2016-04-23","content":"\nThere are a ton of themes for resume-cli that are not immediately\nobvious to find\n\nTo see all the great themes on the command line, check out\n\n```\n    curl http://themes.jsonresume.org/themes.json |jq .\n```\n\nI tried a bunch of them\n\n```\n   4679  resume export site/resume/index.html -t modern-freeland\n   4680  resume export site/resume/index.html -t modern-freelance\n   4682  resume export site/resume/index.html -t modern-with-projects-section\n   4683  resume export site/resume/index.html -t dangerflat\n   4684  resume export site/resume/index.html -t striking\n   4685  resume export site/resume/index.html -t crisp\n   4686  resume export site/resume/index.html -t semantic-ui\n   4687  resume export site/resume/index.html -t material\n   4688  resume export site/resume/index.html -t modern-extended\n   4689  resume export site/resume/index.html -t paper\n   4690  resume export site/resume/index.html -t smart\n   4691  resume export site/resume/index.html -t flat\n\n```\n\nNote: resume.json is setup to use HTML themes, so even though it has a\nPDF output option, it is inherently converting HTML first and then to\nPDF. The PDF conversion is done by a automated cloud service, which\ncurrently can fail sometimes. It is probably better to just choose HTML\nand convert to PDF if you need to.\nSee \u003chttps://github.com/jsonresume/resume-cli/issues/94\u003e\n"},{"title":"Creating a testing framework for JBrowse plugins","date":"2016-04-19","slug":"2016-04-19","content":"\nTesting client side apps requires a couple of tedious steps: Organizing\nthe git clone, the dependencies, wrangling up a web server, the test\nframework, etc.\n\nWhen testing a plugin for jbrowse, the dependency tree is interesting\nbecause the plugin \"depends\" on JBrowse to run, but we will use\ntravis-CI and bower inside the git repo for our plugin to accomplish\nthis.\n\nIn this scenario, we will\n\n1.  Use bower to install jasmine and JBrowse (our platform that we write\n    the plugin for)\n\n2.  Use nginx to launch a webserver on travis-CI\n\n3.  Use the phantomjs run-jasmine.js script to check jasmine test\n    results\n\nWithout further ado\n\nHere is the .travis.yml\n\n    sudo: false\n    addons:\n      apt:\n        packages:\n        - nginx\n    cache:\n      apt: true\n      directories:\n      - $HOME/.cache/bower\n    before_install:\n      - npm install -g jshint bower\n    install:\n      - bower install\n    before_script:\n      - cat test/travis.conf | envsubst \u003e test/travis-envsubst.conf\n      - nginx -c `pwd`/test/travis-envsubst.conf\n    script:\n      - phantomjs test/run-jasmine.js http://localhost:9000/test/\n      - jshint js\n\nRefer to\n\u003chttp://searchvoidstar.tumblr.com/post/141858047213/running-nginx-on-containerised-travis-ci-pt-2\u003e\nfor details on the nginx setup\n\nHere is the bower.json\n\n    {\n      \"name\": \"sashimiplot\",\n      \"homepage\": \"https://github.com/cmdcolin/sashimiplot\",\n      \"description\": \"Sashimi track type for jbrowse\",\n      \"main\": \"js/main.js\",\n      \"keywords\": [\n        \"bioinformatics\",\n        \"jbrowse\"\n      ],\n      \"license\": \"MIT\",\n      \"ignore\": [\n        \"**/.*\",\n        \"node_modules\",\n        \"bower_components\",\n        \"src\",\n        \"test\",\n        \"tests\"\n      ],\n      \"devDependencies\": {\n        \"jasmine-core\": \"jasmine#^2.4.1\",\n        \"jbrowse\": \"git://github.com/GMOD/jbrowse.git#master\"\n      }\n    }\n\nThe key thing here is that it installs jasmine and JBrowse. I set\n.bowerrc to install both jasmine and JBrowse to the \"test\" directory\n\n    {\n        \"directory\": \"test\"\n    }\n\nWith this setup, bower will make a \"flat dependency tree\" in the test\ndirectory, so it will look like this\n\n    $ ls -1 test\n    FileSaver\n    dbind\n    dgrid\n    dijit\n    dojo\n    dojox\n    *index.html*\n    jDataView\n    jasmine-core\n    jbrowse\n    json-schema\n    jszlib\n    lazyload\n    put-selector\n    *run-jasmine.js*\n    *spec*\n    *travis.conf*\n    util\n    xstyle\n\nHere the asterisks indicate things that are part of our app, other's are\nautomatically installed by bower (jbrowse, jasmine-core, the dojo\ndependencies, and other things)\n\nThen we can create the jasmine test/index.html to be something like this\n\n    \u003c!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"\n      \"http://www.w3.org/TR/html4/loose.dtd\"\u003e\n\n\n      \u003cmeta\u003e\n      Jasmine Spec Runner\n\n      \u003clink rel=\"stylesheet\" href=\"jasmine-core/lib/jasmine-core/jasmine.css\"\u003e\u003cscript src=\"jasmine-core/lib/jasmine-core/jasmine.js\"\u003e\u003c/script\u003e\u003cscript src=\"jasmine-core/lib/jasmine-core/boot.js\"\u003e\u003c/script\u003e\u003cscript type=\"text/javascript\" src=\"dojo/dojo.js\" data-dojo-config=\"async: 1\"\u003e\u003c/script\u003e\u003cscript type=\"text/javascript\"\u003e\n        require( { baseUrl: '.',\n                   packages: [\n                       'dojo',\n                       'dijit',\n                       'dojox',\n                       'jszlib',\n                       { name: 'lazyload', location: 'lazyload', main: 'lazyload' },\n                       'dgrid',\n                       'xstyle',\n                       'put-selector',\n                       'FileSaver',\n                       { name: 'jDataView', location: 'jDataView/src', main: 'jdataview' },\n                       { name: 'JBrowse', location: 'jbrowse/src/JBrowse' },\n                       { name: 'SashimiPlot', location: '../js' }\n                   ]\n        });\n      \u003c/script\u003e\u003cscript type=\"text/javascript\" src=\"spec/SashimiPlot.spec.js\"\u003e\u003c/script\u003e\u003cdiv id=\"sandbox\" style=\"overflow:hidden; height:1px;\"\u003e\u003c/div\u003e\n\nThe \"packages\" in the require statement puts all these packages in the\nright \"namespace\" for the AMD includes, and the \"specs\" are defined like\n`\u003cscript type=\"text/javascript\" src=\"spec/Projection.spec.js\"\u003e\u003c/script\u003e`\n\nFinally, run-jasmine.js is used to check the results of the jasmine\ntests (it is run via phantomjs in the travis-CI script). It is a special\nversion for the most recent version of jasmine (2.4)\n\u003chttps://gist.github.com/vmeln/b6cbb319d9a0efc816be\u003e\n\nFor an example of the project using this, see\n\u003chttps://github.com/cmdcolin/sashimiplot\u003e\n"},{"title":"Creating a docker image","date":"2016-04-17","slug":"2016-04-17","content":"\nExample\n\n```\n    brew install docker boot2docker docker-machine\n    docker-machine create --driver virtualbox default\n    docker-machine env default # will output some variables\n    eval \"$(docker-machine env default)\" # use those variables\n    # make dockerfile\n    docker build -t nameof-yourimage .\n```\n"},{"title":"Basic command line productivity tricks and learning experiences","date":"2016-04-06","slug":"2016-04-06","content":"\n- dd deletes line in vim\n- Ctrl+d scrolls down in vim\n- Learn to love your package manager. Homebrew, NPM, gem, cpanm,\n  gvm/sdkman, etc. these all do amazing things\n- Once you learn bash, try zsh and oh-my-zsh, they have things like\n  case-insensitive tab completion\n- Don't make scripts that hardcode paths, make reusable command line\n  scripts. Use bash as your \"REPL\", not R.\n- git log -p helps analyze your log files in full details (make sure\n  autocoloring is turned on in your terminal)\n- There are keys to jump forward and backwards on the command line\n  text editor, learn them...don't scroll one char at a time\n- Learn how \"PATH\" works. Generally it is just a set of directories\n  connected by \":\" separators. You can add things to the path by\n  saying \"export PATH=$PATH:/new/directory/to/add\" and you can add\n  this to ~/.bashrc for example\n- When your install process for a command line tool seems like\n  nonsense, try homebrew instead. barring that, learn PATH, and how to\n  run \"make install\", etc. Most of your headbashing from installing\n  programs is 90% can be explained by not understanding how the\n  developer is intending it to be used, 10% of the tool's install\n  process being wrong\n- Get a static analyzer and basic tests going on your codebase and run\n  it on travis-ci. Getting started with travis-ci is kind of a\n  learning curve, but it is worth it\n- Use cpanm instead of cpan for package management\n- Vocabulary learning curve: catalina is the same thing as tomcat.\n  CATALINA_HOME is the same thing as the tomcat folder\n- alias ll=\"ls -l\", because I type \"ll\" hundreds of times a day.\n- For irc productivity, run irssi on a server in a \"screen\"\n  e.g. \"screen irssi\" and then you can come back to conversations\n  later by just logging into the server with ssh\n- Edit ~/.ssh/config to include your hostnames so you don't have to type out\n  long ssh\n  commands http://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/\n- Use spaces instead of tabs in your source code (\u003e:( yes I think\n  this is the one true way)\n- Try out nodejs and browserify in your spare time to make a \"npm\"\n  based app in the browser. it's fun.\n- Similarly, try making a simple \"api\" endpoint on the server side\n  with express.js or similar. can get started very quickly.\n- Learn how to get a mindset of writing tests. You can write tests\n  proactively (i.e. Test driven development), but you can also write\n  them \"reactively\" too (i.e. if have a bug that you fix, you can make\n  a test to make sure this doesn't happen anymore)\n- Similar to above, tests in this sense are more \"sanity checks\" than\n  they are formal proofs. Take \"assert\" logic and \"debugging\" code out\n  of main codebase and put them in tests\n- Minimize comments in your code, and also don't comment out code and\n  leave it present. Find a way to delete it and move on!\n- When you have a bunch of .orig files after doing a git merge, just\n  use git clean -f to get rid of them. Similarly, to get rid of\n  everythng, including things in your gitignore file (i.e. a super\n  clean) use git clean -fdx. It has a --exclude argument too\n\n::: {#footer}\n[ April 6th, 2016 4:23pm ]{#timestamp}\n:::\n\nexport default ({ children }) =\u003e \u003cLayout\u003e{children}\u003c/Layout\u003e\n"},{"title":"Running nginx on containerised travis-CI pt 2","date":"2016-03-28","slug":"2016-03-28","content":"\nThere are several guides out there about how to setup nginx on travis-CI\nbut I still found it to be a challenge, especially finding a modern one\nthat works with the containerized builds. I was frustrated that things\nlike `SimpleHTTPServer` from python and http-server from npm did not have\nfully enough features to run our app either (a complex \"static-site\ngenerator\" type thing you might say), and I was also too lazy to setup\n\"sauce labs\" (which I have not used, but presume has some better ability\nto run functional/browser tests).\n\nEssentially, the problem with running nginx under the containerized\nbuild is that it \"likes to be sudo\", with many logfiles by default going\nto different places that only sudo has access to.\n\nThis link is probably the most similar to the technique I use here, but\nit is now gone (?) and must be accessed through the internet archive!\n\n[](http://www.doublesignal.com/running-nginx-on-containerised-travis-ci)\u003chttps://web.archive.org/web/20150919050719/http://www.doublesignal.com/running-nginx-on-containerised-travis-ci\u003e\n\nMy technique is very similar, however I use an extra trick to set the\nfile root to the current directory (instead of /tmp/nowhere as in the\nlink) by using \"envsubst\" to replace variables in the nginx config file.\n\nWithout further ado, the .travis.yml can look like this\n\n```\n    sudo: false\n    addons:\n      apt:\n        packages:\n        - nginx\n    install:\n      - cat tests/travis.conf | envsubst \u003e tests/travis-envsubst.conf\n      - nginx -c `pwd`/tests/travis-envsubst.conf\n    script:\n      - wget http://localhost:9000/yourfiles\n```\n\nThen your nginx config file can look like this\n\n```\n    worker_processes 10;\n    pid /tmp/nginx.pid;\n\n    error_log /tmp/error.log;\n\n    events {\n        worker_connections 768;\n    }\n\n    http {\n        client_body_temp_path /tmp/nginx_client_body;\n        fastcgi_temp_path     /tmp/nginx_fastcgi_temp;\n        proxy_temp_path       /tmp/nginx_proxy_temp;\n        scgi_temp_path        /tmp/nginx_scgi_temp;\n        uwsgi_temp_path       /tmp/nginx_uwsgi_temp;\n\n        server {\n            listen 9000 default_server;\n\n            server_name localhost;\n            location / {\n                root $TRAVIS_BUILD_DIR;\n                index  index.html index.htm;\n            }\n            error_log /tmp/error.log;\n            access_log /tmp/access.log;\n        }\n    }\n\n```\n\nThen, when travis-CI is run, it uses envsubst to replace\n`$TRAVIS_BUILD_DIR` in the tests/travis.conf file, and then boots up\nnginx\n"},{"title":"On over-reproducibility","date":"2016-03-05","slug":"2016-03-05","content":"\nRecently, some posts were made by\n\u003chttps://twitter.com/arjunrajlab\u003e about how perhaps we are aiming at\n\"over-reproducibility\". I think this is interesting, and would generally\nagree that not everyone needs to achieve total automation of their whole\npipeline, but I think the post does a lot of \"blaming your tools\" and\ndisparaging good development practices with regards to version control\nand figure generation.\n\nI think that the complaint that version control and automated figures\nare not for everyone is probably true, but it is overgeneralizing a\ndifferent problem. For example, students are not \"trained\" to work with\nGit, and they are not \"trained\" to do software engineering. In fact,\neven computer science students are not generally \"trained\" to do any of\nthose things (computer science != software engineering). But that\ndoesn't mean that your lab needs to forego using all those tools.\nSoftware development can be incredibly complex and sophisticated, but\nit's important to make sure things are \"done right\"! High-quality and\neasy-to-reproduce software is really about process, and engineering. But\nthat is also why there is no one-true-way for reproducibility. Maybe\nArjun doesn't have a reproducible workflow right now, but what about 5\nyears down the road, where he suddenly has a great framework for such\nthings? This happens all the time in software development (for example,\nhow long ago was it that \"push to deploy\" did not exist? how often would\nyou just edit your files live on your site? now that is seen as bad\npractice!), but that said, processes for software quality can evolve\npretty organically, so even though some best practices exist, people can\ngrow their own quality environment.\n\nEven if we agree that software development+version control=good, there\nare still a lot of complaints about it in the blogpost. For example, the\ncomplaint that git is too hard is pretty silly, and the xkcd comic about\ncalling over graph theorist doesn't really help. As a software developer\nat work, I think that version control simply helps define a disciplined\nway of working. Version control makes you analyze your progress,\nsummarize it as a commit message, format the code properly, make sure it\npasses tests, and then talk to your collaborators about accepting it.\nDropbox might accomplish some of those things, but I would really doubt\nthat it is covering that full scope. Arjun seems to agree with using\nversion control for some of his labs software development, so again,\nthere is a spectrum of needs being met. Nevertheless, there are some\nweird comments about whether commit messages are like a \"lab notebook\",\nbut hint: they are not, write documentation for your project or keep a\nseparate journal or blog or wiki. Commit messages in my opinion should\nbe about one line, and the changes should be very self explanatory. But\nanother big argument in the blogpost is whether version control works\nfor something like paper writing, and I believe that this underscores\nsomething else: that paper writing is really a pretty messy procedure.\n\nI think that perhaps the \"google docs\" mode of writing is probably\npretty ok for many things, but it still needs a gatekeeper to\nincorporate the comments from coauthors and reviewers into the document\nin an organized way. In my experience as a \"gatekeeper\" with writing my\nsenior thesis, I organized my paper using knitr, and I automated figures\nbeing generated by R wherever possible, and then I would convert the\npaper to .docx to share with my advisors. Then I would take their\ncomments on the .docx and incorporate it back into my paper. This could\nbe seen as burdensome (\"why not just use google docs\"), but I felt that\nit was a good way to incorporate review into a reproducible workflow.\n\nNow, my pipeline precludes your PI from having to learn git to make a\npull request on your paper. That's a good thing... and we still have\nreproducibility.  But what about the figures themselves? I said I had\nknitr for reproducible figures, but what about everyone else? I think\nfigures have high value, and so people might want to have more\nreproducibility invested in them. In the blog post, it was claimed that\nmaking \"complex\" pub-quality figures was difficult (i.e. the plea for\nAdobe Illustrator), but look at the annotation functions from ggplot2,\nand multifaceted images. I found these annotation functions to be very\neasy to pick up. There is also the on-going debate about ggplot2 vs base\ngraphics on the simplystatistics blog, which covers making publication\nquality figures, and last I checked, I think the ggplot2′ers were\nwinning. I don't know how it works in high profile journals like Nature,\nbecause it looks like they just re-do all the figures to make them have\nsome consistent style, but that doesn't mean your original figure should\nbe irreproducible.\n\nThe debate about reproducible figures is pretty tangible too in things\nlike microscopy images. Simply look at the large amount of discussion\nfrom pubpeer about image fraud and possible duplications. The pubpeer\ncommunity obviously has some pretty sophisticated tools for hunting out\npossibly manipulated microscopy images. These types of things also lead\nto investigations, and you can see in the high-profile retraction case\nover STAP cells that it looks like the investigating committee were\nsimply asking how some figures were made, and upon finding that lab\nmembers don't know, a paper was retracted. The RetractionWatch blog\ncovers these\ninvestigations \u003chttp://retractionwatch.com/2016/02/26/stap-stem-cell-researcher-obokata-loses-another-paper/\u003e\n\nYou can't depend on other people to back your figure up, so you need to\ntake responsibility for making sure your papers and your work are\nreproducible (and, there is a spectrum for reproducibility, but I\nbelieve that version control is a great example of highly disciplined\nwork). I also think that just having folders on some hard drive is not a\ngood way to do things either. There is a saying in software development\nthat is \"if it's not in version control, it doesn't exist\". That's not\nto say that version control is for everything, big data obviously has\ntrouble with being stored in git. But that shouldn't block you from\ncreating reproducible analyses.\n\nAnother example from the over-reproducibility blogpost says that if you\nhave \"analysis1″ and \"analysis2″, then version control advocates would\ntell you to delete analysis1 and just remember that it is in your\nhistory. I think that this is just a different issue. If you actually\ncare about both analyses, just make them separate repositories, with\nbasic README.md files explaining each them, and stop worrying about it.\nHaving one repository containing too many miscellaneous scripts is\nactually an anti-pattern. Stop making repositories\ncalled \"bioinfo-scripts\" that just contain a mish-mash of analysis\nscripts! Make your work purpose driven and do tasks. Also, this is an\nargument against REPL tools: your R REPL history is not a reproducible\nscript. Make your code into a script that generates well defined\noutputs. Windows users: you might not understand this because the\ncommand line on windows is crippled, but you have to make things run on\nthe command line.\n\nNow I wish I could say that I live by my words, but having been involved\nin coauthoring several papers, I will just have to admit that it is\nreally a messy procedure despite my best intentions as an editor and\ncoauthor. I wish things would be better!\n\nOn over-reproducibility: there is no such thing! There are pretty good\narguments to really automate most of a process, especially if it is done\nrepeatedly, to remove human errors, because meat-machines genuinely do\nthings wrong all the time.\n\nAnd, as my parents would say around the dinner table: \"you can always\nhave more, but you can never have less\"...so, you're not going to get to\na point of over-reproducibility. We shouldn't cargo cult it as the only\nway to do science but it's not a bad thing to have.\n"},{"title":"Cheating in your computer science class by copying from stackoverflow","date":"2015-12-17","slug":"2015-12-17","content":"\nI would like to tell a story about how I provided some personal tutoring\nhelp for a friend in a computer science class, and talk about a nagging\nfeeling that really felt wrong for me.\n\nSo, a long time ago, in a land far far away, a friend took an\nintermediate class on C++. I was first updated on his progress when he\nemailed me to get some help with some compiler errors. I was happy to\nhelp the young padawan. Here was the error:\n\n```\n         test.cpp:42:43: error: non-ASCII characters are not allowed outside of literals\n                and identifiers\n              for (startScan = 0; startScan \u003c (size − 1); startScan++)\n                                                    ^~\n```\n\nNow, what does this say to you? For me, it was actually very clear what\nthe error meant. It simply meant that this code was taken from\nsomewhere, and copied and pasted into the compiler. I know that because\nif they had typed it themself, they definitely would not get this error,\nbecause it is the error that implies something was automatically\nconverted to a unicode dash, mostly something done during copying and\npasting. At this point, I just kind of laughed, and helped him fix that.\nI showed how the compiler is actually pretty smart and can help fix\nthese errors and then I said \"l8r dude\".\n\nThe next week, I had another skype meeting with him, and this time I\nwanted to help a little more. It was pretty clear when we started that\nhe was using code that was copied and pasted again. I said, \"uh,\nok,....I'm not sure we need that now, but let's just keep going\", and\nthen I sat down and started helping. I wanted to help get all the\ndetails of the program working, so I helped guide the solution. Each\ntime we needed to test the program, it required repeating some input\nlines via `cin \u003e\u003e`, which is really annoying (obviously, you should test\nyour code with unit tests, but universities don't teach that, a rant for\nanother day). Anyways, it took awhile, because coding really does just\ntake time, but in the end he finally got it fixed and I said great job,\nand he turned it in!\n\nNow, on my friends last assignment, I got another call for help, and\nwhen we started skype, I found yet again that he had copied code from\nsomewhere, which included a C++ class and a main function for doing\nbinary trees. I just simply said \"dude, delete that, we don't need it\"\nand so he deleted it, but I think maybe he had worked on this copied\ncode for awhile, and maybe felt it was kind of his, so was apprehensive.\nI insisted though. Then we walked through the assignment again, very\nslowly. I spent probably 2-3 hours helping him out that night. During\nthose hours, I saw him continually making many programming mistakes such\nas just not knowing how to declare variable or a function properly, or\njust not knowing what to do next. This was kind of frustrating!!! But I\nwanted to absolutely teach him how to make it right! I was patient\nthough, and I wanted to teach a fun lesson, so I showed how you can do\nsome \"unit tests\" which avoids having to constantly re-enter your data\nvia `cin \u003e\u003e`....\n\nNow, the padawan completed his C++ class, and then we all were happy\never after....but a disturbance in the force was sensed...\n![image](http://zelcs.com/wp-content/uploads/2013/02/stackoverflow-logo-dumpster.jpg)\nImage from \u003chttp://zelcs.com/this-is-why-stackoverflow-sucks/\u003e\n\nI was reminded about all this due to seeing that [StackOverflow is now\nchanging their \"license\" over all the little snippets of\ncode](http://meta.stackexchange.com/questions/271080/the-mit-license-clarity-on-using-code-on-stack-overflow-and-on-the-stack-excha)\nthat are posted on their site. It just makes me reflect on literally HOW\nOFTEN PEOPLE JUST COPY AND PASTE FROM THERE. They might understand what\nthey are doing, or they seriously might not!!! I think it is a real\nproblem that people sometimes do not understand, but I cannot deny that\nit can be helpful too.\n\nIf I reflect on education in general, I recall when I took a University\nlevel physics class... it was really hard! We had to enter our validated\nsolutions for the math problems into a computerized website homework\nportal, and that involved being 100% correct about things. Now, what if\nthere was just a physicsoverflow, where they not only had Q\u0026A, but they\nhad \"programs\" that gave you all the right answers to your homework\nproblems that you could just copy and paste and use as solutions to your\nhomework? This isn't even in the realm of asking for \"homework help\"\nanymore, this is just pure cheating if you can copy your answers from\nsomewhere. It is disappointing though because this is what people are\ndoing in computer science!! These students are missing out on basic\nunderstanding of code. !!thisIsNotOk();\n\nNow, at least when I was being a tutor for my friend, I felt like my\nadvice helped my friend learn some things, not just give answers. But\nwhat if I was not there? I guess there is a certain \"impersonal quality\"\nthat makes asking Google/StackOverflow for answers less like\nconventional \"cheating\", but that is still wrong. I think it would be\ngood if more expert knowledge was available for all people, and not just\ncopy and paste snippets. As a start, I thought that [this post by Philip\nCompeau and Pavel\nPevzner](http://cacm.acm.org/magazines/2015/10/192385-life-after-moocs/fulltext)\n(who teach a Bioinformatics Algorithms MOOC on Coursera) was very\ninteresting, and I really liked their quote:\n\n\"Online education should move toward replicating the experience of\nreceiving one-on-one tutoring.\"\n\nThat sounds great, but how can this be acheived? And how can it be done\nright? I think it really requires the student to \"learn how to learn\"\n\nIf I think back to a long time ago, I remember being in 4th or 5th grade\nand I did a book report on World War 1, and I went to the library. I\nremember desperately flipping through pages of a 100 page book to try to\nfind some snippets of information to support some basic idea that I\nwanted to talk about. Maybe I wanted to know something specific, but the\nproblem was that I wasn't REALLY READING THE BOOK! I probably could have\nhad a better understanding of the topic if I had just read it, or even a\npart of it, and asked for help, but instead I just picked and chose\nsnippets from the book to \"sound smart\". I am very guilty of this type\nof error in many instances throughout my school career, so I am no\nsaint! I even have a phrase to describe this style of learning...I call\nit \"predatory learning\" and it is probably the worst kind of learning\nstyle. Predatory learners often pick and choose from scraps of info, but\nthey never get a full meal!\n\n::: {#footer}\n[ December 17th, 2015 3:05am ]{#timestamp} [learning]{.tag}\n[education]{.tag} [computer science]{.tag} [stackoverflow]{.tag}\n[fail]{.tag}\n:::\n"},{"title":"Killing postgres the hard way","date":"2015-10-22","slug":"2015-10-22","content":"\nSo today, I finally decided to do something about a query that we saw\nhad been running for 25 DAYS on our server\n\nNote: If you find this post and you need to follow the hard way, backup\nyour data first if possible.\n\nFirst I could obviously see the culprit: each postgress query runs it's\nown process so I could see in \"htop\" that there was this process that\nhad been running for 600 hours, or about 25 days\n\nNext, I opened a psql console and ran this query:\n\n```\n\u003e SELECT datname,procpid,current_query FROM pg_stat_activity WHERE\n\u003e datname='database_name' ORDER BY procpid ;\n```\n\nThis returns which actual queries are being run on the database at any\ngiven time.  I could easily see the one problematic query being run,\nwhich was a badly constructed intermine template query that resulted in\na weird \"recursion\" essentially.\n\nI wanted to try just terminating this query itself, so I ran this\n\n```\n\u003e SELECT pg_cancel_backend(29033);\n```\n\nEach time I ran it, it would say it returned one result but it did\nnothing.\n\nI also read that you can try to nicely \"kill\" it from the command line\n(no kill -9) so I ran\n\n```\n\u003e kill 29033\n```\n\nThis also did not work!\n\nI thought perhaps all these problems were because tomcat was still\nactive, so we shut down tomcat, and retried killing the specific query,\nbut to no avail\n\nAt this point, I just wanted to restart the whole database server. Kind\nof a risky move... but I am sort of a risky kind of guy...(that is not a\ngood thing with databases). If you are doing this, make backups! I\ndidn't. Luckily I suffered no data loss but what follows is kind of\nintense.\n\nSo first, I try and stop the database service\n\n```\n\u003e service postgresql-9.1 stop\n```\n\nUnfortunately, this `[FAILED]` ! And of course, even though it failed,\nthe database is now unusable. No logging into it anymore, we have to go\nwith the hard way now...\n\nLooking at /etc/init.d/postgres-9.1 told me that the service stop\ncommand was effectively using something like this:\n\n```\n\u003e pg_ctl -D /db/postgres/data -m fast stop\n```\n\nAfter some reading, I learned that you can try using a slightly\ndifferent flag to restart it\n\n```\n\u003e pg_ctl -D /db/postgres/data -m immediate  stop\n```\n\nI ran this and to my horror/surprise, it actually worked! At this point\nI decided to start postgresql back up again!\n\n```\n\u003e service postgresql-9.1 start\n```\n\nThe service start quickly returned a SUCCESS, which was great, but then\nI tried to start a psql console and the console froze on me! I could not\neven ctrl+c it!\n\nI got really worried at this point and I looked at the process manager,\nand saw that there was one postmaster process running but it was not\nclear what it was doing. I actually tried to shutdown the server again\nin a panic mode but at this point it said\n\n```\n\u003e /usr/pgsql-9.1/bin/pg_ctl stop -D /db/postgres/data/ -m immediate\n\u003e waiting for server to shut\n\u003e down...............................................................\n\u003e failed\n```\n\nIt was probably good that it didn't shut down, because I would quickly\nfind out that it was in recovery mode.  I looked at the postgresql logs\nand I saw this, reproduced here for full detail (from before the\nshutdown to the restart)\n\n```\n\u003e WARNING:  pgstat wait timeout\n\u003e WARNING:  pgstat wait timeout\n\u003e WARNING:  pgstat wait timeout\n\u003e WARNING:  pgstat wait timeout\n\u003e WARNING:  pgstat wait timeout\n\u003e WARNING:  pgstat wait timeout\n\u003e WARNING:  pgstat wait timeout\n\u003e WARNING:  pgstat wait timeout\n\u003e\n\u003e ERROR:  canceling statement due to user request\n\u003e STATEMENT:  CREATE TABLE precomp_90519 AS SELECT DISTINCT a1_.id AS\n\u003e a1_id, a2_.id AS a2_id, a3_.id AS a3_id, a4_.id AS a4_id,\n\u003e a5_.id AS a5_id, a6_.id AS a6_id, a12_.id AS a12_id, a10_.id AS\n\u003e a10_id, a1_.id AS a13_, a1_.primaryIdentifier AS a14_,\n\u003e a1_.secondaryIdentifier AS a15_, a2_.type AS a16_, a3_.name AS\n\u003e a17_, a4_.primaryIdentifier AS a18_, a5_.primaryIdentifier AS\n\u003e a19_, a6_.shortName AS a20_, a12_.identifier AS a21_, a10_.code\n\u003e AS a22_ FROM Gene AS a1_, Homologue AS a2_, Organism AS a3_, Gene\n\u003e AS a4_, Gene AS a5_, Organism AS a6_, GOAnnotation AS a7_,\n\u003e GOEvidence AS a8_, OntologyTerm AS a9_, GOEvidenceCode AS a10_,\n\u003e OntologyAnnotation AS a11_, OntologyTerm AS a12_, GeneGoAnnotation\n\u003e AS indirect0, EvidenceGOAnnotation AS indirect1 WHERE a1_.id =\n\u003e a2_.geneId AND a1_.organismId = a3_.id AND a2_.geneId = a4_.id\n\u003e AND a2_.homologueId = a5_.id AND a5_.organismId = a6_.id AND\n\u003e a1_.id = indirect0.Gene AND indirect0.GoAnnotation = a7_.id AND\n\u003e a7_.id = indirect1.GOAnnotation AND indirect1.Evidence = a8_.id AND\n\u003e a7_.ontologyTermId = a9_.id AND a8_.codeId = a10_.id AND a9_.id =\n\u003e a11_.ontologyTermId AND a11_.ontologyTermId = a12_.id ORDER BY\n\u003e a1_.primaryIdentifier, a1_.secondaryIdentifier, a2_.type,\n\u003e a3_.name, a4_.primaryIdentifier, a5_.primaryIdentifier,\n\u003e a6_.shortName, a12_.identifier, a10_.code, a1_.id, a2_.id,\n\u003e a3_.id, a4_.id, a5_.id, a6_.id, a12_.id, a10_.id\n\u003e LOG:  could not send data to client: Broken pipe\n\u003e STATEMENT:  CREATE TABLE precomp_90519 AS SELECT DISTINCT a1_.id AS\n\u003e a1_id, a2_.id AS a2_id, a3_.id AS a3_id, a4_.id AS a4_id,\n\u003e a5_.id AS a5_id, a6_.id AS a6_id, a12_.id AS a12_id, a10_.id AS\n\u003e a10_id, a1_.id AS a13_, a1_.primaryIdentifier AS a14_,\n\u003e a1_.secondaryIdentifier AS a15_, a2_.type AS a16_, a3_.name AS\n\u003e a17_, a4_.primaryIdentifier AS a18_, a5_.primaryIdentifier AS\n\u003e a19_, a6_.shortName AS a20_, a12_.identifier AS a21_, a10_.code\n\u003e AS a22_ FROM Gene AS a1_, Homologue AS a2_, Organism AS a3_, Gene\n\u003e AS a4_, Gene AS a5_, Organism AS a6_, GOAnnotation AS a7_,\n\u003e GOEvidence AS a8_, OntologyTerm AS a9_, GOEvidenceCode AS a10_,\n\u003e OntologyAnnotation AS a11_, OntologyTerm AS a12_, GeneGoAnnotation\n\u003e AS indirect0, EvidenceGOAnnotation AS indirect1 WHERE a1_.id =\n\u003e a2_.geneId AND a1_.organismId = a3_.id AND a2_.geneId = a4_.id\n\u003e AND a2_.homologueId = a5_.id AND a5_.organismId = a6_.id AND\n\u003e a1_.id = indirect0.Gene AND indirect0.GoAnnotation = a7_.id AND\n\u003e a7_.id = indirect1.GOAnnotation AND indirect1.Evidence = a8_.id AND\n\u003e a7_.ontologyTermId = a9_.id AND a8_.codeId = a10_.id AND a9_.id =\n\u003e a11_.ontologyTermId AND a11_.ontologyTermId = a12_.id ORDER BY\n\u003e a1_.primaryIdentifier, a1_.secondaryIdentifier, a2_.type,\n\u003e a3_.name, a4_.primaryIdentifier, a5_.primaryIdentifier,\n\u003e a6_.shortName, a12_.identifier, a10_.code, a1_.id, a2_.id,\n\u003e a3_.id, a4_.id, a5_.id, a6_.id, a12_.id, a10_.id\n\u003e LOG:  unexpected EOF on client connection\n\u003e LOG:  unexpected EOF on client connection\n\u003e LOG:  unexpected EOF on client connection\n\u003e LOG:  unexpected EOF on client connection\n\u003e LOG:  received fast shutdown request\n\u003e LOG:  aborting any active transactions\n\u003e LOG:  autovacuum launcher shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e FATAL:  the database system is shutting down\n\u003e LOG:  received immediate shutdown request\n\u003e WARNING:  terminating connection because of crash of another server\n\u003e process\n\u003e DETAIL:  The postmaster has commanded this server process to roll back\n\u003e the current transaction and exit, because another server process\n\u003e exited abnormally and possibly corrupted shared memory.\n\u003e HINT:  In a moment you should be able to reconnect to the database and\n\u003e repeat your command.\n\u003e LOG:  received fast shutdown request\n\u003e LOG:  database system was interrupted; last known up at 2015-10-22\n\u003e 15:47:43 CDT\n\u003e LOG:  received immediate shutdown request\n\u003e LOG:  database system was interrupted; last known up at 2015-10-22\n\u003e 15:47:43 CDT\n\u003e LOG:  database system was not properly shut down; automatic recovery\n\u003e in progress\n\u003e LOG:  record with zero length at BBD/1CC2F0C0\n\u003e ...\n```\n\nYou can see all the weird activity that was done here\n\n- first the attempt to \"canceling statement due to user request\" did not work\n- then the database stop using -m fast\n- then the database stop using -m immediate\n- the restart (with the HINT, should be ready soon)\n- the panic mode where i tried to shut it again anyways\n\nDuring the recovery period, I was still very concerned about the\ndatabase was doing, so I used \"strace\" to look at the main postmaster\nprocess.\n\nI was pleasantly surprised to see that the postmaster process was just\ncleaning up files in /db/postgres/data/base/pgsql_tmp/, I could see the\nfile system \"unlink\" command with successful status codes.\n\nThere were about 150 large files in /db/postgres/data/base/pgsql_tmp/,\nand I waited about an hour for them to be deleted, and after that, the\npostgresql log file said it was ready, and indeed, it was perfect :)\n\n```\n\u003e LOG:  redo is not required\n\u003e LOG:  database system is ready to accept connections\n\u003e LOG:  autovacuum launcher started\n```\n\nWhat a relief!\n\nI hope this might help any wayward stragglers to see how the postgresql\nrestart process works. Sometimes things don't shut down cleanly, but I\nthink it is still good to know some alternative steps to kill -9\n\n::: {#footer}\n[ October 22nd, 2015 7:29pm ]{#timestamp} [postgresql]{.tag} [dba]{.tag}\n[databases]{.tag} [sql]{.tag} [troubleshooting]{.tag}\n:::\n"},{"title":"Tomcat memory debugging","date":"2015-10-15","slug":"2015-10-15","content":"\nIn my previous posts, I speculated about the issues that were causing\nCPU usage spiking with\ntomcat: \u003chttp://searchvoidstar.tumblr.com/post/129241954103/fixing-spiky-cpu-issues-with-tomcat\u003e\n\nUnfortunately, I was completely wrong in my speculations, but we\nincreased tomcat memory limits so that the entire Lucene search index\ncould fit in memory, which was able to fix the spiky CPU problems.\n\nLuckily, fixing the memory issues had very good implications for our\nwebapp:\n\nI have a cron job uses a simple curl command to grab different pages on\nthe website, and then it logs the time taken to a output file. I charted\nthese output times, before and after we increased the memory limits of\ntomcat, and it turned out that the response time of the webapp was\ndramatically improved by this change.\n\n[](/media/131229569383_0.png)\n\n![](/media/131229569383_0.png)\n\nFigure 1. The webapp response time was extremely variable before the\nredeploy on Oct 2nd where we increased tomcat's memory allocation, which\nthereafter dramatically improved the response time.\n\nClearly, the webapp response time was being severely compromised by the\nmemory issues.\n\nIn response to all of these issues, I also added GC logging to the\ntomcat configuration so that I can see if the GC is correlated with\nthese webapp response time. Figure 2 shows how high GC activity is\ncorrelated with longer webapp response times, but note that this figure\nwas made after the other memory allocation problems were fixed, so it is\nstill much better than the problems we had in the past.\n\n[](/media/131229569383_1.png)\n\n![](/media/131229569383_1.png)\n\nFigure 2. After increasing the memory, you can see webapp response time\nis much better, except if the GC activity becomes very high, and then\nthis increases the response time.\n\nEdit: Bonus screenshot, seemingly each friday we get a majoy activity\nburst that triggers GC activity!\n\n![](/media/131229569383_2.png)\n\nFigure 3. Crazy Java GC activity on a friday night, but the app seems to\nrecover from it\n\nConclusion\n\nIncreasing the memory allocation to java and tomcat allows the entire\nsystem to perform much better. If you can afford to get more memory to\nallocate to tomcat, then it's probably a good idea.\n\nAlso, tracking your webapp response times will help you see if your\nchanges are having a good effect. I made this a script for graphing log\noutputs here \u003chttps://github.com/cmdcolin/loggraph\u003e\n\nPS:\n\nIf your tomcat is running as the tomcat user, then it can be difficult\nto debug the memory problems simply with the \"get heap dump\" from\njvisualvm, because the permissions will be wrong. To fix this, try using\na privileged user to run the jmap command:\n\nrunuser -l tomcat -c \"/usr/java/latest/bin/jmap\n-dump:format=b,file=/db/tomcat/tomcat.dump 25543\"\n\n::: {#footer}\n[ October 15th, 2015 1:31pm ]{#timestamp}\n:::\n\nexport default ({ children }) =\u003e \u003cLayout\u003e{children}\u003c/Layout\u003e\n"},{"title":"Weekend project - graphing tumblr reblogs using cytoscape.js","date":"2015-08-30","slug":"2015-08-30","content":"\nIn the past, I made an app that used RStudio's Shiny platform to plot\nnetwork graphs with RGraphviz. This worked, and gave some nice results,\nbut when I found out about cytoscape.js, I really wanted to try that\nout.\n\nThe app is designed to plot tumblr reblogs, so it has a tree structure,\nbut simply plotting things as a tree is not very space efficient (as in,\nthe visualization takes up too much space). Therefore, using different\ntypes of layouts can really help.\n\nIn my first app with graphviz\n\u003chttps://colindiesh.shinyapps.io/tumblrgraph\u003e, there are several\nbuilt-in graph layouts including \"neato\" \"twopi\", \"circo\", and \"dot\"\n\nI made all of these available for users to try in the Shiny app. The\nnames of the layouts don't lend much to their behavior, but they are\nbuilt-in functions in Graphviz. There are both \"tree\" and\n\"force-directed\" style graph views. As I mentioned, the \"tree\" style\nview make a lot of sense for the tumblr reblogs, but the force directed\ngraphs are also a lot more compact, so offering both styles is useful.\n\n![](../../media/128000908903_0.png)\n\nFigure 1. My default example graph from graphviz using the twopi layout.\n\nI wanted to replicate all the features that I had in the Graphviz app in\nCytoscape.js. Here is the breakdown of the basic components that needed\nreplicating:\n\n1. Build the \"graph\" representation of reblogs in memory\n\n2. Add user forms and configurability\n\n3. Add color for distance from root using a breadth first search\n\n4. Draw the graph\n\nAs I went along, I was happy to learn that the concepts mapped very\neasily to javascript and cytoscape.js. The implementations are a little\ndifferent, but it worked out very nicely.\n\n![](../../media/128000908903_1.png)\n\nFigure 2. Same data plotted in Cytoscape.js with the springy layout.\n\nIn the new app, we enabled several different layouts similar to the\nGraphviz app too. In cytoscape.js, the layouts that are offered\ninclude \"arbor\", \"springy\", \"cola\", \"cose\", and \"dagre\". I like \"cola\"\nbecause it really looks like bubbles moving around in a soda. Others are\nworth experimenting with too.\n\n![](../../media/128000908903_2.png)\n\nFigure 3. A Cytoscape.js springy layout for a larger tumblr reblog graph\n\nThe new cytoscape.js app also has a nice animation feature. The old\ngraphviz app offered animation too (using Yihui's animation library for\nR) but the new version can automatically encode HTML5 video on the\nclient side from individual picture frames in the browser using\n[\"Whammy\"](https://github.com/antimatter15/whammy)! This quite\nimpressive!\n\nSo to animate the graph, what is done is\n\n1. Add nodes/edges and layout the graph (the simulation time is\n   configurable, because allowing the user to interact with the graph while\n   the simulation is running is useful)\n\n2. Once layout is complete, the user can save the graph as an\n   animation, which first hides all nodes by adding visibility: hidden to\n   the CSS.\n\n3. Then the nodes are re-shown one-by-one, preserving the layout, and a\n   frame is saved by the renderer at each step (takes a snapshot of the\n   canvas).\n\nThis strategy for the animation is actually better than the original\ngraphviz version that I had because the layout is only done once, which\nis time saving and it is also more consistent (the layout changes a lot\nif you re run it on different sets of nodes).\n\nCheck out the app here \u003chttp://cmdcolin.github.io/tumblrgraph2/\u003e\n\nFuture goals:\n\n- Test out super large graphs (I have tested up to about 500 reblogs\n  but after this, around 1000 reblogs, it slows down a lot and produces\n  bad layouts. Needs fixing)\n- Test out ability to place importance on certain nodes by increasing\n  node size based on it's degree\n\nCheck out an example of the HTML5 video here\n\n::: {#footer}\n[ August 30th, 2015 11:49pm ]{#timestamp} [cytoscapejs]{.tag}\n[cytoscape]{.tag} [javascript]{.tag}\n:::\n\nexport default ({ children }) =\u003e \u003cLayout\u003e{children}\u003c/Layout\u003e\n"},{"title":"Creating high-resolution screenshots (of jbrowse) with phantomJS","date":"2015-03-02","slug":"2015-03-02","content":"\nGenerating screenshots that are of high quality can be a great benefit\nfor things like science publications. PhantomJS is great for automating\nthis in a reproducible way. While many HTML pages can be rendered in\nhigh resolution without modification, HTML5 canvas apps need special\nconsiderations (see this [previous post on the\ntopic](http://searchvoidstar.tumblr.com/post/86542847038/high-dpi-rendering-on-html5-canvas-some-problems)).\n\nOne of the key things that we noticed when we developed the high\nresolution canvas rendering (see above link) is that the\n\"devicePixelRatio\" can increase based on the browser's zoom level, and\nit can also take fractional values. This was a difficult problem, to\nmake rendering 100% consistent under all devicePixelRatio values, so we\ncreated a config parameter called highResolutionMode to accept arbitrary\nresolutions.\n\nLater, we learned about PhantomJS and how it can be used for creating\nscreenshots, it was clear that our design for the settings arbitrary\nscaling factors for the HTML5 canvas was very helpful, as we can set\nhighResolutionMode=4 along with the phantomJS variable\npage.zoomFactor=4, which matches the resolutions and creates high-res\ncanvas screenshots.\n\nOne of the reasons that this is important is that it doesn't look like\nPhantomJS allows \"devicePixelRatio\" to be emulated, so the\npage.zoomFactor doesn't necessarily set the devicePixelRatio to a higher\nnumber, so being able to set the the arbitrary high resolution canvas\nscalings ourselves is a good solution. Reference: issue open Jan 2013\n\u003chttps://github.com/ariya/phantomjs/issues/10964\u003e and we are now in Aug\n2015\n\nHere are some examples of the rendering process.\n\n## Examples\n\n1.  Rendering screenshots to PNG\n\n    phantomjs rasterize.js\n    \"\u003chttp://localhost/jbrowse/?data=sample_data/json/volvox\u0026tracklist=0\u003e\"\n    output.png \"3800px\\*1600px\" 2\n\n    [![](http://i.imgur.com/ABLo6WJ.png)](http://i.imgur.com/ABLo6WJ.png)\n\n    Figure 1. A basic image output from phantomJS. It uses a\n    zoomFactor=2 on the command line to match highResolutionMode=2 in\n    the config file. \\`\n\n2.  Rendering screenshots to PDF. In JBrowse, this requires PhantomJS\n    2.0. Also see footnote.\n\n    phantomjs rasterize.js\n    \"http://localhost/jbrowse/?data=sample_data/json/volvox\u0026tracklist=0\"\n    output.pdf \"16in\\*8in\"\n\n    [Dropbox PDF\n    906kb](https://www.dropbox.com/s/7pceo4o406dys8s/output.pdf?dl=0)\n\n    Figure 2. Outputted PDF from phantomJS. This still requires setting\n    the configuration such as highResolutionMode=2 too\n\n    ## Conclusion\n\n    In the future, we want to consider adding highResolutionMode to be\n    specified via the URL so that it doesn't need to be changed\n    manually, although, setting highResolutionMode=2 by default is not a\n    bad strategy.\n\n    **Footnote**\n\n    I used the following patch for rasterize.js to help \"fill out\" the\n    page space in PDF renderings (otherwise, it is a square page, not\n    super pretty for a widescreen app). I guess rasterize.js is really\n    just a template and not meant to be super multi-purposed, so this\n    custom modification helps for our case.\n\n```{=html}\n\u003c!-- --\u003e\n```\n\n        diff --git a/examples/rasterize.js b/examples/rasterize.js\n        index b0e0f67..3b0b6e4 100644\n        --- a/examples/rasterize.js\n        +++ b/examples/rasterize.js\n        _@@ -14,6 +14,7 @@ if (system.args.length \u003c 3 || system.args.length \u003e 5) {\n            page.viewportSize = { width: 600, height: 600 };\n            if (system.args.length \u003e 3 \u0026\u0026 system.args[2].substr(-4) === \".pdf\") {\n                size = system.args[3].split('_');\n\n        +       page.viewportSize.width *= parseInt(size[0])/parseInt(size[1]);\n                page.paperSize = size.length === 2 ? { width: size[0], height: size[1], margin: '0px' }\n\n**Reference**\n\ngmod.org/wiki/JBrowse_Configuration_Guide\\#Rendering_high_resolution_screenshots_using_PhantomJS\n\n**Comparison**\n\n![image](/media/112494997473_0.png)\n\nBig improvement on font rendering\n\n::: {#footer}\n[ March 2nd, 2015 1:52am ]{#timestamp} [javascript]{.tag}\n[phantomjs]{.tag} [html5]{.tag} [canvas]{.tag}\n:::\n\nexport default ({ children }) =\u003e \u003cLayout\u003e{children}\u003c/Layout\u003e\n"},{"title":"Post graduation survey","date":"2015-02-01","slug":"2015-02-01","content":"\nI recently received some post-graduation survey results from my class of\n2013 about salaries, job satisfaction, and other things. I thought I'd\ntry to visualize the data using R and ggplot2 as an exercise.\n\n[](http://i.imgur.com/5rVnQHC.png)\n\n![](/media/109823235838_0.png)\n\nFigure 1. The fancy ggplot2 graph of salaries with standard deviation\nbars comparing salaries of BS/MS grads (red) with BS grads (blue).\n\nAs a CS grad, I suppose I'm happy to see that we have the a highest\naverage salary right out of the gate. CS also has a high standard\ndeviation which I thought was interesting. Perhaps CS majors work in a\nmyriad of fields that demand computational skills where other\nengineering majors may be more focused on certain types of fields,\ngiving less deviation.\n\nIn the process of making this graph, I was looking for how to do the\nside-by-side bar charts in ggplot and ended up supplying a \"correction\"\nto a answer on crossvalidated, a stackexchange site. The correction\nentailed how the syntax for using reshape2 vs reshape has changed\nslightly, so hopefully that helps other people searching for the same\nissue.\n\nHere is the code for processing\n\n```R\n library(xlsx)\n library(ggplot2)\n library(reshape2)\n\n salaries=read.xlsx(\"workbook.xlsx\",1)\n df=melt(salaries,measure.vars = c(\"BS.MS.annual.salary\",\n \"BS.annual.salary\"))\n #awkward step to merge standard deviations\n df[df$variable==\"BS.MS.annual.salary\",\"stdev\"]=df[df$variable==\"BS.MS.annual.salary\",\"stdev.1\"]\n ggplot(df, aes(NA., value, fill=variable)) +\n      geom_bar(position=\"dodge\",stat=\"identity\") +\n      geom_errorbar(aes(ymin=value-stdev, ymax=value+stdev),\n position=position_dodge(width=0.9)) +\n      ggtitle(\"Salary for 2013 class of Engineering (2014 survey)\") +\n      xlab(\"Major\") +\n      ylab(\"Salary w/ stddev\")\n```\n\nTable pictured\n\n![](/media/109823235838_1.png)\n\n::: {#footer}\n[ February 1st, 2015 7:05pm ]{#timestamp} [rstats]{.tag} [ggplot2]{.tag}\n[college]{.tag} [salary]{.tag}\n:::\n\nexport default ({ children }) =\u003e \u003cLayout\u003e{children}\u003c/Layout\u003e\n"},{"title":"High DPI rendering on HTML5 canvas - some problems and solutions","date":"2014-05-22","slug":"2014-05-22","content":"\nRecently our code has been moving towards the use of HTML5 canvas, as it has\nmany benefits. I felt that if we were going to keep this going towards canvas,\nthe rendering needed to match the quality of regular HTML based tracks.\nUnfortunately, the HTML5 canvas by default looks very \"fuzzy\" on a high\nresolution display (Figure 1).\n\n![](/media/86542847038_0.jpg)\n\n_Figure 1._ An example of really bad font rendering before and after enabling\nhigh resolution on the HTML5 canvas.\n\n**Background **\n\nMajor credit goes to the tutorial at\n\u003chttp://www.html5rocks.com/en/tutorials/canvas/hidpi/\u003e for pioneering this!\n The html5rocks tutorial, written in 2010 it still remains relevant. The major\nthing it introduces is these browser variables called devicePixelRatio and\nbackingStoreRatio that can be used to adjust your canvas drawing. In my\ninterpretation, these two variables have the following purpose:\n\n_devicePixelRatio_\n\nOn high DPI displays, screen pixels are actually abstracted away from the\nphysical pixels, so, when you create some HTML element with width 100, height\n100, that element actually takes up a larger number of pixels than 100x100. The\nactual ratio of the pixels that it takes up is 100*devicePixelRatio x\n100*devicePixelRatio. On a high DPI platform like Retina, the devicePixelRatio\nis normally 2 at 100% zoom.\n\n_backingStoreRatio_\n\nThe backing store ratio doesn't seem to change as much from platform to\nplatform, but my interpretation of this value is that it essentially gives the\nsize of the memory buffer for the canvas. On my platform, the backingStoreRatio\nis \"1\". I think this value had more historical use, but it may not really be\nused anymore (update aug 7th, 2015 deprecated?\n\u003chttp://stackoverflow.com/questions/24332639/why-context2d-backingstorepixelratio-deprecated\u003e)\n\nSo, what are the consequences of the backing store ratio and the device pixel\nratio? If the backing store ratio equals the device pixel ratio, then no\nscaling takes place, but what we often see is that they are not equal, so the\nimage is up-scaled from the backing store to the screen, and then it is\nstretched and blurred.\n\n**So, how do you enable the high DPI mode?**\n\nThe solution to properly scale your HTML5 canvas content involves a couple of\nsteps that are described in the tutorial here\n\u003chttp://www.html5rocks.com/en/tutorials/canvas/hidpi/\u003e, but here is the\nessence:\n\n1. Use the canvas.scale method, which tells the canvas's drawing area to become\n   bigger, but keeps drawing operations consistent.\n\n2. The scaling factor for the canvas.scale method is\n   devicePixelRatio/backingStoreRatio. This will be 2 for instance on a Retina\n   screen at a typical 100% zoom level. The zoom level is relevant which will be\n   discussed later in this post...\n\n3. Multiply the width and height attributes of the canvas by\n   devicePixelRatio/backingStoreRatio, so that the \"canvas object\" is as big as\n   the scaled size.\n\n4. Here's the tricky part: set the CSS width and height attributes to be the\n   UNSCALED size that you want.\n\nNote: you can also set CSS width:100% or something and then the canvas will be\nsized appropriately. Normally though, what you will have is something like\n`\u003ccanvas width=640 height=480 style=\"width:320px;height:240px\"\u003e` so you can see\nthat the canvas size is larger than what the CSS actually resizes it to be.\n\n**Issues: Browser zoom and fractional devicePixelRatios **\n\nWhen I first started this project, the benefit of this high resolution\nrendering seemed limited to the fancy people who had Retina or other High DPI\nscreens. However, what I didn't even realize is that the devicePixelRatio value\nchanges depending on browser zoom settings, so even people with a regular\nscreen can have improved rendering of the HTML5 canvas. (Update: we even saw\nthat if you have customized canvas renderings, then you an generate good\nscreenshots of the canvas with PhantomJS too. See [my other more recent\narticle](http://searchvoidstar.tumblr.com/post/112494997473/creating-high-resolution-screenshots-of-jbrowse))\n\nThe issue with these zoom settings though is that when you change the zoom\nlevel, especially on chrome and firefox browsers, the devicePixelRatio can end\nup being a fractional value e.g. 2.223277 which can result in sub-pixel\nrendering problems.\n\nRemember that when we scaled the canvas, it also scales the drawing functions\nto be consistent, so that essentially if you draw a 1 pixel width line on a\nscaled canvas, it might draw a 2.223277 pixel width line. Hence, we can get\nfuzzy rendering issues.\n\nThis issue is very noticeable if you draw many 1px wide lines right next to\neach other. In this case, there will be noticeable gaps between the lines due\nto the imperfect rendering (see green box below).\n\n[](http://i.imgur.com/THsfjX4.png)\n\n[](http://i.imgur.com/THsfjX4.png)\n\n[](http://i.imgur.com/THsfjX4.png)\n\n![](/media/86542847038_1.png)\n\n_Figure 2._ Examples of 1px wide lines rendered next to each other when there\nis fractional devicePixelRatio.\n\nBottom Green box: 1px wide lines drawn 1px apart. (note: bad rendering! tiny\ngaps)  Middle Blue box: 1px wide line rendered every 2 px (intentional gaps for\ndemonstration).  Top Red box: 1.3px wide lines (a fudge factor is used to make\neliminate the tiny gaps).\n\n**My solution: The Red Box -- add a fudge factor **\n\nAs you can see in the above figure, my solution to the sub-pixel rendering is\nto add a \"fudge factor\" to the line width to make it render lines that are\n1.3px wide instead of 1px wide when the devicePixelRatio is not a whole number,\nwhich effectively eliminates any gaps due to the sub-pixel rendering problem.\n\nI heuristically determined the value 1.3px to be sufficient, as testing values\nlike 1.1px, 1.2px and even 1.25px were too small. I'd love to see a proof of\ndetermining this value empirically, or even better, something that isn't this\nbig of a hack, but for now that's what I have.\n\nYou can see the effect of the fudge factor (red box) vs the bad rendering\n(green box) in Figure 2. You can also try this out yourself here\n\u003chttp://jsfiddle.net/4xe4d/\u003e, just zoom your browser and then refresh (zooming\nand not refreshing doesn't modify device pixel ratio) to test out different\nvalues of devicePixelRatio.\n\n**Conclusion**\n\nIn conclusion...we now have high resolution rendering on canvas! The solution\nfor drawing lots of lines right next to each other is sort of suboptimal, so\nthe question continues...what shall be done in this case?\n\nMaybe someone could implement some sort of library that replaces the\ncanvas.scale method to do better layout and obtain more pixel perfect\nrendering. Alternatively, you could force the scaling factor to always round to\na whole number. This is actually not a bad solution, because the canvas is\nalready being resized, and then you can control your rendering better.\n\nThanks for reading\n\n::: {#footer} [ May 22nd, 2014 7:03pm ]{#timestamp} [html5]{.tag}\n[canvas]{.tag} [javascript]{.tag} :::\n"}]},"__N_SSG":true},"page":"/archive","query":{},"buildId":"wRE5shyr7ZdzrAZY-3qqN","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>