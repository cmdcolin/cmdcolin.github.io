<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Misc scribblings</title>
        <link>https://cmdcolin.github.io</link>
        <description>Blog by Colin Diesh</description>
        <lastBuildDate>Sun, 26 Dec 2021 21:55:18 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <copyright>n/a</copyright>
        <item>
            <title><![CDATA[A spooky error when you have a string bigger than 512MB in Chrome]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sat, 30 Oct 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Now gather round for a spooky story

Late one night... in the haunted office space castle (hindenbugs cackling in
the background amongst the dusty technical books) the midnight candles were
burning bright and we entered data for a user file

A simple 52MB gzipped datafile that we want to process in the browser. We unzip
it, decode it, and ...an error

ERROR: data not found

![](/media/pumpkin-dark.jpg)

But... our code is so simple (we of course abide by the religion of writing "simple code" you know)...what could be happening?

The code looks like this

```js
const buf = unzip(file)
const str = new TextDecoder().decode(buf)
```

We trace it back and run a console.log(str)

It looks empty. We try running console.log(str.length) ... it prints out 0

But if we console.log(buffer.length) we get 546,483,710 bytes...

What could be happening?

We see in the TextDecoder documentation that it has a note called "fatal". We try

```js
const buf = unzip(file)
const str = new TextDecoder('utf8', { fatal: true }).decode(buf)
```

This doesn't change the results though

Then it dawns on us while the lightning hits and the thunderclap booms and the
wind blows through the rattly windows

We have hit...the maximum string length in Chrome

BWAHAHAHAHA

The maximum string length!!! Nooooooo

It is 512MB on the dot... 536,870,888 bytes. We test this to be sure

```
const len = 536_870_888;
const buf = new Uint8Array(len);
for (let i = 0; i < len; i++) {
  buf[i] = "a".charCodeAt(0);
}
const str = new TextDecoder().decode(buf);
console.log(str.length);

```

This is correct, outputs 536,870,888

With anything, even one byte more, it fails and outputs 0

happy halloween!!

pumpkin photo source: http://mountainbikerak.blogspot.com/2010/11/google-chrome-pumpkin.html

chrome 95 tested

nodejs 15 - at 512MB+1 bytes it prints an error message `Error: Cannot create a string longer than 0x1fffffe8 characters` for significantly greater than 512MB
e.g. 600MB it actually prints a different error `TypeError [ERR_ENCODING_INVALID_ENCODED_DATA]: The encoded data was not valid for encoding utf-8`)

firefox 93 - goes up to ~1GB but then gives Exception { name: "NS_ERROR_OUT_OF_MEMORY", message: "", result: 2147942414

midori 6 (safari-alike/webkit) - goes up to ~2GB fine! will have to test more
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Jest parallelization, globals, mocks, and squawkless tests]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Tue, 05 Oct 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
I found that there is a little bit of confusion and misunderstanding around how
things like parallelization work in jest, which sometimes leads to additional
hacking around problems that may not exist or speculating incorrectly about
test failure. This is also of course a point of concern when you have code that
for some reason or another uses global variables. Here are a short summary of
things that may cause confusion.

## Tests in a single file are NOT run in parallel

Simple example, the global variable r is included in the test condition, but it
is accurately run in all cases because the tests are not run in parallel.

```js
let r = 0

function timeout(ms) {
  return new Promise(resolve => setTimeout(resolve, ms))
}

describe('tests', () => {
  it('t1', async () => {
    await timeout(1000)
    expect(r).toBe(0)
    r++
  })
  it('t2', async () => {
    await timeout(1000)
    expect(r).toBe(1)
    r++
  })
  it('t3', async () => {
    await timeout(1000)
    expect(r).toBe(2)
    r++
  })
})
```

This test will take 3 seconds, and will accurately count the global variable.
If it was in parallel, it may only take 1 second, and would inaccurately count
the global variable due to race conditions

## Tests in different files ARE run in parallel

Let's take another example where we use a global variable, and then two
different tests use the global variable.

file_using_some_globals.js

```js
let myGlobal = 0

export function doStuff() {
  myGlobal++
  return myGlobal
}

export function resetMyGlobal() {
  myGlobal = 0
}

export function timeout(ms) {
  return new Promise(resolve => setTimeout(resolve, ms))
}
```

test_global_vars1.test.js

```js
import { doStuff, timeout } from './dostuff'
test('file1', async () => {
  doStuff()
  await timeout(1000)
  expect(doStuff()).toEqual(2)
})
```

test_global_vars2.test.js

```js
import { doStuff, timeout } from './dostuff'

test('file1', async () => {
  await timeout(1000)
  expect(doStuff()).toEqual(1)
})
```

This test completes in less than 2 seconds, and these tests are run in
parallel. They use different instances of the global state, and therefore have
no worries with colliding their state.

## Does a mock from one test affect another test?

While seeking the fabled "squawk-less" test, it is often useful to mock console
so that tests that produce an expected error don't actually print an error
message. However, if not done carefully, you will remove errors across tests

So, could a mock from one test affect another test? If it's in the same file,
yes!

mock_console.test.js

```
test("test1", () => {
  console.error = jest.fn();
  console.error("wow");
  expect(console.error).toHaveBeenCalled();
});

test("test2", () => {
  // this console.error will not appear because test1 mocked away console.error
  // without restoring it
  console.error("Help I can't see!");
});


```

To properly mock these, you should restore the console mock at the end of your
function

```
test("test1", () => {
  const orig = console.error;
  console.error = jest.fn();
  console.error("I should not see this!");
  expect(console.error).toHaveBeenCalled();
  console.error = orig;
});

test("test2", () => {
  const consoleMock = jest.spyOn(console, "error").mockImplementation();
  console.error("I should not see this!");
  consoleMock.mockRestore();
});

test("test3", () => {
  console.error("I should see this error!");
});
```

## Add-on: Achieve squawkless tests!

Your test output should just be a big list of PASS statements, not interleaved
with console.error outputs from when you are testing error conditions of your
code

"Squawkless tests" is a term I made up, but it means that if you have code
under test that prints some errors to the console, then mock the console.error
function, as in the previous section. Don't stand for having a bunch of verbose
errors in your CI logs! However, I also suggest only mocking out console.error
for tests that are **expected** to have errors, lest you paper over unexpected
errors.

![](/media/squawkless_tests.png)

Figure: a nice clean test suite without a bunch of crazy console.error outputs

## Conclusion

Getting better at testing requires exercise, and understanding the basics of
your tools can help! Hopefully this helps you achieve a better understanding
and write cleaner jest tests.
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Decrease your idle CPU usage when developing typescript apps with this one weird environment variable]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sun, 05 Sep 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
TL;DR:

add this to your bashrc

```
export TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling
```

<hr/>

By default, the typescript watcher configuration e.g. tsc --watch or whatever
is run internally to a create-react-app typescript app (I see it in the process
manager as fork-ts-checker-webpack-plugin cpu usage) can have high idling
(doing nothing...) CPU usage

This is because the default configuration polls for file changes (constantly
asks the computer if there are changes every 250ms or so). There is an
alternative configuration for this to change it to a file watcher so it
receives file system notifications on file change. There is discussion here on
this.

The main summary is that a env variable set to
TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling allows this

https://github.com/microsoft/TypeScript/issues/31048

The issue thread shows that it can go from roughly ~7% idle CPU usage to 0.2%.
This corresponds with what I see too after applying this! Detailed docs for
typescript discuss some of the reasoning behing not making this the default

https://github.com/microsoft/TypeScript-Handbook/blob/master/pages/Configuring%20Watch.md#background

It claims that some OS specific behaviors of file watching could be harmful to
making it the default. For example, that (maybe?) on linux, it may use a large
number of file watchers which can exceed notify handles (this is a setting I
commonly have to increase in linux, guide here
https://dev.to/rubiin/ubuntu-increase-inotify-watcher-file-watch-limit-kf4)

PS: if you have a package.json of a `create-react-app --template typescript` or
something like this then you can edit the package.json to apply this
automatically

```
-"start": "react-scripts start"
+"start": "cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling react-scripts start"
```

Phew. I can already feel my laptop running cooler...or at least I can sleep
more soundly knowing that my readers adopt this and save some CPU cycles for
planet earth...and hopefully don't run into any of the caveats

Edit: It may be worth it to note, the 'UseFsEvents' part of this uses the
node.js fs.watch API and the polling based API is based on fs.watchFile

Fun table of how the watchers are implemented on different OSs
[[1](https://github.com/microsoft/TypeScript/issues/31048#issuecomment-495483957)]

```
On Linux systems, this uses inotify(7).
On BSD systems, this uses kqueue(2).
On macOS, this uses kqueue(2) for files and FSEvents for directories.
On SunOS systems (including Solaris and SmartOS), this uses event ports.
On Windows systems, this feature depends on ReadDirectoryChangesW.
On Aix systems, this feature depends on AHAFS, which must be enabled.
```

And in general, these should all respond more or less the same, but there are
small corner cases that are discussed
https://nodejs.org/docs/latest/api/fs.html#fs_availability

Disclaimer: it may be worth reading the reasons that typescript does not have
this enabled by default before pushing this into your dev environment and all
your teammates, but as far as I could tell, it seems ok!
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[An amazing error message if you put more than 2^24 items in a JS Map object]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sun, 15 Aug 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
One of the fun things about working with big data is that you can often hit
weird limits with a system.

I was personally trying to load every 'common' single nucleotide polymorphism
for the human genome into memory (dbSNP), of which there are over 37 million
entries (there are many more uncommon ones) for the purposes of making a custom
search index for them [1].

Turns out, you may run into some hard limits. Note that these are all V8-isms
and may not apply to all browsers or engines (I was using node.js for this)

```
const myObject = new Map();
for (let i = 0; i <= 50_000_000; i++) {
  myObject.set(i,i);
  if(i%100000==0) { console.log(i) }
}
```

This will crash after adding approx 16.7M elements and say

```
0
100000
200000
...
16400000
16500000
16600000
16700000

Uncaught RangeError: Value undefined out of range for undefined options
property undefined
```

That is a very weird error message. It says “undefined” three times! Much
better than your usual “TypeError: Can’t find property ‘lol’ of undefined”. See
https://bugs.chromium.org/p/v8/issues/detail?id=11852 for a bug filed to help
improve the error message perhaps.

Now, also interestingly enough, if you use an Object instead of a Map

```js
const myObject = {};
for (let i = 0; i <= 50_000_000; i++) {
  myObject['myobj_’+i]=i;
  if(i%100000==0) { console.log(i) }
}
```

Then it will print….

```
0
100000
200000
...
8000000
8100000
8200000
8300000
```

And it will actually just hang there…frozen…no error message though! And it is
failing at ~8.3M elements. Weird right? This is roughly half the amount of
elements as the 16.7M case

Turns out there is a precise hard limit for the Map case

For the Map: 2^24=16,777,216

For the Object it is around 2^23=8,388,608 HOWEVER, I can actually add more
than this, e.g. I can add 8,388,609 or 8,388,610 or even more, but the
operations start taking forever to run, e.g. 8,388,999 was taking many minutes

Very weird stuff! If you expected me to dig into this and explain it in deep
technical detail, well, you’d be wrong. I am lazy. However, this helpful post
on stackoverflow by a V8 js engine developer clarifies the Map case!!
https://stackoverflow.com/questions/54452896/maximum-number-of-entries-in-node-js-map

```
V8 developer here. I can confirm that 2^24 is the maximum number of entries in
a Map. That’s not a bug, it’s just the implementation-defined limit.

The limit is determined by:

The FixedArray backing store of the Map has a maximum size of 1GB (independent
of the overall heap size limit) On a 64-bit system that means 1GB / 8B = 2^30 /
2^3 = 2^27 ~= 134M maximum elements per FixedArray A Map needs 3 elements per
entry (key, value, next bucket link), and has a maximum load factor of 50% (to
avoid the slowdown caused by many bucket collisions), and its capacity must be
a power of 2. 2^27 / (3 * 2) rounded down to the next power of 2 is 2^24, which
is the limit you observe.  FWIW, there are limits to everything: besides the
maximum heap size, there’s a maximum String length, a maximum Array length, a
maximum ArrayBuffer length, a maximum BigInt size, a maximum stack size, etc.
Any one of those limits is potentially debatable, and sometimes it makes sense
to raise them, but the limits as such will remain. Off the top of my head I
don’t know what it would take to bump this particular limit by, say, a factor
of two – and I also don’t know whether a factor of two would be enough to
satisfy your expectations.

```

Great details there. It would also be good to know what the behavior is for the
Object, which has those 100% CPU stalls after ~8.3M, but not the same error
message...

Another fun note: if I modify the Object code to use only “integer IDs” the
code actually works fine, does not hit any errors, and is “blazingly fast” as
the kids call it

```js
const myObject = {}
for (let i = 0; i <= 50_000_000; i++) {
  myObject[i] = i
  if (i % 100000 == 0) {
    console.log(i)
  }
}
```

I presume that this code works because it detects that I’m using it like an
array and it decides to transform how it is working internally and not use a
hash-map-style data structure, so does not hit a limit. There is a slightly
higher limit though, e.g. 1 billion elements gives “Uncaught RangeError:
Invalid array length”

```js
const myObject = {}
for (let i = 0; i <= 1_000_000_000; i++) {
  myObject[i] = i
  if (i % 100000 == 0) {
    console.log(i)
  }
}
```

This has been another episode of ....the twilight zone (other episodes
catalogued here) https://github.com/cmdcolin/technical_oddities/

[1] The final product of this adventure was this, to create a search index for
a large number of elements https://github.com/GMOD/ixixx-js
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Do you understand your NPM dependencies?]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Tue, 27 Jul 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
You are writing a library...or your writing an app and you want to publish some
of the components of it as a library...

Here are some questions in the form of comments

- Did you realize that your yarn.lock will be ignored for anyone who installs
  your libraries?

- Did you realize this means that your perfectly running test suite with your
  yarn.lock could be a failing case for consumers of your app unless you don’t
  use semver strings like ^1.0.0 and just hardcode it to 1.0.0?

- Did you realize the default of ^1.0.0 automatically gets minor version bumps
  which are often fairly substantial changes, e.g. even breaking possibly?

- Did you know that larger libraries like @material-ui/core don’t like to bump
  their major version all the time for example so large changes are often made
  to the minor version?

- Did you know if you run `yarn upgrade`, it may update what is in your yarn.lock file but will not update what is in your package.json?

- Did you realize that this means that if you depend on the results of running `yarn upgrade` e.g. it gave you a bugfix, you will be shipping buggy code to consumers of your library?

Just something to be aware of! You can always ride the dragon and accept these
minor breakages from semver bumps, but it can introduce some issues for your
consumers

Random fun thing: Adding a yarn package can even downgrade some other packages.
For example if you have ^6.0.0 in your package.json, you yarn upgrade it up to
^6.1.0 but then later install another library that requires a hard 6.0.1, yarn
will decide to downgrade you to 6.0.1
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Making a HTTPS accessible S3 powered static site with CloudFront+route 53]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sat, 26 Dec 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
This is not a very authoritative post because I stumbled though this but
I think I got it working now on my website :)

## Setup your S3 bucket

First setup your S3 bucket, your bucket must be named yourdomain.com
e.g. named after your domain

Then if you have a create-react-app setup I add a script in package.json
that runs

```
 "predeploy": "npm run build",
 "deploy": "aws sync --delete build s3://yourdomain.com"
```

Then we can run "yarn deploy" and it will automatically upload our
create-react-app website to our S3 static site bucket.

Then make sure your bucket has public permissions enabled
<https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2>Then
make sure your bucket has "static site hosting" enabled too

## Setup route 53, and make your NS entries in domains.google.com

I bought a domain with domains.google.com

Google then emailed me to validate my ownership

Then I went to aws.amazon.com route 53 and I created a hosted zone

This generated 4 name server entries and I added those to the
domains.google.com site

![](/media/638618421776515072_0.png)

Screenshot shows copying the NS values from route 53 to the name servers
area of domains.google.com

## Setup your Amazon certificate for making SSL work on CloudFront

To properly setup However, this does not work so you need to go to
Amazon Certificates->Provision certificates

We request the certificate for

[www.yourdomain.com](http://www.yourdomain.com)
yourdomain.com

Then it generates some codes for a CNAME value for each of those two
entries, and has a button to autoimport those CNAME values to route53

Then it will say "Pending validation"...I waited like an hour and then
it changed to "Success".

![](/media/638618421776515072_1.png)

Screenshot shows the now successful Amazon Certificate. After you get
this, you can proceed to finishing your cloudfront

## Create a CloudFront distribution and add "Alternative CNAME" entries for your domain

Then we can update our CloudFront distribution and add these to
the "Alternative CNAME" input box

yourdomain.com
[www.yourdomain.com](http://www.yourdomain.com)

Note also that I first generated my certificate in us-east-2 but the
"Import certificate form" in cloudfront said I had to create it in
us-east-1

![](/media/638618421776515072_2.png)

## Add a default object index.html to the CloudFront setting

Make your CloudFront "default object" is index.html

You have to manually type this in :)

## Add the CloudFront distribution to your Route 53

Add a Route 53 "A" record that points to the CloudFront domain name e.g.
d897d897d87d98dd.cloudfront.net

## Summary of steps needed

The general hindsight 20/20 procedure is

1.  Upload your static content to an S3 bucket called yoursite.com (must
    be your domain name)
2.  Make your S3 bucket have the "static website" setting on in the
    properties menu and add a permissions policy that supports getObject
    e.g. <https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2>
3.  Create a CloudFront distribution for your website
4.  Make the CloudFront default object index.html
5.  Create your domain with domains.google.com or similar
6.  Point the google domain's name server to Route 53 NS list from AWS
7.  Add Route 53 A records that point to the CloudFront domain name e.g.
    d897d897d87d98dd.cloudfront.net
8.  Create Amazon issued certificate for yourdomain.com, which can
    auto-import a validation CNAME to your Route 53
9.  Make your CloudFront domain support your Alternative CNAME's e.g.
    yourdomain.com which requires importing (e.g. selecting from a list
    that they auto-populate) your Amazon-issued-certificate

## Troubleshooting and notes

Problem: Your website gives 403 CloudFlare error
Solution: You have to get the Alternateive CNAME configuration setup
(pre-step involves the certificate request and validation)

Problem: Your website gives an object not found error
Solution: Set the CloudFront "default object" to index.html

## Random comment

This is one of those processes (creating the cloudfront/route 53) that
probably could have done with the aws-sam CLI and it would have possibly
been easier, it is quite fiddly doing all these steps in the web
interface
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Making a serverless website for photo and video upload pt. 2]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sat, 26 Dec 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
This post follows
on <https://cmdcolin.github.io/2020-12-24.html>

It is possible I zoomed ahead too fast to make this a continuous
tutorial, but overall I just wanted to post an update

In pt. 1 I learned how to use the `aws-sam` CLI tool. This was a great
insight for me about automating deployments. I can now simply run `sam deploy` and it will create new dynamodb tables, lambda functions, etc.

After writing pt 1. I converted the existing vue-js app that was in the
aws tutorial and converted it to react. Then I extended the app to allow

- Posting comments on photos
- Uploading multiple files
- Uploading videos
  etc.

It will be hard to summarize all the changes since now the app has taken
off a little bit but it looks like this:

Repo structure

```
 ./frontend # created using npx create-react-app frontend --template
 typescript
 ./frontend/src/App.tsx # main frontend app code in react
 ./lambdas/
 ./lambdas/postFile # post a file to the lambda, this uploads a row to
 dynamodb and returns a pre-signed URL for uploading (note that if the
 client failed it's upload, that row in the lambda DB might be in a bad
 state...)
 ./lambdas/getFiles # get all files that were ever posted
 ./lambdas/postComment # post a comment on a picture with POST
 request
 ./lambdas/getComments?file=filename.jpg # get comments on a
 picture/video with GET request
```

Here is a detailed code for uploading the file. We upload one file at a
time, but the client code post to the lambda endpoint individually for
each file

This generates a pre-signed URL to allow the client-side JS (not the
lambda itself) to directly upload to S3, and also posts a row in the S3
to the filename that will. It is very similar code in
to <https://cmdcolin.github.io/2020-12-24.html>

./lambdas/postFile/app.js

```js
'use strict'

const AWS = require('aws-sdk')
const multipart = require('./multipart')
AWS.config.update({ region: process.env.AWS_REGION })
const s3 = new AWS.S3()

// Change this value to adjust the signed URL's expiration
const URL_EXPIRATION_SECONDS = 300

// Main Lambda entry point
exports.handler = async event => {
  return await getUploadURL(event)
}

const { AWS_REGION: region } = process.env

const dynamodb = new AWS.DynamoDB({ apiVersion: '2012-08-10', region })

async function uploadPic({
  timestamp,
  filename,
  message,
  user,
  date,
  contentType,
}) {
  const params = {
    Item: {
      timestamp: {
        N: `${timestamp}`,
      },
      filename: {
        S: filename,
      },
      message: {
        S: message,
      },
      user: {
        S: user,
      },
      date: {
        S: date,
      },
      contentType: {
        S: contentType,
      },
    },
    TableName: 'files',
  }
  return dynamodb.putItem(params).promise()
}

const getUploadURL = async function (event) {
  try {
    const data = multipart.parse(event)
    const { filename, contentType, user, message, date } = data
    const timestamp = +Date.now()
    const Key = `${timestamp}-${filename}` // Get signed URL from S3

    const s3Params = {
      Bucket: process.env.UploadBucket,
      Key,
      Expires: URL_EXPIRATION_SECONDS,
      ContentType: contentType, // This ACL makes the uploaded object publicly readable. You must also uncomment // the extra permission for the Lambda function in the SAM template.

      ACL: 'public-read',
    }

    const uploadURL = await s3.getSignedUrlPromise('putObject', s3Params)

    await uploadPic({
      timestamp,
      filename: Key,
      message,
      user,
      date,
      contentType,
    })

    return JSON.stringify({
      uploadURL,
      Key,
    })
  } catch (e) {
    const response = {
      statusCode: 500,
      body: JSON.stringify({ message: `${e}` }),
    }
    return response
  }
}
```

./lambdas/getFiles/app.js

```js
// eslint-disable-next-line import/no-unresolved
const AWS = require('aws-sdk')

const { AWS_REGION: region } = process.env

const docClient = new AWS.DynamoDB.DocumentClient()

const getItems = function () {
  const params = {
    TableName: 'files',
  }

  return docClient.scan(params).promise()
}

exports.handler = async event => {
  try {
    const result = await getItems()
    return {
      statusCode: 200,
      body: JSON.stringify(result),
    }
  } catch (e) {
    return {
      statusCode: 400,
      body: JSON.stringify({ message: `${e}` }),
    }
  }
}
```

./frontend/src/App.tsx (excerpt)

```tsx
async function myfetch(params: string, opts?: any) {
  const response = await fetch(params, opts)
  if (!response.ok) {
    throw new Error(`HTTP ${response.status}
 ${response.statusText}`)
  }
  return response.json()
}

function UploadDialog({
  open,
  onClose,
}: {
  open: boolean
  onClose: () => void
}) {
  const [images, setImages] = useState<FileList>()
  const [error, setError] = useState<Error>()
  const [loading, setLoading] = useState(false)
  const [total, setTotal] = useState(0)
  const [completed, setCompleted] = useState(0)
  const [user, setUser] = useState('')
  const [message, setMessage] = useState('')
  const classes = useStyles()

  const handleClose = () => {
    setError(undefined)
    setLoading(false)
    setImages(undefined)
    setCompleted(0)
    setTotal(0)
    setMessage('')
    onClose()
  }

  return (
    <Dialog onClose={handleClose} open={open}>
           <DialogTitle>upload a file (supports picture or video)</DialogTitle> 
         <DialogContent>
               <label htmlFor="user">name (optional) </label>
               <input
          type="text"
          value={user}
          onChange={event => setUser(event.target.value)}
          id="user"
        />
               <br />       <label htmlFor="user">message (optional) </label>
               
        <input
          type="text"
          value={message}
          onChange={event => setMessage(event.target.value)}
          id="message"
        />
               <br />
               
        <input
          multiple
          type="file"
          onChange={e => {
            let files = e.target.files
            if (files && files.length) {
              setImages(files)
            }
          }}
        />
               {error ? (
          <div className={classes.error}>{`${error}`}</div>
        ) : loading ? (
          `Uploading...${completed}/${total}`
        ) : completed ? (
          <h2>Uploaded </h2>
        ) : null}       
        <DialogActions>
                   
          <Button
            style={{ textTransform: 'none' }}
            onClick={async () => {
              try {
                if (images) {
                  setLoading(true)
                  setError(undefined)
                  setCompleted(0)
                  setTotal(images.length)
                  await Promise.all(
                    Array.from(images).map(async image => {
                      const data = new FormData()
                      data.append('message', message)
                      data.append('user', user)
                      data.append('date', new Date().toLocaleString())
                      data.append('filename', image.name)
                      data.append('contentType', image.type)
                      const res = await myfetch(API_ENDPOINT + '/postFile', {
                        method: 'POST',
                        body: data,
                      })

                      await myfetch(res.uploadURL, {
                        method: 'PUT',
                        body: image,
                      })

                      setCompleted(completed => completed + 1)
                    }),
                  )
                  setTimeout(() => {
                    handleClose()
                  }, 500)
                }
              } catch (e) {
                setError(e)
              }
            }}
            color="primary"
          >
                       upload          
          </Button>
                   <Button
            onClick={handleClose}
            color="primary"
            style={{ textTransform: 'none' }}
          >
                       cancel          
          </Button>       
        </DialogActions>
             
      </DialogContent>   
    </Dialog>
  )
}
```

template.yaml for AWS

```
 AWSTemplateFormatVersion: 2010-09-09
 Transform: AWS::Serverless-2016-10-31
 Description: S3 Uploader

 Resources:
  filesDynamoDBTable:
    Type: AWS::DynamoDB::Table
    Properties:
      AttributeDefinitions:
        - AttributeName: "timestamp"
          AttributeType: "N"
      KeySchema:
        - AttributeName: "timestamp"
          KeyType: "HASH"
      ProvisionedThroughput:
        ReadCapacityUnits: "5"
        WriteCapacityUnits: "5"
      TableName: "files"

  # HTTP API
  MyApi:
    Type: AWS::Serverless::HttpApi
    Properties:
      # CORS configuration - this is open for development only and
 should be restricted in prod.
      # See
 <https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-property-httpapi-httpapicorsconfiguration.html>
      CorsConfiguration:
        AllowMethods:
          - GET
          - POST
          - DELETE
          - OPTIONS
        AllowHeaders:
          - "*"
        AllowOrigins:
          - "*"

  UploadRequestFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: lambdas/postFile/
      Handler: app.handler
      Runtime: nodejs12.x
      Timeout: 3
      MemorySize: 128
      Environment:
        Variables:
          UploadBucket: !Ref S3UploadBucket
      Policies:
        - AmazonDynamoDBFullAccess
        - S3WritePolicy:
            BucketName: !Ref S3UploadBucket
        - Statement:
            - Effect: Allow
              Resource: !Sub "arn:aws:s3:::${S3UploadBucket}/"
              Action:
                - s3:putObjectAcl
      Events:
        UploadAssetAPI:
          Type: HttpApi
          Properties:
            Path: /postFile
            Method: post
            ApiId: !Ref MyApi


  FileReadFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: lambdas/getFiles/
      Handler: app.handler
      Runtime: nodejs12.x
      Timeout: 3
      MemorySize: 128
      Policies:
        - AmazonDynamoDBFullAccess
      Events:
        UploadAssetAPI:
          Type: HttpApi
          Properties:
            Path: /getFiles
            Method: get
            ApiId: !Ref MyApi

  ## S3 bucket
  S3UploadBucket:
    Type: AWS::S3::Bucket
    Properties:
      CorsConfiguration:
        CorsRules:
          - AllowedHeaders:
              - "*"
            AllowedMethods:
              - GET
              - PUT
              - HEAD
            AllowedOrigins:
              - "*"


 ## Take a note of the outputs for deploying the workflow templates
 in this sample application
 Outputs:
  APIendpoint:
    Description: "HTTP API endpoint URL"
    Value: !Sub
 "https://${MyApi}.execute-api.${AWS::Region}.amazonaws.com"
  S3UploadBucketName:
    Description: "S3 bucket for application uploads"
    Value: !Ref "S3UploadBucket"

```

To display all the pictures I use a switch from video or img tag based
on contentType.startsWith('video'). I also use the "figcaption" HTML tag
to have a little caption on the pics/videos

./frontend/src/App.tsx

```
 function Media({
  file,
  style,
  onClick,
  children,
 }: {
  file: File;
  onClick?: Function;
  style?: React.CSSProperties;
  children?: React.ReactNode;
 }) {
  const { filename, contentType } = file;
  const src = `${BUCKET}/${filename}`;
  return (
    <figure style={{ display: "inline-block" }}>
      <picture>
        {contentType.startsWith("video") ? (
          <video style={style} src={src} controls onClick={onClick as
 any} />
        ) : (
          <img style={style} src={src} onClick={onClick as any} />
        )}
      </picture>
      <figcaption>{children}</figcaption>
    </figure>
  );
 }
```

Now the really fun part: if you get an image of a picture frame
like <https://www.amazon.com/Paintings-Frames-Antique-Shatterproof-Osafs2-Gld-A3/dp/B06XNQ8W9T>

You can make it a border for any image or video using border-image CSS

```
     style = {
         border: "30px solid",
         borderImage: `url(borders/${border}) 30 round`
     }
```

![](/media/638602799897329664_0.png)

Summary

The template.yaml automatically deploys the lambdas for postFile/getFile
and the files table in dynamoDB

The React app uses postFile for each file in an `<input type="file"/>`,
the code uses React hooks and functional components but is hopefully not
too complex

I also added commenting on photos. The code is not shown here but you
can look in the source code for details

![](/media/638602799897329664_1.png)

Overall this has been a good experience learning to develop this app and
learning to automate the cloud deployment is really good for ensuring
reliability and fast iteration.

Also quick note on serverless CLI vs aws-sam. I had tried a serverless
CLI tutorial from another user but it didn't click with me, while the
aws-sam tutorial from
<https://searchvoidstar.tumblr.com/post/638408397901987840/making-a-serverless-website-for-photo-upload-pt-1> was
a great kick start for me. I am sure the serverless CLI is great too and
it ensures a bit less vendor lock in, but then is also a little bit
removed from the native aws config schemas. Probably fine though

Source code <https://github.com/cmdcolin/aws_photo_gallery/>
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Making a serverless website for photo upload pt. 1]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Thu, 24 Dec 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
I set out to make a serverless website for photo uploads. Our dearly
departed dixie dog needed a place to have photo uploads.

I didn't want to get charged dollars per month for a running ec2
instance, so I wanted something that was lightweight e.g. serverless,
and easy

I decided to follow this tutorial

<https://aws.amazon.com/blogs/compute/uploading-to-amazon-s3-directly-from-a-web-or-mobile-application/>

I really liked the command line deployment (aws-sam) because fiddling
around with the AWS web based control panel is ridiculously complicated

For example I also tried following this tutorial which uses the web
based UI (<https://www.youtube.com/watch?v=mw_-0iCVpUc>) and it just did
not work for me....I couldn't stay focused (blame ADHD or just my CLI
obsession?) and certain things like "Execution role" that they say to
modify are not there in the web UI anymore, so I just gave up (I did try
though!)

To install aws-sam I used homebrew

```
 brew tap aws/tap
 brew install aws-sam-cli
 brew install aws-sam-cli # I had to run the install command twice ref https://github.com/aws/aws-sam-cli/issues/2320#issuecomment-721414971

 git clone https://github.com/aws-samples/amazon-s3-presigned-urls-aws-sam
 cd amazon-s3-presigned-urls-aws-sam
 sam deploy --guided

 # proceeeds with a guided installation, I used all defaults except I
 made "UploadRequestFunction may not have authorization defined, Is
 this okay? [y/N]: y"
```

![](/media/638408397901987840_0.png)

They then in the tutorial describe trying to use postman to test

I test with `curl` instead

```
curl 'https://fjgbqj5436.execute-api.us-east-2.amazonaws.com/uploads' {"uploadURL":"https://sam-app-s3uploadbucket-1653634.s3.us-east-2.amazonaws.com/112162.jpg?Content-Type=image%2Fjpeg&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAU6CQBER6YBNCDDMJ%2F20201224%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20201224T174804Z&X-Amz-Expires=300&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDIaCXVzLWVhc3QtMiJGMEQCIH65IvgJsofUpIX46lTaG3Pi5WC85ti1lukM3iICh%2BB%2BAiAJEyynPNPhZN8%2Bg1ylO7wthqud9cBcNIChIp2H%2F%2BR7mCryAQjb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDMzOTQ3MDI2MzQyMSIMLqPo1IYyH7udCGZuKsYBSEF3c50YXkmPeSWcLsEzq%2BFBTpeOIrwZTyCUjbJ7fgJUakhM1YRX40jExstN8eJcMXqw00Xd5lYHvZDbU9ajwWPLRAxcEN5BQ0utqn0NGTLyJhibzJUj8cjgm5RguIEKe9GUtMVWa9mi7C5%2FlFpS0i9jK5BSVf74JyPSLETV5mzMMzy5kHBQMGjw1dR66E3MG8PjIqfgKjhVtZmlaicf5OmeqNI2%2F8T5ye%2FICRsH4d7KNEmj4FELa8buW8U%2Fn97ThfH3P7XmMNOok%2F8FOuEBDj1EHluCT4DfZ1jIXjvrJsVv1WtV4POQDn2Dah%2BWosBn%2BFNTtQtw841ACDarYR1ZVbuwcpTjfBPlGuSOncPsbzOhzDy7wYyumsPKsXoPdxTncMWbx4BQkbU5SeF9hjpfIKRMSOqkJBN7%2BtgHXwuW1rfYMDN2OAlQZpTj7uWMPWojUMbvMzyHvI2pfgcRAlrBdGGYDigyjWl9QXP%2Bdi6WiR7XCSXbWcIAJDZh%2Beb%2BIH1asmMJtpAK6nMP8gWczaYh7PMeYyVOIs2B20xQBy%2Bz7oe%2BYQ2GfdEr2hgqPH3jd%2B7c&X-Amz-Signature=11b8cd524c25ef51193e3b3fc4816760ebcde8bfc74bd52f3f91d8bf409620f5&X-Amz-SignedHeaders=host","Key":"112162.jpg"}% 

```

The premise of this is you make a request, and then the response from
the API is a pre-signed URL that then allows you to upload directly to
S3. You can use `curl <url> --upload-file yourfile.jpg`. This
automatically does a PUT request to the s3 bucket (yes, this is talking
directly to s3 now, not the lambda! the lambda is just for generating
the "pre-signed URL" to let you upload). Careful to copy it exactly as
is

```
 curl "https://sam-app-s3uploadbucket-1653634.s3.us-east-2.amazonaws.com/112162.jpg?Content-Type=image%2Fjpeg&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAU6CQBER6YBNCDDMJ%2F20201224%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20201224T174804Z&X-Amz-Expires=300&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDIaCXVzLWVhc3QtMiJGMEQCIH65IvgJsofUpIX46lTaG3Pi5WC85ti1lukM3iICh%2BB%2BAiAJEyynPNPhZN8%2Bg1ylO7wthqud9cBcNIChIp2H%2F%2BR7mCryAQjb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDMzOTQ3MDI2MzQyMSIMLqPo1IYyH7udCGZuKsYBSEF3c50YXkmPeSWcLsEzq%2BFBTpeOIrwZTyCUjbJ7fgJUakhM1YRX40jExstN8eJcMXqw00Xd5lYHvZDbU9ajwWPLRAxcEN5BQ0utqn0NGTLyJhibzJUj8cjgm5RguIEKe9GUtMVWa9mi7C5%2FlFpS0i9jK5BSVf74JyPSLETV5mzMMzy5kHBQMGjw1dR66E3MG8PjIqfgKjhVtZmlaicf5OmeqNI2%2F8T5ye%2FICRsH4d7KNEmj4FELa8buW8U%2Fn97ThfH3P7XmMNOok%2F8FOuEBDj1EHluCT4DfZ1jIXjvrJsVv1WtV4POQDn2Dah%2BWosBn%2BFNTtQtw841ACDarYR1ZVbuwcpTjfBPlGuSOncPsbzOhzDy7wYyumsPKsXoPdxTncMWbx4BQkbU5SeF9hjpfIKRMSOqkJBN7%2BtgHXwuW1rfYMDN2OAlQZpTj7uWMPWojUMbvMzyHvI2pfgcRAlrBdGGYDigyjWl9QXP%2Bdi6WiR7XCSXbWcIAJDZh%2Beb%2BIH1asmMJtpAK6nMP8gWczaYh7PMeYyVOIs2B20xQBy%2Bz7oe%2BYQ2GfdEr2hgqPH3jd%2B7c&X-Amz-Signature=11b8cd524c25ef51193e3b3fc4816760ebcde8bfc74bd52f3f91d8bf409620f5&X-Amz-SignedHeaders=host" --upload-file test.jpg
```

There is no response, but I can then check the s3 console and see the
file upload is successful (all files are renamed)

![](/media/638408397901987840_1.png)

Figure shows that the file upload is successful :)

Then we can edit the file frontend/index.html from the repo we cloned to
contain the lambda with the /uploads/ suffix

![](/media/638408397901987840_2.png)

Figure shows editing the index.html with the lambda endpoint

Then we manually upload this file to another s3 bucket or test it
locally

```
 aws s3 cp index.html s3://mybucket/


# then ...visit that in the browser
```

At this point the files are getting uploaded but not publically
accessible. To make them publically accessible we uncomment the
ACL: 'public-read' in the getSignedURL/app.js folder in the github repo

![](/media/638408397901987840_3.png)

Figure showing the public-read uncommented

![](/media/638408397901987840_4.png)

Figure showing the lines that need uncommenting in template.yaml in the
root of the github repo that allows putObject in s3 with the public-read
ACL

Re-run `sam deploy --guided`, same thing as at the start

Now the objects are publicly accessible!
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Challenges I have faced learning React]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sat, 04 Jul 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Learning React was a big challenge for me. I started learning React in earnest
in 2019. It was a difficult experience overall, but I wanted to go over my
learning experience, and maybe find some lessons in the mix. This goes mostly
into personal details and doesn't really get too technical, however, I review
the commit logs and try and backtrace my feelings that I remember at the time.

If I were to take away anything from this, it's probably that pair programming
was really useful especially as a remote worker, I had nothing before that
except weekly standups where I felt really depressed. Also stay patient, stay
thankful, and try to focus while you learn

**Introduction to me**

I am maybe what you'd call a front-end engineer. I have done web development
for about 7 years now. I worked on various fast-becoming-legacy projects and
greenfield that were made in Ruby, PHP, Perl CGI, Java servlets, etc.

**Early dabbles with React circa 2016**

I had a random `<form>` that I was tasked with making and I wanted to code and
wanted to try using React. I tried importing React via a CDN  and gave it a
shot, and it seemed simple enough, but I kept getting really confused about how
to even read and initialize the value of a textbox for example properly. TLDR:
I was not aware of what a _controlled component_ was.

The idea of controlled components (not a word in my vocabulary at the time) was
quite unintuitive and instead, I kept googling weird things like "two way data
binding react" and variants of this. I had never used Angular but I heard of
two-way data binding from Angular, and I just felt like it was what I needed.
I even posted about my frustrations about this on the React subreddit and was
downvoted. Felt bad. I was just really confused. I abandoned the project in
React and just used our normal jqueryish thing.

**New job in 2018**

When I got a call about a new job in 2018, I was really happy and started in
June 2018. They decided they are going to do "the big rewrite" and are going to
use React. My coworker started building the new React app prototype. My
coworker keeps asking me "what state management library should we use". I just
had no idea about React still, I had not ever looked into state management, and
basically just was like "I dunno!". I had no way to form an opinion. I was also
working on some misc stuff sort of unrelated to the rewrite and remained pretty
out of the loop. We would have weekly meetings but I just wouldn't really
understand the goings ons. The project started using mobx-state-tree and I saw
them start to write fresh code for the project but things like prop-types just
were confusing to me, e.g. there were the mobx-state-tree model types, and
suddenly and the React prop-types and it was still the days of class-based
React components. I couldn't get any clear idea of what was happening

**I am floundering...not understanding what's going on with the rewrite**

It's December 2018, I go home for Christmas and I have an honest talk with my
parents and tell them "I don't get what is happening in the new codebase, I'm
honestly unhappy, and it just does all this 'React' stuff" but I can't explain
React to them I just say the code is automatically reacting to other things. My
parents say "well if you are unhappy you might have to leave your job" and they
are not like, cheering for me to leave, but they tell me that. At this point,
it really hit me that I do like this job and I decided to try to focus on work.

**I try and make an honest attempt to get involved in the project, start pair
programming**

On January 10th 2019 I make my first commit to the rewrite by doing some
monkey-see monkey-do type coding. I copy a bunch of files and just put them in
the right place, tweak some lines, and start to figure out how to make things
run. By the end of January 2019 I get my first code change merged.

I also suggested that we start doing **pair-programming sessions**. Once I
started these it made a huge difference for me in learning how to code. The
pair programming often still way over my head due to how my coworker presented
stuff or how much he assumed I understood. Nevertheless, these were extremely
helpful for me to help get caught up.

**I start to reading "Learning React"**

In March 2019, I got the book "Learning React" (O'Reilly2017
<https://www.oreilly.com/library/view/learning-react/9781491954614/>) for my
kindle.  Reading this book was a big help I felt, and provided a needed "brain
reset" for me. The book worked well for me, I read it each night on my kindle,
and the function component concepts were super enlightening. To me it was so
much better reading a book than, say, an internet tutorial. With the book, I
could focus, not have distractions, etc. My eyes would just glaze over every
time I clicked on internet tutorials and stuff before this.

So anyways, March 2019 goes on, and I'm learning, but our codebase still feels
pretty complicated and alien. We use mobx-state-tree and the glue for
mobx-state-tree to React e.g. the mobx-react doesn't really make sense to me. I
remember asking my coworkers why my component was not updating and they
eventually find out it's because I keep not using the observe() wrapper around
my components.

**I start to experiment with Typescript**

In April 2019 I start to experiment with typescript and release a typescript
version of some data parsing code. I start by explicity specifying a lot of
types but I eventually start getting into the zen of "type inference" and I
turn off the @typescript-eslint/explicit-function-return-type so I get implied
return types.

**I start using React hooks**

In May 2019 I try out my first React hook, a useState. It worked well. I
couldn't really figure out why I would use it instead of the mobx state
management we used elsewhere, but the example was that it was a click and drag
and it made sense to keep that click and drag state local to the component
rather than the "app"

**I start using react-testing-library**

In June 2019, I create "integration test" level tests for our app. I had used
react-testing-library for some components before this, but this was using
react-testing-library to render the entire "app level" component. I was happy
to pioneer this and was happy to try this out instead of doing true browser
tests, and I think this has worked out well.

Some caveats: I got very caught up with trying to do canvas tests initially. I
really wanted to use jest-mock-canvas but we were using offscreencanvas via a
pretty complicated string of things, so I don't make progress here, and I also
got confused about the relationship between node-canvas and jest-mock-canvas
(they are basically totally different approaches). Later on, I find using
jest-image-snapshot of the canvas contents works nice (ref
<https://stackoverflow.com/questions/33269093/how-to-add-canvas-support-to-my-tests-in-jest>)

Other random note: when building out the integration tests, we got a lot
of "act warnings" which were confusing. These were fixed in React 16.9
(released August 2019), but we had to ignore them and they basically just
confused me a lot and made it feel like I was battling a very complex system
rather than a nice simple one.

**Conclusions**

Overall, I just wanted to write up my whole experience. It felt really
difficult for me to make these changes. I also went through a breakup during
this time, had a bad living situation, etc. so things were a struggle. If
anyone else has had struggles learning React, tell your story, and let me know.
I'd like to also thank everyone who helped me along the way. I feel like a much
better coder now, yet, I should always keep growing. The feeling of
uncomfortableness could be a growing experience.
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Misconceptions your team might have during The Big Rewrite]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Wed, 03 Jun 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Disclaimer: I enjoy the project I am working on and this is still a work
in progress. I just had to rant about the stuff I go through in my job
here, but it does not reflect the opinions of my emplorer, and my
personal opinion is despite these troubles we are coming along nicely

I joined a team that was doing the big rewrite in 2018. I was involved
in the project before then and knew it's ins and outs, and frankly think
it's still a great system. In order to break it's "limitations" a grand
v2 gets started. I think my team has been good. My tech lead is really
good at architecture. Where I really resist kind of "writing new
architecture that is not already there", he can pull up entirely new
concepts and abstractions that are all pretty good. Myself, I don't much
enjoy writing "new architecture" if there is something already there
that I can use, and I'll try to refer to the existence of an existing
thing instead of creating new exotic stuff.

Now, what happened during the big rewrite so far. 4 people on the team,
2 years in

Persistent confusion about sources of slowness in our app

- it's only slow because devtools is open (maybe it is! but this is
  definitely a red herring. the code should work with devtools open.
  reason that's been stated: devtools adds a "bunch of instrumentation to
  the promises that slows it down"...stated without any evidence during a
  3 hour long planning call...)
   - it's only slow because we're using a development build of react, try
  a production build (the production build makes some stuff faster, but it
  is NOT going to save your butt if you are constantly rerending all your
  components unnecessarily every millisecond during user scroll, which is
  something we suffered from, and it creeps back in if you are not careful
  because you can't write tests against this so often one day I'll be
  looking at my devtools and suddenly things are rendering twice per frame
  (signature of calling an unnecessary setState), tons of unnecessary
  components rendering in every frame (signature of
  componentShouldUpdate/bad functional react memoizing, etc))
   - it's slow because we are hogging the main thread all the time, our
  killer new feature in v2 is an intense webworker framework. now main
  thread contention is a concern, but really our app needs to just be
  performant all around, webworkers just offloads that cpu spinning to
  another core. what we have done in v2 is we went whole hog and made our
  code rely on OffscreenCanvas which 0 browsers support. also, our
  webworker bundles (worker-loader webpack build) are huge webpack things
  that pretty much contain all the code that is on the main thread so it's
  just massive. that makes it slow at loading time, and makes it harder to
  think about our worker threads in a lighter-weight way, and the worker
  concept is now very deeply entrenched in a lot of the code (all code has
  to think of things in terms of rpc calls)
   - it's slow because there are processes that haven't been aborted
  spinning in the background, so we must build out an intensive
  AbortController thing that touches the entirety of all our code
  including sending abort signals across the RPC boundary in hopes that a
  locked up webworker will respond to this (note: our first version of the
  software had zero aborting, did not from my perspective suffer.
  arguments with the team have gotten accusatory where I just claim that
  there is no evidence that the aborting is helping us, pointing to the
  fact that our old code works fine, and that if our new code suffers
  without aborting, that means something else is wrong. I have not really
  been given a proper response for this, and so the curse of passing
  AbortSignals onto every function via an extra function parameter drags
  on
   - it's slow because we are not multithreading..., so we put two views
  of the same data into different webworkers (but now each webworker
  separately downloads the same data, which leads to more resource spent,
  more network IO, more slowness)

confusion about what our old users needs are

- tracks not having per-track scroll (problem: leads to many scrolls
  within-scrolls, still unresolved problem)
   - the name indexing was always a big problem (yes it is slow but is it
  really THE critical problem we face? likely not: bioinformatics people
  run a data pipeline, it takes a couple days, so what). use elasticsearch
  if it sucks so bad
   - our users are "stupid" so they need to have every single thing GUI
  editable (interesting endeavor, but our design for this has been
  difficult, and has not yet delivered on simplifying the system for
  users)
   - our users "do not like modal popups" so we design everything into a
  tiny sidedrawer that barely can contain the relevant data that they want
  to see

- having interest in catering to obscure or not very clear "user
  stories" like displaying the same exact region twice on the screen at
  once saying "someone will want to do this", but causing a ton of extra
  logical weirdness from this
- not catering to emerging areas of user needs such as breaking our
  large app into components that can be re-used, and instead just going
  full hog on a large monolith project and treating our monolith as a
  giant hammer that will solve everyones problems, when in reality, our
  users are also programmers that could benefit from using smaller
  componentized versions of our code
- confusion about "what our competitors have". sometimes my team one day
  was like "alright we just do that and then we have everything product X
  has?" and I just had to be clear and be like, no! the competitor has a
  reall pretty intricate complex system that we could never hope to
  replicate. but does that matter? probably not, but even still, we likely
  don't have even 20% of the full set of functions of a competitor.
  luckily we have our own strengths that make us compelling besides that
  20%
- making it so our product requires a server side component to run,
  where our first version was much more amenable to running as a static
  site

- etc...

but what does all this imply?

there are persistent confusion about what the challenges we face are,
what the architectural needs are, what our user stores are, what our new
v2 design goals are, and more. It's really crazy
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Behind the release - the story of the bugs and features in JBrowse 1.16.0]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Mon, 17 Dec 2018 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Every once in awhile, you might see that your favorite program, JBrowse,  has a
new release. There are a ton of little snippets in the release notes, you might
as well just go ahead and upgrade, but what went into all those little fixes?
Going to the blog post has links to the github
issues, <http://jbrowse.org/blog/2018/12/13/jbrowse-1-16-0.html> but I felt
like maybe I'd add a little more context for some of them:

PS This is sort of motivated by @zcbenz blog on Electron
(<https://twitter.com/zcbenz> <http://cheng.guru/>) which tells the software in
terms of actual commit messages and such.

- The webpack build doing a production build by default. This seems pretty
  straightforward, but was also difficult because I use WSL and the UglifyJs
  plugin had trouble on WSL using the parallel: 4 option to use multiple
  processors. This was really annoying and resulted in the webpack build just
  hanging for no reason and only careful google-fu really uncovered other
  people having this issue. I removed the parallelism as the speed gain wasn't
  even really justifiable https://github.com/gmod/jbrowse/pull/1223

- The incorporation of the `@gmod/bam` module. This was an almost 2 months
  process after my first module, `@gmod/indexedfasta`. It required really
  getting down to the binary level for BAM and was pretty tough. The module
  has already itself had 12 releases [here](https://github.com/GMOD/bam-js/blob/master/CHANGELOG.md)

- Added support for indexing arbitrary fields from GFF3Tabix files. This was
fairly straightforward but required making design decisions about this.
Previously flatfile-to-json.pl files would have a command line flag to index
arbitrary fields. Since gff3tabix files are specified via config, I allowed
specifying arbitrary fields via config.

- Added ability to render non-coding transcript types to the default Gene
glyph. This one was a nice feature and enables you to see non-coding types, but
required some weird design decisions because I could not override
the `box->style->color` from a higher level type simply using the
`_defaultConfig` function, so I needed to override the `getStyle` callback
that was passed down to the lower levels, so that it was able to use the
default lower level style and also our non-coding transcript style. See this
part of the code for
details <https://github.com/GMOD/jbrowse/commit/ec638ea1cc62c8727#diff-a14e88322d8f4e8e940f995417277878R22>

- Added `hideImproperPairs` filter. This was fairly straightforward but it is one
of these bugs that went unnoticed for years...the `hideMissingMatepairs` flag
would hide things didn't have the sam 0x02 flag for "read mapped in proper
pair", but reads with this flag could still be paired. Doing the 1.16 release
that focused on paired reads helped focus on this issue and now
hideMissingMatepairs filters on "mate unmapped" and `hideImproperPairs` is
the "read mapped in proper pair"

- Added `useTS` flag. This one is fairly straightforward, it is similar to
  `useXS` which colors reads based on their alignment in canonical splice site
  orientations. I figured I could just copy the `useXS` to the `useTS` since I
  figured they are the same, but I went ahead and manually generated RNA-seq
  alignments with minimap2 and found that the useTS is actually flipped the
  opposite of `useXS`, so it was valuable to get actual test data here.

- Fixed issue where some `generate-names` setups would fail to index features.
This was a bad bug that was brought to light by a user. I was kind of mind
boggled when I saw it. In JBrowse 1.13-JBrowse 1.15 a change was introduced to
name indexing with a memory leak. In JBrowse 1.15 that was removed. But, there
was another change where refseqs could return empty name records, because they
were handled separately. But if the initial fill up of the name buffer of 50000
was exceeded by the reference sequence, then there would be empty name records
after this point and cause the name indexing to stop. Therefore this bug would
only happen when the reference sequence indexing buffer exceeded 50000 items
which could happen even when there are less than 50000 refseqs due to
autocompletions

-  Fixed issue with getting feature density from BAM files via the index stats
estimation. This involved parsing the "dummy bin" from index files, and I found
it was failing on certain 1000 genomes files. I actually don't really know what
the story behind this was, but our tabix code was better at parsing the dummy
bins than my bam code, and it was the same concept, so I took a note from their
codebase to use it in bam-js code. Commit
here https://github.com/GMOD/bam-js/commit/d5796dfc8750378ac8b875615ae0a7e81371af76

-  Fixed issue with some GFF3Tabix tracks having some inconsistent layout of
features. This is a persistently annoying fact in tabix files where we cannot
really get a unique ID of a feature based on it's file offset. Therefore this
takes the full crc32 of a line as it's unique ID.

- Fixed CRAM store not renaming reference sequences in the same way as other
  stores. This one was interesting because rbuels made a fix but it caused
  features from one chromosome to show up on the wrong ones, so chr1 reads
  where showing up on chrMT. This happened because it was falling back to the
  refseq index if it chrMT wasn't in the embedded "sam header" in the CRAM
  file, but it should only fallback to refseq index if there is not any
  embedded "sam header" in the CRAM file.

-  Fixed bug where older browsers e.g. IE11 were not being properly supported
via babel. This was a absolutely terrible bug that I found over thanksgiving
break. It was a regression from 1.15 branch of JBrowse. Previous versions from
1.13 when webpack was up until 1.15 used `@babel/env`. It was changed to
babel-preset-2015 but it was not being run correctly. Then I found that even if
I did get it running correctly, it was unable to properly babel-ify the
lru-cache module because it used something called
`Object.defineProperty('length', ...)` to change how the length property was
intepreted which was illegal in IE11. The 'util.promisify' NPM module also did
this in some contexts. I found that I could use the quick-lru module and the
es6-promisify module instead of lru-cache and util.promisify as a workaround.
Then I had to update all `@gmod/tabix`, `@gmod/vcf`, `@gmod/bgzf-filehandle`,
`@gmod/indexedfasta`, `@gmod/tribble-index`, `@gmod/bam`, and JBrowse proper to
use these modules instead, and make the bable chain, which typically does not
parse node_modules, to build these modules specifically (I didn't want to setup
babel toolchains for every single one of these modules, just one in the jbrowse
main codebase...). This was really a lot of work to support IE11 but now that
works so ...ya

-  Fixed bug where some files were not being fetched properly when changing
refseqs. This was actually fixed when I changed out lru-cache for quick-lru and
fixed a bug where the cache size was set to 0 due to a erroneous comment that
said `50*1024 // 50MB`...of course it should have said `50*1024*1024 // 50MB` https://github.com/GMOD/jbrowse/commit/2025dc0aa0091b70

- Fixed issue where JBrowse would load the wrong area of the refseq on startup
  resulting in bad layouts and excessive data fetches. This was actually a
  heinous bug where jbrowse upon loading would just navigateTo the start of the
  reference sequence automatically and then to wherever was specified by the
  user. This resulted in track data to start downloading immediately from the
  start of the chromosome and resulted in for example 350 kilobases of
  reference sequence from all tracks to start downloading, which when I was
  implementing view as pairs, was causing me to download over 100MB routinely.
  This was terrible, and after fixing I only download about 10MB over even
  large regions for most BAM files. Additionally, this bug was causing the
  track heights to be calculated incorrectly because the track heights would
  actually be calculated based on distorted canvas
  bitmaps. https://github.com/gmod/jbrowse/issues/1187

- JBrowse Desktop was not fetching remote files. This was a weird issue where
  remote file requests were considered a CORS requests to any external remote.
  This was solved by changing the usage of the fetch API in JBrowse for
  node-fetch which does not obey CORS. Note that electron-fetch was also
  considered, which uses Chromiums network stack instead of node's, but that
  had specific assumptions about the context in which it was called.

-  Fixed issue where some parts of a CRAM file would not be displayed in
JBrowse due to a CRAM index parsing issue. This was based on a sort of binary
search that was implemented in JBrowse where the elements of the lists were
non-overlapping regions, and the query was a region, and the output should be a
list of the non-overlapping regions that overlap the query. Most algorithms for
binary search don't really tell you how to do searches on ranges so needed to
roll up my sleeves and write a little custom code. An interval tree could have
been used but this is too heavy-weight for non-overlapping regions from the
index https://github.com/GMOD/cram-js/pull/10

-  Fixed an issue where BAM features were not lazily evaluating their tags.
When a function `feature.get('blahblah')` is called on a BAM feature, it checks
to see if it's part of a default list of things that are parsed like feature
start, end, id, but if not, it has to parse all the BAM tags to see if it is a
tag. Since they are called "lazy features" the tag processing is deferred until
it is absolutely needed. As it turned out, the incorporation of CRAM in 1.15
was calling a function to try to get the CRAM's version of CIGAR/MD on the BAM
features unnecessarily invoking the tag parsing on every feature up front and
therefore making the feature not really lazy anymore. This restored
the "lazyness" aspect of BAM.

-  Fixed issue where CRAM layout and mouseover would be glitchy due to ID
collisions on features. In the 1.15 releases, CRAM was introduced, and we
thought that the concept of taking CRC32 of the entire feature data days were
over because there is the concept of a "unique ID" on the features. However,
this ID was only unique within the slices, so around the slice boundaries there
were a lot of bad feature layouts and mouseovers would fail because they would
map to multiple features, etc. I found a way to unique-ify this by giving it
the sliceHeader file offset. <https://github.com/GMOD/cram-js/pull/10>

- We also had behind the scenes work by igv.js team member jrobinso who helped
  on the CRAM codebase to incorporate a feature where for lossy read names, so
  that a read and it's mate pair would consistently be assigned the same read
  name based on the unique ID mentioned above. There was also a rare issue
  where sometimes the mate pair's orientation was incorrectly reported based on
  the CRAM flags, but the embedded BAM flags correctly reported it.

- Finally the paired reads feature. This was a feature that I really wanted to
  get right. It started when garrett and rbuels were going to san diego for the
  CIVIC hackathon, and we talked about doing something that matched a "variant
  review system" that they had done for the IGV codebase, which involved
  detailed inspection of reads. I thought it would probably be feasible for
  jbrowse to do this, but I thought essentially at some point that enhancing
  jbrowse's read visualizations with paired reads would be a big win. I had
  thought about this at the JBrowse hackathon also and my discussions then were
  that this was very hard. Overall, I invented a compromise that I thought was
  reasonable which was that there can be a "maxInsertSize" for the pileup view
  beyond which the pairing wouldn't be resolved. This allowed (a) a significant
  reduction in data fetches because I implemented a "read redispatcher" that
  would actually literally resolve the read pairs in the separate chunks and
  (b) a cleaner view because the layout wouldn't be polluted by very long read
  inserts all the time and also, for example, if you scrolled to the right, and
  suddenly a read was paired to the left side of your view, it would result in
  a bad layout (but with max insert size, the window of all reads within
  maxinsertsize are always resolved so this does not happen) and finally ( c)
  the paired arc view was incorporated which does not use read redispatching
  and which can do very long reads. All of these things took time to think
  through and resolve, but it is now I think a pretty solid system and I look
  forward to user feedback!
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Problems that I experienced with the HPCC]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Fri, 21 Apr 2017 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Many of these issues may be due to me being stubborn with a weird build system.
Nonetheless, they were baffling, and I had very little interest in debugging
these issues. I just wanted to get my science done after all!

# Module load completely barfs with incomprehensible error

    $ module spider bedtools
    Using system spider cache file
    /opt/software/lmod/bin/lua: /opt/software/lmod/4.1.4icer5/libexec/Spider.lua:662: attempt to perform arithmetic on a nil value
    stack traceback:
        /opt/software/lmod/4.1.4icer5/libexec/Spider.lua:662: in function 'Level1'
        /opt/software/lmod/4.1.4icer5/libexec/Spider.lua:640: in function 'spiderSearch'
        /opt/software/lmod/4.1.4icer5/libexec/lmod:967: in function 'cmd'
        /opt/software/lmod/4.1.4icer5/libexec/lmod:1195: in function 'main'
        /opt/software/lmod/4.1.4icer5/libexec/lmod:1222: in main chunk
        [C]: ?

# Linuxbrew is terribly confused by things that depend on gcc

    brew install hello
    ==> Installing dependencies for hello: glibc, xz, gmp, mpfr, libmpc, isl, gcc
    ==> Installing hello dependency: glibc
    Error: glibc cannot be built with any available compilers.
    Install Clang or brew install gcc

Using module load Clang does not fix problem \>\_\<

# Compiling things manually on software machine does not work on interactive machine

    $ mummer
    Illegal instruction (core dumped)

# Many modules have a secret dependency on loading other modules

    $ module load LASTZ

    Lmod Warning: Did not find: LASTZ

    Try: "module spider LASTZ"
    $ module load GNU
    $ module load LASTZ
    $ lastz
    You must specify a target file
    lastz-- Local Alignment Search Tool, blastZ-like
      (version 1.03.02 released 20110719)
    ...

Etc etc.
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[How I learned to hate ORM (especially for data import scripts)]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sun, 12 Mar 2017 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
When I was tasked with making a new application for our websites, I was
given several CSV files with some expectation that these files could
basically be just loaded into a database and jumped into production really
quickly. If you are using R and Shiny to make a data visualization dashboard,
especially if it is read only, this can actually be a reality for you: load
those CSVs and just pretend you're a full featured database. I had to actually
create some read write functionality though. This was sort of experimental for
me and I'm not that well versed in databases, but I wanted to share my
experience

When I started, I chose grails/groovy/hibernate/GORM as a platform to
use. This quickly turned into pain when I tried to make a data importer
using grails also.

Each CSV row from the source file would have to be turned into many
different rows in the database because it represented multiple
relationships, example:

![](/media/158300473458_0.png)

Initially I made my data importer in grails, and was hardcoding column
names knowing full well this was really inflexible. At the same time I
was also trying to "iterate" on my database schema, and I'd want to
re-import my data to test it out, but it was really really slow. I tried
many different approaches to try to speed this up such as cleanUpGorm,
StatelessSessions, and other tricks, but it would take 10-20 minutes for
imports on a 100KB input file.

What I basically realised is that for bulk data import

**1) Using the ORM is really painful for bulk import.**

**2) If you can pre-process your data so that it is already in the
format the database expects, then you can use the CSV COPY command which
is very fast**

**3) If you can then abandon the ORM mentality and even ignore it as
a convenience factor, then you can embrace my database system itself**

Overall, after all this work, it just seemed like ORM treats the
database as a danger and something to be heavily abstracted over, but I
actually found joy in learning how to treat my database as a first class
citizen. Soon I started gaining appreciation of

- using plain SQL queries
- learning about full text search in postgres with ts_query
- learning about triggers to make a "last updated" field get updated
  automatically

I am pretty happy this way, and although I miss some things like
criteria queries which are very powerful, I am happy that I can interact
with my database as a friend

At the very least, due to the fact that I now pre-process the data
before database loading, I can now import large amounts of data super
fast with the CSV COPY command
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Plotting a coordinate on the screen]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Thu, 16 Feb 2017 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
I always end up having to remember the math for plotting a coordinate on the
screen, for example an HTML5 canvas and end up stitching it together manually

If you step through the math it becomes very simple though

Say you have a coordinate range of 1000 to 2000 that you want to plot in a
HTML5 canvas of size 100px

Let's do a quick example and then generalize. Let's say you want to plot the
value 1500, and put it into screen coordinates, so you take that and subtract
the minimum of the range

```
1500-1000
```

Second, you know your point is going to be halfway in the range, and in
general, to get this position, you divide now by the size of the interval you
are plotting in, e.g. 2000-1000

```
(1500-1000)/(2000-1000) = 0.5
```

We get 0.5 as expected. Then you multiply this proportion times the width of
box you are rendering in, e.g. 100 pixels wide, and get that you put your pixel
at position 50px

To summarize, the general formula for plotting a point x in a range (x1,x2) on
a screen of width w is

```
w*(x -  x1) / (x2 - x1)
```

Of course same thing applies for y

```
h*(y - y1) / (y2 - y1)
```

This does not take into account small possible adjustments for closed vs open
ranges, which could be important to avoid subpixel rendering on a canvas, but
that can be a further exercise
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Creating a JBrowse plugin]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Thu, 10 Nov 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
I have been very impressed with peoples creativity and willingness to
dig into all the details of JBrowse to customize it's features. One
great way to do this in a modular way is to create a "JBrowse plugin".
This concept sounds hard but you can set up a simple couple of files and
refresh your browser and it will "just work"!

**Introduction to the plugin mindset**

In a plugin, you can define new things like custom track types, custom
adaptors to new file types, new track selectors, or something really
different. A key insight about the custom types of tracks and things
though is that you can put the name of your new custom class in the
jbrowse config file which will then find the code in your plugin and run
it. Plugins can do other things, but the ability to just swap out track
types or other components in the config file is a great benefit.

**A scenario**

One example that was talked about on the mailing list might involve
adding new menu items for a given track. We might consider a plugin that
defines a custom track type to add that functionality.

Basically, we can use object- oriented principles to "inherit" from some
existing track type like CanvasFeatures and then extend its
functionality by overriding the functions.

If you are not familiar with object-oriented javascript, dojo makes it
pretty easy (but, especially get a background on this if you need to,
see footnotes below).

We can inherit a new track type by using the "define" function to
include the dependencies needed in a file, and they are listed in an
array at the top of your file.

Then the "declare" function creates a new class. The first argument to
declare is the is your parent class, like CanvasFeatures, and we type
"return declare" because we are returning our new track class from the
module.

```
 define( ["dojo/_base/declare",
 "JBrowse/View/Track/CanvasFeatures"],
     function(declare,CanvasFeatures) {
     return declare(CanvasFeatures, {
         _trackMenuOptions: function() {

             var opts=this.inherited(arguments); //call the parent
 classes function

             opts.push( // add an extra menu item to the array returned
 from parent class function
                 {       
                     label: "Custom item",
                     type: 'dijit/CheckedMenuItem',
                     onClick: function(event) {
                         console.log('Clicked');
                     },  
                     iconClass: "dijitIconPackage"
                 }   
             );  
             return opts;
         }   
     });
     }   
 );
```

Code listing 1. an example custom track type, MyTrack.js, that adds an
extra track menu item

**Now how do we make this a plugin?**

In the above section, we created a new track subclass with a custom menu
option. How do we use this track? We want to turn it into part of afine
the boilerplate code from the [Writing
plugins](http://gmod.org/wiki/JBrowse_Configuration_Guide#Writing_JBrowse_Plugins)
guide.

```js
 define([
            'dojo/_base/declare',
            'JBrowse/Plugin'
        ],  
        function(
            declare,
            JBrowsePlugin
        ) {
  
 return declare( JBrowsePlugin, // our plugin's main.js derives from
 the "JBrowse/Plugin" base class
 {
     constructor: function( args ) {
         /*don't necessarily have to do any initializing here, but you
 can get a handle to various jbrowse components if any initialization
 is needed from the args.browser variable*/
     }   
 });
 });
```

Code listing 2. Our plugin's main.js

After this, we create the plugin directory structure

> jbrowse/plugins/MyPlugin
>
> > jbrowse/plugins/MyPlugin/js
> >
> > > jbrowse/plugins/MyPlugin/js/main.js
> > >
> > > jbrowse/plugins/MyPlugin/js/MyTrack.js

Then we can add our new plugin to a config file like jbrowse_conf.json
as "plugins": ["MyPlugin"]  and then make a track in trackList.json
have "type":  "MyPlugin/MyTrack" instead of for
example "type": "CanvasFeatures". That will tell jbrowse to load the
MyTrack class from your plugin instead of the normal CanvasFeatures
class.

That's about it!

Note that the bin/new-plugin.pl script can automatically initialize some
of this directory structure too. Try running "bin/new-plugin.pl
MyPlugin" and see what happens.

Footnotes:

It is important to understand the module format (AMD) which is what the
"define" function is about and the dojo way of definining classes using
the "declare" function. Together, this allows the dojo to create
object-oriented programs that are modular in javascript. See
<http://dojotoolkit.org/reference-guide/1.10/dojo/_base/declare.html>
and <http://dojotoolkit.org/documentation/tutorials/1.9/modules/>
(understanding this helps you understand the "preamble" for declaring a
jbrowse plugin)
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Fixing spiky CPU issues with Tomcat]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Fri, 16 Sep 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
The symptoms of spiking that we saw were simply that after light usage
of the applications, the CPU usage would start spiking and rapidly cycle
from many CPU cores (e.g. 2000% CPU usage) back to 0% CPU for no
apparent reason.

We now know this was due to memory issues and garbage collection, but it
was confusing because it wasn't strictly showing up as GC usage in
JVisualVm (the GC usage, blue spikes on the left in fig 1, are small,
but the orange spikes are large, even though the memory issues are the
problem)

Here is what it looked like during spiking (obviously, pushing the
memory limits here, a linked in article suggests having 6GB of "newgen"
memory, so on top of the old gen, tomcat needs a bunch more for the
newgen to make things happy.

![](/media/129241954103_0.png)

Here is what it looks like when it is not spiking

![](/media/129241954103_1.png)

Edit: See this follow up post for showing that increasing memory helps
<http://searchvoidstar.tumblr.com/post/131229569383/tomcat-memory-debugging>

::: {#footer}
[ September 16th, 2015 6:37pm ]{#timestamp} [tomcat]{.tag} [java]{.tag}
[programming]{.tag} [coding]{.tag} [troubleshooting]{.tag}
[intermine]{.tag} [bioinformatics]{.tag}
:::

export default ({ children }) => <Layout>{children}</Layout>
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Installing clamav on OSX]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Mon, 20 Jun 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
It is a common trope that OSX doesn't need anti-virus because everyone targets
windows. That is maybe comforting to some but I think it's pretty naive. It
would be better to have a system on your machine to tell you about viruses,
trojan horses, malware, or spying.  I have decided to employ a free open source
scanner called clamAV <https://www.clamav.net/>. I don't really know if it has
any good features for Mac scanning but thought it could be fun to install

ClamAV is the top choice for linux based OSs being free and open source (GPL)
virus scanner.

To install we can use homebrew

```
    $ brew install clamav
```

Then there is s config file to setup. This is located
in /usr/local/etc/clamav/freshclam.conf

To setup, edit this file and delete the line that says "Example" and
then uncheck the desired settings. I would chose to enable logging to
/var/log/clamav.log and also database directories in /var/lib/clamav

Then run the "freshclam" program

```
    $ freshclam
```

This will download the virus scanner database (main) and daily scanning
updates

Then you can run clamscan on a given directory (recursively, only print
infected files)

```
    $ clamscan -ri ~/
```

Or add this to a cronjob

```
    $ crontab -

    @hourly clamscan -ri ~/ | mail -v -s "clamscan results" your.email@gmail.com  >/dev/null 2>&1

```
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Querying InterMine databases using R]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Fri, 17 Jun 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
In the past, I had found some ways to do simple queries on InterMine web
services using basic HTTP commands with R (see
<https://gist.github.com/cmdcolin/4758167bdd89e6c9c055>)

However, the InterMineR (<https://github.com/intermine/intermineR>)
package automates some of these features and makes it easier to load the
data in R.

**Installation**

One way to install InterMineR is to install from github with
hadley/devtools

    install.packages("devtools")
    devtools::install_github("hadley/devtools")
    devtools::install_github("intermine/intermineR")

**Usage**

Basic usage includes loading the "intermine URL" using the initInterMine
function. Then various functions can be called on this result.

    library(InterMineR)
    mine=initInterMine("http://bovinegenome.org/bovinemine/")
    getVersion(mine) #18, intermine API version
    getRelease(mine) #1.0, our data release version
    getTemplates(mine) # lists all templates on interminer

**Run a template query**

From the getTemplates function, if you see a template query that you
want to run, you can use the getTemplateQuery function with it's name,
and run it with the runQuery function

    getTemplateQuery(mine,"TQ_protein_to_gene") # see what template looks like
    template=getTemplateQuery(mine,"TQ_protein_to_gene") # save template
    runQuery(mine,template) # run the template query with default params, receive data.frame

This method is good, but some improvement could be added to change
default parameters in the template query, etc.

**Run query XML**

Another option for running queries is to use the query XML that you can
download from the InterMine query result pages.

```
 # get all Ensembl genes on chr28 from bovinemine
 query='<query model="genomic" view="Gene.primaryIdentifier
 Gene.secondaryIdentifier Gene.symbol Gene.name Gene.source
 Gene.organism.shortName Gene.chromosome.primaryIdentifier"
 sortOrder="Gene.primaryIdentifier ASC" ><constraint
 path="Gene.organism.shortName" op="=" value="B. taurus"
 /><constraint path="Gene.chromosome.primaryIdentifier" op="="
 value="GK000028.2" /></query>'

 results=runQuery(mine, query)

 head(results)
```

**Conclusion**

The InterMineR package has a couple of nice features for getting
InterMine data with a couple of functions for looking at templates. For
many use cases, copying the Query XML from a InterMine webpage and
pasting that into the runQuery function is sufficient and produces a
data frame that can be analyzed.

PS it is not easy to post XML on tumblr after editing the post in
markdown mode. You have to add the lt and gt shortcuts and even after
that it gets filtered?!
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[How to make your resume.json or resume-cli look great]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sat, 23 Apr 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
There are a ton of themes for resume-cli that are not immediately
obvious to find

To see all the great themes on the command line, check out

```
    curl http://themes.jsonresume.org/themes.json |jq .
```

I tried a bunch of them

```
   4679  resume export site/resume/index.html -t modern-freeland
   4680  resume export site/resume/index.html -t modern-freelance
   4682  resume export site/resume/index.html -t modern-with-projects-section
   4683  resume export site/resume/index.html -t dangerflat
   4684  resume export site/resume/index.html -t striking
   4685  resume export site/resume/index.html -t crisp
   4686  resume export site/resume/index.html -t semantic-ui
   4687  resume export site/resume/index.html -t material
   4688  resume export site/resume/index.html -t modern-extended
   4689  resume export site/resume/index.html -t paper
   4690  resume export site/resume/index.html -t smart
   4691  resume export site/resume/index.html -t flat

```

Note: resume.json is setup to use HTML themes, so even though it has a
PDF output option, it is inherently converting HTML first and then to
PDF. The PDF conversion is done by a automated cloud service, which
currently can fail sometimes. It is probably better to just choose HTML
and convert to PDF if you need to.
See <https://github.com/jsonresume/resume-cli/issues/94>
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Creating a testing framework for JBrowse plugins]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Tue, 19 Apr 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Testing client side apps requires a couple of tedious steps: Organizing
the git clone, the dependencies, wrangling up a web server, the test
framework, etc.

When testing a plugin for jbrowse, the dependency tree is interesting
because the plugin "depends" on JBrowse to run, but we will use
travis-CI and bower inside the git repo for our plugin to accomplish
this.

In this scenario, we will

1.  Use bower to install jasmine and JBrowse (our platform that we write
    the plugin for)

2.  Use nginx to launch a webserver on travis-CI

3.  Use the phantomjs run-jasmine.js script to check jasmine test
    results

Without further ado

Here is the .travis.yml

    sudo: false
    addons:
      apt:
        packages:
        - nginx
    cache:
      apt: true
      directories:
      - $HOME/.cache/bower
    before_install:
      - npm install -g jshint bower
    install:
      - bower install
    before_script:
      - cat test/travis.conf | envsubst > test/travis-envsubst.conf
      - nginx -c `pwd`/test/travis-envsubst.conf
    script:
      - phantomjs test/run-jasmine.js http://localhost:9000/test/
      - jshint js

Refer to
<http://searchvoidstar.tumblr.com/post/141858047213/running-nginx-on-containerised-travis-ci-pt-2>
for details on the nginx setup

Here is the bower.json

    {
      "name": "sashimiplot",
      "homepage": "https://github.com/cmdcolin/sashimiplot",
      "description": "Sashimi track type for jbrowse",
      "main": "js/main.js",
      "keywords": [
        "bioinformatics",
        "jbrowse"
      ],
      "license": "MIT",
      "ignore": [
        "**/.*",
        "node_modules",
        "bower_components",
        "src",
        "test",
        "tests"
      ],
      "devDependencies": {
        "jasmine-core": "jasmine#^2.4.1",
        "jbrowse": "git://github.com/GMOD/jbrowse.git#master"
      }
    }

The key thing here is that it installs jasmine and JBrowse. I set
.bowerrc to install both jasmine and JBrowse to the "test" directory

    {
        "directory": "test"
    }

With this setup, bower will make a "flat dependency tree" in the test
directory, so it will look like this

    $ ls -1 test
    FileSaver
    dbind
    dgrid
    dijit
    dojo
    dojox
    *index.html*
    jDataView
    jasmine-core
    jbrowse
    json-schema
    jszlib
    lazyload
    put-selector
    *run-jasmine.js*
    *spec*
    *travis.conf*
    util
    xstyle

Here the asterisks indicate things that are part of our app, other's are
automatically installed by bower (jbrowse, jasmine-core, the dojo
dependencies, and other things)

Then we can create the jasmine test/index.html to be something like this

    <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
      "http://www.w3.org/TR/html4/loose.dtd">


      <meta>
      Jasmine Spec Runner

      <link rel="stylesheet" href="jasmine-core/lib/jasmine-core/jasmine.css"><script src="jasmine-core/lib/jasmine-core/jasmine.js"></script><script src="jasmine-core/lib/jasmine-core/boot.js"></script><script type="text/javascript" src="dojo/dojo.js" data-dojo-config="async: 1"></script><script type="text/javascript">
        require( { baseUrl: '.',
                   packages: [
                       'dojo',
                       'dijit',
                       'dojox',
                       'jszlib',
                       { name: 'lazyload', location: 'lazyload', main: 'lazyload' },
                       'dgrid',
                       'xstyle',
                       'put-selector',
                       'FileSaver',
                       { name: 'jDataView', location: 'jDataView/src', main: 'jdataview' },
                       { name: 'JBrowse', location: 'jbrowse/src/JBrowse' },
                       { name: 'SashimiPlot', location: '../js' }
                   ]
        });
      </script><script type="text/javascript" src="spec/SashimiPlot.spec.js"></script><div id="sandbox" style="overflow:hidden; height:1px;"></div>

The "packages" in the require statement puts all these packages in the
right "namespace" for the AMD includes, and the "specs" are defined like
`<script type="text/javascript" src="spec/Projection.spec.js"></script>`

Finally, run-jasmine.js is used to check the results of the jasmine
tests (it is run via phantomjs in the travis-CI script). It is a special
version for the most recent version of jasmine (2.4)
<https://gist.github.com/vmeln/b6cbb319d9a0efc816be>

For an example of the project using this, see
<https://github.com/cmdcolin/sashimiplot>
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Creating a docker image]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sun, 17 Apr 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Example

```
    brew install docker boot2docker docker-machine
    docker-machine create --driver virtualbox default
    docker-machine env default # will output some variables
    eval "$(docker-machine env default)" # use those variables
    # make dockerfile
    docker build -t nameof-yourimage .
```
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Basic command line productivity tricks and learning experiences]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Wed, 06 Apr 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
- dd deletes line in vim
- Ctrl+d scrolls down in vim
- Learn to love your package manager. Homebrew, NPM, gem, cpanm,
  gvm/sdkman, etc. these all do amazing things
- Once you learn bash, try zsh and oh-my-zsh, they have things like
  case-insensitive tab completion
- Don't make scripts that hardcode paths, make reusable command line
  scripts. Use bash as your "REPL", not R.
- git log -p helps analyze your log files in full details (make sure
  autocoloring is turned on in your terminal)
- There are keys to jump forward and backwards on the command line
  text editor, learn them...don't scroll one char at a time
- Learn how "PATH" works. Generally it is just a set of directories
  connected by ":" separators. You can add things to the path by
  saying "export PATH=$PATH:/new/directory/to/add" and you can add
  this to ~/.bashrc for example
- When your install process for a command line tool seems like
  nonsense, try homebrew instead. barring that, learn PATH, and how to
  run "make install", etc. Most of your headbashing from installing
  programs is 90% can be explained by not understanding how the
  developer is intending it to be used, 10% of the tool's install
  process being wrong
- Get a static analyzer and basic tests going on your codebase and run
  it on travis-ci. Getting started with travis-ci is kind of a
  learning curve, but it is worth it
- Use cpanm instead of cpan for package management
- Vocabulary learning curve: catalina is the same thing as tomcat.
  CATALINA_HOME is the same thing as the tomcat folder
- alias ll="ls -l", because I type "ll" hundreds of times a day.
- For irc productivity, run irssi on a server in a "screen"
  e.g. "screen irssi" and then you can come back to conversations
  later by just logging into the server with ssh
- Edit ~/.ssh/config to include your hostnames so you don't have to type out
  long ssh
  commands http://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/
- Use spaces instead of tabs in your source code (>:( yes I think
  this is the one true way)
- Try out nodejs and browserify in your spare time to make a "npm"
  based app in the browser. it's fun.
- Similarly, try making a simple "api" endpoint on the server side
  with express.js or similar. can get started very quickly.
- Learn how to get a mindset of writing tests. You can write tests
  proactively (i.e. Test driven development), but you can also write
  them "reactively" too (i.e. if have a bug that you fix, you can make
  a test to make sure this doesn't happen anymore)
- Similar to above, tests in this sense are more "sanity checks" than
  they are formal proofs. Take "assert" logic and "debugging" code out
  of main codebase and put them in tests
- Minimize comments in your code, and also don't comment out code and
  leave it present. Find a way to delete it and move on!
- When you have a bunch of .orig files after doing a git merge, just
  use git clean -f to get rid of them. Similarly, to get rid of
  everythng, including things in your gitignore file (i.e. a super
  clean) use git clean -fdx. It has a --exclude argument too

::: {#footer}
[ April 6th, 2016 4:23pm ]{#timestamp}
:::

export default ({ children }) => <Layout>{children}</Layout>
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Running nginx on containerised travis-CI pt 2]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Mon, 28 Mar 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
There are several guides out there about how to setup nginx on travis-CI
but I still found it to be a challenge, especially finding a modern one
that works with the containerized builds. I was frustrated that things
like `SimpleHTTPServer` from python and http-server from npm did not have
fully enough features to run our app either (a complex "static-site
generator" type thing you might say), and I was also too lazy to setup
"sauce labs" (which I have not used, but presume has some better ability
to run functional/browser tests).

Essentially, the problem with running nginx under the containerized
build is that it "likes to be sudo", with many logfiles by default going
to different places that only sudo has access to.

This link is probably the most similar to the technique I use here, but
it is now gone (?) and must be accessed through the internet archive!

[](http://www.doublesignal.com/running-nginx-on-containerised-travis-ci)<https://web.archive.org/web/20150919050719/http://www.doublesignal.com/running-nginx-on-containerised-travis-ci>

My technique is very similar, however I use an extra trick to set the
file root to the current directory (instead of /tmp/nowhere as in the
link) by using "envsubst" to replace variables in the nginx config file.

Without further ado, the .travis.yml can look like this

```
    sudo: false
    addons:
      apt:
        packages:
        - nginx
    install:
      - cat tests/travis.conf | envsubst > tests/travis-envsubst.conf
      - nginx -c `pwd`/tests/travis-envsubst.conf
    script:
      - wget http://localhost:9000/yourfiles
```

Then your nginx config file can look like this

```
    worker_processes 10;
    pid /tmp/nginx.pid;

    error_log /tmp/error.log;

    events {
        worker_connections 768;
    }

    http {
        client_body_temp_path /tmp/nginx_client_body;
        fastcgi_temp_path     /tmp/nginx_fastcgi_temp;
        proxy_temp_path       /tmp/nginx_proxy_temp;
        scgi_temp_path        /tmp/nginx_scgi_temp;
        uwsgi_temp_path       /tmp/nginx_uwsgi_temp;

        server {
            listen 9000 default_server;

            server_name localhost;
            location / {
                root $TRAVIS_BUILD_DIR;
                index  index.html index.htm;
            }
            error_log /tmp/error.log;
            access_log /tmp/access.log;
        }
    }

```

Then, when travis-CI is run, it uses envsubst to replace
`$TRAVIS_BUILD_DIR` in the tests/travis.conf file, and then boots up
nginx
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[On over-reproducibility]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sat, 05 Mar 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Recently, some posts were made by
<https://twitter.com/arjunrajlab> about how perhaps we are aiming at
"over-reproducibility". I think this is interesting, and would generally
agree that not everyone needs to achieve total automation of their whole
pipeline, but I think the post does a lot of "blaming your tools" and
disparaging good development practices with regards to version control
and figure generation.

I think that the complaint that version control and automated figures
are not for everyone is probably true, but it is overgeneralizing a
different problem. For example, students are not "trained" to work with
Git, and they are not "trained" to do software engineering. In fact,
even computer science students are not generally "trained" to do any of
those things (computer science != software engineering). But that
doesn't mean that your lab needs to forego using all those tools.
Software development can be incredibly complex and sophisticated, but
it's important to make sure things are "done right"! High-quality and
easy-to-reproduce software is really about process, and engineering. But
that is also why there is no one-true-way for reproducibility. Maybe
Arjun doesn't have a reproducible workflow right now, but what about 5
years down the road, where he suddenly has a great framework for such
things? This happens all the time in software development (for example,
how long ago was it that "push to deploy" did not exist? how often would
you just edit your files live on your site? now that is seen as bad
practice!), but that said, processes for software quality can evolve
pretty organically, so even though some best practices exist, people can
grow their own quality environment.

Even if we agree that software development+version control=good, there
are still a lot of complaints about it in the blogpost. For example, the
complaint that git is too hard is pretty silly, and the xkcd comic about
calling over graph theorist doesn't really help. As a software developer
at work, I think that version control simply helps define a disciplined
way of working. Version control makes you analyze your progress,
summarize it as a commit message, format the code properly, make sure it
passes tests, and then talk to your collaborators about accepting it.
Dropbox might accomplish some of those things, but I would really doubt
that it is covering that full scope. Arjun seems to agree with using
version control for some of his labs software development, so again,
there is a spectrum of needs being met. Nevertheless, there are some
weird comments about whether commit messages are like a "lab notebook",
but hint: they are not, write documentation for your project or keep a
separate journal or blog or wiki. Commit messages in my opinion should
be about one line, and the changes should be very self explanatory. But
another big argument in the blogpost is whether version control works
for something like paper writing, and I believe that this underscores
something else: that paper writing is really a pretty messy procedure.

I think that perhaps the "google docs" mode of writing is probably
pretty ok for many things, but it still needs a gatekeeper to
incorporate the comments from coauthors and reviewers into the document
in an organized way. In my experience as a "gatekeeper" with writing my
senior thesis, I organized my paper using knitr, and I automated figures
being generated by R wherever possible, and then I would convert the
paper to .docx to share with my advisors. Then I would take their
comments on the .docx and incorporate it back into my paper. This could
be seen as burdensome ("why not just use google docs"), but I felt that
it was a good way to incorporate review into a reproducible workflow.

Now, my pipeline precludes your PI from having to learn git to make a
pull request on your paper. That's a good thing... and we still have
reproducibility.  But what about the figures themselves? I said I had
knitr for reproducible figures, but what about everyone else? I think
figures have high value, and so people might want to have more
reproducibility invested in them. In the blog post, it was claimed that
making "complex" pub-quality figures was difficult (i.e. the plea for
Adobe Illustrator), but look at the annotation functions from ggplot2,
and multifaceted images. I found these annotation functions to be very
easy to pick up. There is also the on-going debate about ggplot2 vs base
graphics on the simplystatistics blog, which covers making publication
quality figures, and last I checked, I think the ggplot2′ers were
winning. I don't know how it works in high profile journals like Nature,
because it looks like they just re-do all the figures to make them have
some consistent style, but that doesn't mean your original figure should
be irreproducible.

The debate about reproducible figures is pretty tangible too in things
like microscopy images. Simply look at the large amount of discussion
from pubpeer about image fraud and possible duplications. The pubpeer
community obviously has some pretty sophisticated tools for hunting out
possibly manipulated microscopy images. These types of things also lead
to investigations, and you can see in the high-profile retraction case
over STAP cells that it looks like the investigating committee were
simply asking how some figures were made, and upon finding that lab
members don't know, a paper was retracted. The RetractionWatch blog
covers these
investigations <http://retractionwatch.com/2016/02/26/stap-stem-cell-researcher-obokata-loses-another-paper/>

You can't depend on other people to back your figure up, so you need to
take responsibility for making sure your papers and your work are
reproducible (and, there is a spectrum for reproducibility, but I
believe that version control is a great example of highly disciplined
work). I also think that just having folders on some hard drive is not a
good way to do things either. There is a saying in software development
that is "if it's not in version control, it doesn't exist". That's not
to say that version control is for everything, big data obviously has
trouble with being stored in git. But that shouldn't block you from
creating reproducible analyses.

Another example from the over-reproducibility blogpost says that if you
have "analysis1″ and "analysis2″, then version control advocates would
tell you to delete analysis1 and just remember that it is in your
history. I think that this is just a different issue. If you actually
care about both analyses, just make them separate repositories, with
basic README.md files explaining each them, and stop worrying about it.
Having one repository containing too many miscellaneous scripts is
actually an anti-pattern. Stop making repositories
called "bioinfo-scripts" that just contain a mish-mash of analysis
scripts! Make your work purpose driven and do tasks. Also, this is an
argument against REPL tools: your R REPL history is not a reproducible
script. Make your code into a script that generates well defined
outputs. Windows users: you might not understand this because the
command line on windows is crippled, but you have to make things run on
the command line.

Now I wish I could say that I live by my words, but having been involved
in coauthoring several papers, I will just have to admit that it is
really a messy procedure despite my best intentions as an editor and
coauthor. I wish things would be better!

On over-reproducibility: there is no such thing! There are pretty good
arguments to really automate most of a process, especially if it is done
repeatedly, to remove human errors, because meat-machines genuinely do
things wrong all the time.

And, as my parents would say around the dinner table: "you can always
have more, but you can never have less"...so, you're not going to get to
a point of over-reproducibility. We shouldn't cargo cult it as the only
way to do science but it's not a bad thing to have.
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Cheating in your computer science class by copying from stackoverflow]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Thu, 17 Dec 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
I would like to tell a story about how I provided some personal tutoring
help for a friend in a computer science class, and talk about a nagging
feeling that really felt wrong for me.

So, a long time ago, in a land far far away, a friend took an
intermediate class on C++. I was first updated on his progress when he
emailed me to get some help with some compiler errors. I was happy to
help the young padawan. Here was the error:

```
         test.cpp:42:43: error: non-ASCII characters are not allowed outside of literals
                and identifiers
              for (startScan = 0; startScan < (size − 1); startScan++)
                                                    ^~
```

Now, what does this say to you? For me, it was actually very clear what
the error meant. It simply meant that this code was taken from
somewhere, and copied and pasted into the compiler. I know that because
if they had typed it themself, they definitely would not get this error,
because it is the error that implies something was automatically
converted to a unicode dash, mostly something done during copying and
pasting. At this point, I just kind of laughed, and helped him fix that.
I showed how the compiler is actually pretty smart and can help fix
these errors and then I said "l8r dude".

The next week, I had another skype meeting with him, and this time I
wanted to help a little more. It was pretty clear when we started that
he was using code that was copied and pasted again. I said, "uh,
ok,....I'm not sure we need that now, but let's just keep going", and
then I sat down and started helping. I wanted to help get all the
details of the program working, so I helped guide the solution. Each
time we needed to test the program, it required repeating some input
lines via `cin >>`, which is really annoying (obviously, you should test
your code with unit tests, but universities don't teach that, a rant for
another day). Anyways, it took awhile, because coding really does just
take time, but in the end he finally got it fixed and I said great job,
and he turned it in!

Now, on my friends last assignment, I got another call for help, and
when we started skype, I found yet again that he had copied code from
somewhere, which included a C++ class and a main function for doing
binary trees. I just simply said "dude, delete that, we don't need it"
and so he deleted it, but I think maybe he had worked on this copied
code for awhile, and maybe felt it was kind of his, so was apprehensive.
I insisted though. Then we walked through the assignment again, very
slowly. I spent probably 2-3 hours helping him out that night. During
those hours, I saw him continually making many programming mistakes such
as just not knowing how to declare variable or a function properly, or
just not knowing what to do next. This was kind of frustrating!!! But I
wanted to absolutely teach him how to make it right! I was patient
though, and I wanted to teach a fun lesson, so I showed how you can do
some "unit tests" which avoids having to constantly re-enter your data
via `cin >>`....

Now, the padawan completed his C++ class, and then we all were happy
ever after....but a disturbance in the force was sensed...
![image](http://zelcs.com/wp-content/uploads/2013/02/stackoverflow-logo-dumpster.jpg)
Image from <http://zelcs.com/this-is-why-stackoverflow-sucks/>

I was reminded about all this due to seeing that [StackOverflow is now
changing their "license" over all the little snippets of
code](http://meta.stackexchange.com/questions/271080/the-mit-license-clarity-on-using-code-on-stack-overflow-and-on-the-stack-excha)
that are posted on their site. It just makes me reflect on literally HOW
OFTEN PEOPLE JUST COPY AND PASTE FROM THERE. They might understand what
they are doing, or they seriously might not!!! I think it is a real
problem that people sometimes do not understand, but I cannot deny that
it can be helpful too.

If I reflect on education in general, I recall when I took a University
level physics class... it was really hard! We had to enter our validated
solutions for the math problems into a computerized website homework
portal, and that involved being 100% correct about things. Now, what if
there was just a physicsoverflow, where they not only had Q&A, but they
had "programs" that gave you all the right answers to your homework
problems that you could just copy and paste and use as solutions to your
homework? This isn't even in the realm of asking for "homework help"
anymore, this is just pure cheating if you can copy your answers from
somewhere. It is disappointing though because this is what people are
doing in computer science!! These students are missing out on basic
understanding of code. !!thisIsNotOk();

Now, at least when I was being a tutor for my friend, I felt like my
advice helped my friend learn some things, not just give answers. But
what if I was not there? I guess there is a certain "impersonal quality"
that makes asking Google/StackOverflow for answers less like
conventional "cheating", but that is still wrong. I think it would be
good if more expert knowledge was available for all people, and not just
copy and paste snippets. As a start, I thought that [this post by Philip
Compeau and Pavel
Pevzner](http://cacm.acm.org/magazines/2015/10/192385-life-after-moocs/fulltext)
(who teach a Bioinformatics Algorithms MOOC on Coursera) was very
interesting, and I really liked their quote:

"Online education should move toward replicating the experience of
receiving one-on-one tutoring."

That sounds great, but how can this be acheived? And how can it be done
right? I think it really requires the student to "learn how to learn"

If I think back to a long time ago, I remember being in 4th or 5th grade
and I did a book report on World War 1, and I went to the library. I
remember desperately flipping through pages of a 100 page book to try to
find some snippets of information to support some basic idea that I
wanted to talk about. Maybe I wanted to know something specific, but the
problem was that I wasn't REALLY READING THE BOOK! I probably could have
had a better understanding of the topic if I had just read it, or even a
part of it, and asked for help, but instead I just picked and chose
snippets from the book to "sound smart". I am very guilty of this type
of error in many instances throughout my school career, so I am no
saint! I even have a phrase to describe this style of learning...I call
it "predatory learning" and it is probably the worst kind of learning
style. Predatory learners often pick and choose from scraps of info, but
they never get a full meal!

::: {#footer}
[ December 17th, 2015 3:05am ]{#timestamp} [learning]{.tag}
[education]{.tag} [computer science]{.tag} [stackoverflow]{.tag}
[fail]{.tag}
:::
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Killing postgres the hard way]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Thu, 22 Oct 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
So today, I finally decided to do something about a query that we saw
had been running for 25 DAYS on our server

Note: If you find this post and you need to follow the hard way, backup
your data first if possible.

First I could obviously see the culprit: each postgress query runs it's
own process so I could see in "htop" that there was this process that
had been running for 600 hours, or about 25 days

Next, I opened a psql console and ran this query:

```
> SELECT datname,procpid,current_query FROM pg_stat_activity WHERE
> datname='database_name' ORDER BY procpid ;
```

This returns which actual queries are being run on the database at any
given time.  I could easily see the one problematic query being run,
which was a badly constructed intermine template query that resulted in
a weird "recursion" essentially.

I wanted to try just terminating this query itself, so I ran this

```
> SELECT pg_cancel_backend(29033);
```

Each time I ran it, it would say it returned one result but it did
nothing.

I also read that you can try to nicely "kill" it from the command line
(no kill -9) so I ran

```
> kill 29033
```

This also did not work!

I thought perhaps all these problems were because tomcat was still
active, so we shut down tomcat, and retried killing the specific query,
but to no avail

At this point, I just wanted to restart the whole database server. Kind
of a risky move... but I am sort of a risky kind of guy...(that is not a
good thing with databases). If you are doing this, make backups! I
didn't. Luckily I suffered no data loss but what follows is kind of
intense.

So first, I try and stop the database service

```
> service postgresql-9.1 stop
```

Unfortunately, this `[FAILED]` ! And of course, even though it failed,
the database is now unusable. No logging into it anymore, we have to go
with the hard way now...

Looking at /etc/init.d/postgres-9.1 told me that the service stop
command was effectively using something like this:

```
> pg_ctl -D /db/postgres/data -m fast stop
```

After some reading, I learned that you can try using a slightly
different flag to restart it

```
> pg_ctl -D /db/postgres/data -m immediate  stop
```

I ran this and to my horror/surprise, it actually worked! At this point
I decided to start postgresql back up again!

```
> service postgresql-9.1 start
```

The service start quickly returned a SUCCESS, which was great, but then
I tried to start a psql console and the console froze on me! I could not
even ctrl+c it!

I got really worried at this point and I looked at the process manager,
and saw that there was one postmaster process running but it was not
clear what it was doing. I actually tried to shutdown the server again
in a panic mode but at this point it said

```
> /usr/pgsql-9.1/bin/pg_ctl stop -D /db/postgres/data/ -m immediate
> waiting for server to shut
> down...............................................................
> failed
```

It was probably good that it didn't shut down, because I would quickly
find out that it was in recovery mode.  I looked at the postgresql logs
and I saw this, reproduced here for full detail (from before the
shutdown to the restart)

```
> WARNING:  pgstat wait timeout
> WARNING:  pgstat wait timeout
> WARNING:  pgstat wait timeout
> WARNING:  pgstat wait timeout
> WARNING:  pgstat wait timeout
> WARNING:  pgstat wait timeout
> WARNING:  pgstat wait timeout
> WARNING:  pgstat wait timeout
>
> ERROR:  canceling statement due to user request
> STATEMENT:  CREATE TABLE precomp_90519 AS SELECT DISTINCT a1_.id AS
> a1_id, a2_.id AS a2_id, a3_.id AS a3_id, a4_.id AS a4_id,
> a5_.id AS a5_id, a6_.id AS a6_id, a12_.id AS a12_id, a10_.id AS
> a10_id, a1_.id AS a13_, a1_.primaryIdentifier AS a14_,
> a1_.secondaryIdentifier AS a15_, a2_.type AS a16_, a3_.name AS
> a17_, a4_.primaryIdentifier AS a18_, a5_.primaryIdentifier AS
> a19_, a6_.shortName AS a20_, a12_.identifier AS a21_, a10_.code
> AS a22_ FROM Gene AS a1_, Homologue AS a2_, Organism AS a3_, Gene
> AS a4_, Gene AS a5_, Organism AS a6_, GOAnnotation AS a7_,
> GOEvidence AS a8_, OntologyTerm AS a9_, GOEvidenceCode AS a10_,
> OntologyAnnotation AS a11_, OntologyTerm AS a12_, GeneGoAnnotation
> AS indirect0, EvidenceGOAnnotation AS indirect1 WHERE a1_.id =
> a2_.geneId AND a1_.organismId = a3_.id AND a2_.geneId = a4_.id
> AND a2_.homologueId = a5_.id AND a5_.organismId = a6_.id AND
> a1_.id = indirect0.Gene AND indirect0.GoAnnotation = a7_.id AND
> a7_.id = indirect1.GOAnnotation AND indirect1.Evidence = a8_.id AND
> a7_.ontologyTermId = a9_.id AND a8_.codeId = a10_.id AND a9_.id =
> a11_.ontologyTermId AND a11_.ontologyTermId = a12_.id ORDER BY
> a1_.primaryIdentifier, a1_.secondaryIdentifier, a2_.type,
> a3_.name, a4_.primaryIdentifier, a5_.primaryIdentifier,
> a6_.shortName, a12_.identifier, a10_.code, a1_.id, a2_.id,
> a3_.id, a4_.id, a5_.id, a6_.id, a12_.id, a10_.id
> LOG:  could not send data to client: Broken pipe
> STATEMENT:  CREATE TABLE precomp_90519 AS SELECT DISTINCT a1_.id AS
> a1_id, a2_.id AS a2_id, a3_.id AS a3_id, a4_.id AS a4_id,
> a5_.id AS a5_id, a6_.id AS a6_id, a12_.id AS a12_id, a10_.id AS
> a10_id, a1_.id AS a13_, a1_.primaryIdentifier AS a14_,
> a1_.secondaryIdentifier AS a15_, a2_.type AS a16_, a3_.name AS
> a17_, a4_.primaryIdentifier AS a18_, a5_.primaryIdentifier AS
> a19_, a6_.shortName AS a20_, a12_.identifier AS a21_, a10_.code
> AS a22_ FROM Gene AS a1_, Homologue AS a2_, Organism AS a3_, Gene
> AS a4_, Gene AS a5_, Organism AS a6_, GOAnnotation AS a7_,
> GOEvidence AS a8_, OntologyTerm AS a9_, GOEvidenceCode AS a10_,
> OntologyAnnotation AS a11_, OntologyTerm AS a12_, GeneGoAnnotation
> AS indirect0, EvidenceGOAnnotation AS indirect1 WHERE a1_.id =
> a2_.geneId AND a1_.organismId = a3_.id AND a2_.geneId = a4_.id
> AND a2_.homologueId = a5_.id AND a5_.organismId = a6_.id AND
> a1_.id = indirect0.Gene AND indirect0.GoAnnotation = a7_.id AND
> a7_.id = indirect1.GOAnnotation AND indirect1.Evidence = a8_.id AND
> a7_.ontologyTermId = a9_.id AND a8_.codeId = a10_.id AND a9_.id =
> a11_.ontologyTermId AND a11_.ontologyTermId = a12_.id ORDER BY
> a1_.primaryIdentifier, a1_.secondaryIdentifier, a2_.type,
> a3_.name, a4_.primaryIdentifier, a5_.primaryIdentifier,
> a6_.shortName, a12_.identifier, a10_.code, a1_.id, a2_.id,
> a3_.id, a4_.id, a5_.id, a6_.id, a12_.id, a10_.id
> LOG:  unexpected EOF on client connection
> LOG:  unexpected EOF on client connection
> LOG:  unexpected EOF on client connection
> LOG:  unexpected EOF on client connection
> LOG:  received fast shutdown request
> LOG:  aborting any active transactions
> LOG:  autovacuum launcher shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> FATAL:  the database system is shutting down
> LOG:  received immediate shutdown request
> WARNING:  terminating connection because of crash of another server
> process
> DETAIL:  The postmaster has commanded this server process to roll back
> the current transaction and exit, because another server process
> exited abnormally and possibly corrupted shared memory.
> HINT:  In a moment you should be able to reconnect to the database and
> repeat your command.
> LOG:  received fast shutdown request
> LOG:  database system was interrupted; last known up at 2015-10-22
> 15:47:43 CDT
> LOG:  received immediate shutdown request
> LOG:  database system was interrupted; last known up at 2015-10-22
> 15:47:43 CDT
> LOG:  database system was not properly shut down; automatic recovery
> in progress
> LOG:  record with zero length at BBD/1CC2F0C0
> ...
```

You can see all the weird activity that was done here

- first the attempt to "canceling statement due to user request" did not work
- then the database stop using -m fast
- then the database stop using -m immediate
- the restart (with the HINT, should be ready soon)
- the panic mode where i tried to shut it again anyways

During the recovery period, I was still very concerned about the
database was doing, so I used "strace" to look at the main postmaster
process.

I was pleasantly surprised to see that the postmaster process was just
cleaning up files in /db/postgres/data/base/pgsql_tmp/, I could see the
file system "unlink" command with successful status codes.

There were about 150 large files in /db/postgres/data/base/pgsql_tmp/,
and I waited about an hour for them to be deleted, and after that, the
postgresql log file said it was ready, and indeed, it was perfect :)

```
> LOG:  redo is not required
> LOG:  database system is ready to accept connections
> LOG:  autovacuum launcher started
```

What a relief!

I hope this might help any wayward stragglers to see how the postgresql
restart process works. Sometimes things don't shut down cleanly, but I
think it is still good to know some alternative steps to kill -9

::: {#footer}
[ October 22nd, 2015 7:29pm ]{#timestamp} [postgresql]{.tag} [dba]{.tag}
[databases]{.tag} [sql]{.tag} [troubleshooting]{.tag}
:::
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Tomcat memory debugging]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Thu, 15 Oct 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
In my previous posts, I speculated about the issues that were causing
CPU usage spiking with
tomcat: <http://searchvoidstar.tumblr.com/post/129241954103/fixing-spiky-cpu-issues-with-tomcat>

Unfortunately, I was completely wrong in my speculations, but we
increased tomcat memory limits so that the entire Lucene search index
could fit in memory, which was able to fix the spiky CPU problems.

Luckily, fixing the memory issues had very good implications for our
webapp:

I have a cron job uses a simple curl command to grab different pages on
the website, and then it logs the time taken to a output file. I charted
these output times, before and after we increased the memory limits of
tomcat, and it turned out that the response time of the webapp was
dramatically improved by this change.

[](/media/131229569383_0.png)

![](/media/131229569383_0.png)

Figure 1. The webapp response time was extremely variable before the
redeploy on Oct 2nd where we increased tomcat's memory allocation, which
thereafter dramatically improved the response time.

Clearly, the webapp response time was being severely compromised by the
memory issues.

In response to all of these issues, I also added GC logging to the
tomcat configuration so that I can see if the GC is correlated with
these webapp response time. Figure 2 shows how high GC activity is
correlated with longer webapp response times, but note that this figure
was made after the other memory allocation problems were fixed, so it is
still much better than the problems we had in the past.

[](/media/131229569383_1.png)

![](/media/131229569383_1.png)

Figure 2. After increasing the memory, you can see webapp response time
is much better, except if the GC activity becomes very high, and then
this increases the response time.

Edit: Bonus screenshot, seemingly each friday we get a majoy activity
burst that triggers GC activity!

![](/media/131229569383_2.png)

Figure 3. Crazy Java GC activity on a friday night, but the app seems to
recover from it

Conclusion

Increasing the memory allocation to java and tomcat allows the entire
system to perform much better. If you can afford to get more memory to
allocate to tomcat, then it's probably a good idea.

Also, tracking your webapp response times will help you see if your
changes are having a good effect. I made this a script for graphing log
outputs here <https://github.com/cmdcolin/loggraph>

PS:

If your tomcat is running as the tomcat user, then it can be difficult
to debug the memory problems simply with the "get heap dump" from
jvisualvm, because the permissions will be wrong. To fix this, try using
a privileged user to run the jmap command:

runuser -l tomcat -c "/usr/java/latest/bin/jmap
-dump:format=b,file=/db/tomcat/tomcat.dump 25543"

::: {#footer}
[ October 15th, 2015 1:31pm ]{#timestamp}
:::

export default ({ children }) => <Layout>{children}</Layout>
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Weekend project - graphing tumblr reblogs using cytoscape.js]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sun, 30 Aug 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
In the past, I made an app that used RStudio's Shiny platform to plot
network graphs with RGraphviz. This worked, and gave some nice results,
but when I found out about cytoscape.js, I really wanted to try that
out.

The app is designed to plot tumblr reblogs, so it has a tree structure,
but simply plotting things as a tree is not very space efficient (as in,
the visualization takes up too much space). Therefore, using different
types of layouts can really help.

In my first app with graphviz
<https://colindiesh.shinyapps.io/tumblrgraph>, there are several
built-in graph layouts including "neato" "twopi", "circo", and "dot"

I made all of these available for users to try in the Shiny app. The
names of the layouts don't lend much to their behavior, but they are
built-in functions in Graphviz. There are both "tree" and
"force-directed" style graph views. As I mentioned, the "tree" style
view make a lot of sense for the tumblr reblogs, but the force directed
graphs are also a lot more compact, so offering both styles is useful.

![](../../media/128000908903_0.png)

Figure 1. My default example graph from graphviz using the twopi layout.

I wanted to replicate all the features that I had in the Graphviz app in
Cytoscape.js. Here is the breakdown of the basic components that needed
replicating:

1. Build the "graph" representation of reblogs in memory

2. Add user forms and configurability

3. Add color for distance from root using a breadth first search

4. Draw the graph

As I went along, I was happy to learn that the concepts mapped very
easily to javascript and cytoscape.js. The implementations are a little
different, but it worked out very nicely.

![](../../media/128000908903_1.png)

Figure 2. Same data plotted in Cytoscape.js with the springy layout.

In the new app, we enabled several different layouts similar to the
Graphviz app too. In cytoscape.js, the layouts that are offered
include "arbor", "springy", "cola", "cose", and "dagre". I like "cola"
because it really looks like bubbles moving around in a soda. Others are
worth experimenting with too.

![](../../media/128000908903_2.png)

Figure 3. A Cytoscape.js springy layout for a larger tumblr reblog graph

The new cytoscape.js app also has a nice animation feature. The old
graphviz app offered animation too (using Yihui's animation library for
R) but the new version can automatically encode HTML5 video on the
client side from individual picture frames in the browser using
["Whammy"](https://github.com/antimatter15/whammy)! This quite
impressive!

So to animate the graph, what is done is

1. Add nodes/edges and layout the graph (the simulation time is
   configurable, because allowing the user to interact with the graph while
   the simulation is running is useful)

2. Once layout is complete, the user can save the graph as an
   animation, which first hides all nodes by adding visibility: hidden to
   the CSS.

3. Then the nodes are re-shown one-by-one, preserving the layout, and a
   frame is saved by the renderer at each step (takes a snapshot of the
   canvas).

This strategy for the animation is actually better than the original
graphviz version that I had because the layout is only done once, which
is time saving and it is also more consistent (the layout changes a lot
if you re run it on different sets of nodes).

Check out the app here <http://cmdcolin.github.io/tumblrgraph2/>

Future goals:

- Test out super large graphs (I have tested up to about 500 reblogs
  but after this, around 1000 reblogs, it slows down a lot and produces
  bad layouts. Needs fixing)
- Test out ability to place importance on certain nodes by increasing
  node size based on it's degree

Check out an example of the HTML5 video here

::: {#footer}
[ August 30th, 2015 11:49pm ]{#timestamp} [cytoscapejs]{.tag}
[cytoscape]{.tag} [javascript]{.tag}
:::

export default ({ children }) => <Layout>{children}</Layout>
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Creating high-resolution screenshots (of jbrowse) with phantomJS]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Mon, 02 Mar 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Generating screenshots that are of high quality can be a great benefit
for things like science publications. PhantomJS is great for automating
this in a reproducible way. While many HTML pages can be rendered in
high resolution without modification, HTML5 canvas apps need special
considerations (see this [previous post on the
topic](http://searchvoidstar.tumblr.com/post/86542847038/high-dpi-rendering-on-html5-canvas-some-problems)).

One of the key things that we noticed when we developed the high
resolution canvas rendering (see above link) is that the
"devicePixelRatio" can increase based on the browser's zoom level, and
it can also take fractional values. This was a difficult problem, to
make rendering 100% consistent under all devicePixelRatio values, so we
created a config parameter called highResolutionMode to accept arbitrary
resolutions.

Later, we learned about PhantomJS and how it can be used for creating
screenshots, it was clear that our design for the settings arbitrary
scaling factors for the HTML5 canvas was very helpful, as we can set
highResolutionMode=4 along with the phantomJS variable
page.zoomFactor=4, which matches the resolutions and creates high-res
canvas screenshots.

One of the reasons that this is important is that it doesn't look like
PhantomJS allows "devicePixelRatio" to be emulated, so the
page.zoomFactor doesn't necessarily set the devicePixelRatio to a higher
number, so being able to set the the arbitrary high resolution canvas
scalings ourselves is a good solution. Reference: issue open Jan 2013
<https://github.com/ariya/phantomjs/issues/10964> and we are now in Aug
2015

Here are some examples of the rendering process.

## Examples

1.  Rendering screenshots to PNG

    phantomjs rasterize.js
    "<http://localhost/jbrowse/?data=sample_data/json/volvox&tracklist=0>"
    output.png "3800px\*1600px" 2

    [![](http://i.imgur.com/ABLo6WJ.png)](http://i.imgur.com/ABLo6WJ.png)

    Figure 1. A basic image output from phantomJS. It uses a
    zoomFactor=2 on the command line to match highResolutionMode=2 in
    the config file. \`

2.  Rendering screenshots to PDF. In JBrowse, this requires PhantomJS
    2.0. Also see footnote.

    phantomjs rasterize.js
    "http://localhost/jbrowse/?data=sample_data/json/volvox&tracklist=0"
    output.pdf "16in\*8in"

    [Dropbox PDF
    906kb](https://www.dropbox.com/s/7pceo4o406dys8s/output.pdf?dl=0)

    Figure 2. Outputted PDF from phantomJS. This still requires setting
    the configuration such as highResolutionMode=2 too

    ## Conclusion

    In the future, we want to consider adding highResolutionMode to be
    specified via the URL so that it doesn't need to be changed
    manually, although, setting highResolutionMode=2 by default is not a
    bad strategy.

    **Footnote**

    I used the following patch for rasterize.js to help "fill out" the
    page space in PDF renderings (otherwise, it is a square page, not
    super pretty for a widescreen app). I guess rasterize.js is really
    just a template and not meant to be super multi-purposed, so this
    custom modification helps for our case.

```{=html}
<!-- -->
```

        diff --git a/examples/rasterize.js b/examples/rasterize.js
        index b0e0f67..3b0b6e4 100644
        --- a/examples/rasterize.js
        +++ b/examples/rasterize.js
        _@@ -14,6 +14,7 @@ if (system.args.length < 3 || system.args.length > 5) {
            page.viewportSize = { width: 600, height: 600 };
            if (system.args.length > 3 && system.args[2].substr(-4) === ".pdf") {
                size = system.args[3].split('_');

        +       page.viewportSize.width *= parseInt(size[0])/parseInt(size[1]);
                page.paperSize = size.length === 2 ? { width: size[0], height: size[1], margin: '0px' }

**Reference**

gmod.org/wiki/JBrowse_Configuration_Guide\#Rendering_high_resolution_screenshots_using_PhantomJS

**Comparison**

![image](/media/112494997473_0.png)

Big improvement on font rendering

::: {#footer}
[ March 2nd, 2015 1:52am ]{#timestamp} [javascript]{.tag}
[phantomjs]{.tag} [html5]{.tag} [canvas]{.tag}
:::

export default ({ children }) => <Layout>{children}</Layout>
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[Post graduation survey]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Sun, 01 Feb 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
I recently received some post-graduation survey results from my class of
2013 about salaries, job satisfaction, and other things. I thought I'd
try to visualize the data using R and ggplot2 as an exercise.

[](http://i.imgur.com/5rVnQHC.png)

![](/media/109823235838_0.png)

Figure 1. The fancy ggplot2 graph of salaries with standard deviation
bars comparing salaries of BS/MS grads (red) with BS grads (blue).

As a CS grad, I suppose I'm happy to see that we have the a highest
average salary right out of the gate. CS also has a high standard
deviation which I thought was interesting. Perhaps CS majors work in a
myriad of fields that demand computational skills where other
engineering majors may be more focused on certain types of fields,
giving less deviation.

In the process of making this graph, I was looking for how to do the
side-by-side bar charts in ggplot and ended up supplying a "correction"
to a answer on crossvalidated, a stackexchange site. The correction
entailed how the syntax for using reshape2 vs reshape has changed
slightly, so hopefully that helps other people searching for the same
issue.

Here is the code for processing

```R
 library(xlsx)
 library(ggplot2)
 library(reshape2)

 salaries=read.xlsx("workbook.xlsx",1)
 df=melt(salaries,measure.vars = c("BS.MS.annual.salary",
 "BS.annual.salary"))
 #awkward step to merge standard deviations
 df[df$variable=="BS.MS.annual.salary","stdev"]=df[df$variable=="BS.MS.annual.salary","stdev.1"]
 ggplot(df, aes(NA., value, fill=variable)) +
      geom_bar(position="dodge",stat="identity") +
      geom_errorbar(aes(ymin=value-stdev, ymax=value+stdev),
 position=position_dodge(width=0.9)) +
      ggtitle("Salary for 2013 class of Engineering (2014 survey)") +
      xlab("Major") +
      ylab("Salary w/ stddev")
```

Table pictured

![](/media/109823235838_1.png)

::: {#footer}
[ February 1st, 2015 7:05pm ]{#timestamp} [rstats]{.tag} [ggplot2]{.tag}
[college]{.tag} [salary]{.tag}
:::

export default ({ children }) => <Layout>{children}</Layout>
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
        <item>
            <title><![CDATA[High DPI rendering on HTML5 canvas - some problems and solutions]]></title>
            <link>https://cmdcolin.github.io/undefined</link>
            <guid>https://cmdcolin.github.io/undefined</guid>
            <pubDate>Thu, 22 May 2014 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
Recently our code has been moving towards the use of HTML5 canvas, as it has
many benefits. I felt that if we were going to keep this going towards canvas,
the rendering needed to match the quality of regular HTML based tracks.
Unfortunately, the HTML5 canvas by default looks very "fuzzy" on a high
resolution display (Figure 1).

![](/media/86542847038_0.jpg)

_Figure 1._ An example of really bad font rendering before and after enabling
high resolution on the HTML5 canvas.

**Background **

Major credit goes to the tutorial at
<http://www.html5rocks.com/en/tutorials/canvas/hidpi/> for pioneering this!
 The html5rocks tutorial, written in 2010 it still remains relevant. The major
thing it introduces is these browser variables called devicePixelRatio and
backingStoreRatio that can be used to adjust your canvas drawing. In my
interpretation, these two variables have the following purpose:

_devicePixelRatio_

On high DPI displays, screen pixels are actually abstracted away from the
physical pixels, so, when you create some HTML element with width 100, height
100, that element actually takes up a larger number of pixels than 100x100. The
actual ratio of the pixels that it takes up is 100*devicePixelRatio x
100*devicePixelRatio. On a high DPI platform like Retina, the devicePixelRatio
is normally 2 at 100% zoom.

_backingStoreRatio_

The backing store ratio doesn't seem to change as much from platform to
platform, but my interpretation of this value is that it essentially gives the
size of the memory buffer for the canvas. On my platform, the backingStoreRatio
is "1". I think this value had more historical use, but it may not really be
used anymore (update aug 7th, 2015 deprecated?
<http://stackoverflow.com/questions/24332639/why-context2d-backingstorepixelratio-deprecated>)

So, what are the consequences of the backing store ratio and the device pixel
ratio? If the backing store ratio equals the device pixel ratio, then no
scaling takes place, but what we often see is that they are not equal, so the
image is up-scaled from the backing store to the screen, and then it is
stretched and blurred.

**So, how do you enable the high DPI mode?**

The solution to properly scale your HTML5 canvas content involves a couple of
steps that are described in the tutorial here
<http://www.html5rocks.com/en/tutorials/canvas/hidpi/>, but here is the
essence:

1. Use the canvas.scale method, which tells the canvas's drawing area to become
   bigger, but keeps drawing operations consistent.

2. The scaling factor for the canvas.scale method is
   devicePixelRatio/backingStoreRatio. This will be 2 for instance on a Retina
   screen at a typical 100% zoom level. The zoom level is relevant which will be
   discussed later in this post...

3. Multiply the width and height attributes of the canvas by
   devicePixelRatio/backingStoreRatio, so that the "canvas object" is as big as
   the scaled size.

4. Here's the tricky part: set the CSS width and height attributes to be the
   UNSCALED size that you want.

Note: you can also set CSS width:100% or something and then the canvas will be
sized appropriately. Normally though, what you will have is something like
`<canvas width=640 height=480 style="width:320px;height:240px">` so you can see
that the canvas size is larger than what the CSS actually resizes it to be.

**Issues: Browser zoom and fractional devicePixelRatios **

When I first started this project, the benefit of this high resolution
rendering seemed limited to the fancy people who had Retina or other High DPI
screens. However, what I didn't even realize is that the devicePixelRatio value
changes depending on browser zoom settings, so even people with a regular
screen can have improved rendering of the HTML5 canvas. (Update: we even saw
that if you have customized canvas renderings, then you an generate good
screenshots of the canvas with PhantomJS too. See [my other more recent
article](http://searchvoidstar.tumblr.com/post/112494997473/creating-high-resolution-screenshots-of-jbrowse))

The issue with these zoom settings though is that when you change the zoom
level, especially on chrome and firefox browsers, the devicePixelRatio can end
up being a fractional value e.g. 2.223277 which can result in sub-pixel
rendering problems.

Remember that when we scaled the canvas, it also scales the drawing functions
to be consistent, so that essentially if you draw a 1 pixel width line on a
scaled canvas, it might draw a 2.223277 pixel width line. Hence, we can get
fuzzy rendering issues.

This issue is very noticeable if you draw many 1px wide lines right next to
each other. In this case, there will be noticeable gaps between the lines due
to the imperfect rendering (see green box below).

[](http://i.imgur.com/THsfjX4.png)

[](http://i.imgur.com/THsfjX4.png)

[](http://i.imgur.com/THsfjX4.png)

![](/media/86542847038_1.png)

_Figure 2._ Examples of 1px wide lines rendered next to each other when there
is fractional devicePixelRatio.

Bottom Green box: 1px wide lines drawn 1px apart. (note: bad rendering! tiny
gaps)  Middle Blue box: 1px wide line rendered every 2 px (intentional gaps for
demonstration).  Top Red box: 1.3px wide lines (a fudge factor is used to make
eliminate the tiny gaps).

**My solution: The Red Box -- add a fudge factor **

As you can see in the above figure, my solution to the sub-pixel rendering is
to add a "fudge factor" to the line width to make it render lines that are
1.3px wide instead of 1px wide when the devicePixelRatio is not a whole number,
which effectively eliminates any gaps due to the sub-pixel rendering problem.

I heuristically determined the value 1.3px to be sufficient, as testing values
like 1.1px, 1.2px and even 1.25px were too small. I'd love to see a proof of
determining this value empirically, or even better, something that isn't this
big of a hack, but for now that's what I have.

You can see the effect of the fudge factor (red box) vs the bad rendering
(green box) in Figure 2. You can also try this out yourself here
<http://jsfiddle.net/4xe4d/>, just zoom your browser and then refresh (zooming
and not refreshing doesn't modify device pixel ratio) to test out different
values of devicePixelRatio.

**Conclusion**

In conclusion...we now have high resolution rendering on canvas! The solution
for drawing lots of lines right next to each other is sort of suboptimal, so
the question continues...what shall be done in this case?

Maybe someone could implement some sort of library that replaces the
canvas.scale method to do better layout and obtain more pixel perfect
rendering. Alternatively, you could force the scaling factor to always round to
a whole number. This is actually not a bad solution, because the canvas is
already being resized, and then you can control your rendering better.

Thanks for reading

::: {#footer} [ May 22nd, 2014 7:03pm ]{#timestamp} [html5]{.tag}
[canvas]{.tag} [javascript]{.tag} :::
]]></content:encoded>
            <author>colin.diesh@gmail.com (Colin Diesh)</author>
        </item>
    </channel>
</rss>