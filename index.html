<!DOCTYPE html><html lang="en"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="shortcut icon" href="favicon.ico"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><meta name="description" content="Blogging for the future"/><meta name="next-head-count" content="5"/><link rel="preload" href="/_next/static/css/895e0128ed383e47.css" as="style"/><link rel="stylesheet" href="/_next/static/css/895e0128ed383e47.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-514908bffb652963.js" defer=""></script><script src="/_next/static/chunks/framework-91d7f78b5b4003c8.js" defer=""></script><script src="/_next/static/chunks/main-7c3bf82eed00c281.js" defer=""></script><script src="/_next/static/chunks/pages/_app-76cd57bf65f05d70.js" defer=""></script><script src="/_next/static/chunks/162-afcc9fe4184ebd89.js" defer=""></script><script src="/_next/static/chunks/pages/index-61b300c6af645418.js" defer=""></script><script src="/_next/static/OTCGaf-RlOFSHOyCKH0Gd/_buildManifest.js" defer=""></script><script src="/_next/static/OTCGaf-RlOFSHOyCKH0Gd/_ssgManifest.js" defer=""></script><script src="/_next/static/OTCGaf-RlOFSHOyCKH0Gd/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="min-h-screen"><main><div class="container mx-auto px-5"><div class="text-2xl md:text-4xl font-bold py-14 border-b border-accent-2 flex flex-col lg:flex-row items-center"><a class="hover:underline" href="/">Misc scribbles</a></div><h1 class="text-2xl md:text-2xl font-bold"> Colin Diesh</h1><p><img src="/me.jpg" alt=""/></p>
<p>I&#x27;m a web developer and bioinformatician working on JBrowse. Interested in all
sorts of creative hacking, performance. engineering, and data science. Also
crafting, DIY and nature.</p><section><div class="py-14"><h1>Posts</h1><div><div><a class="hover:underline" href="/posts/2021-10-30-spooky">2021-10-30<!-- --> - <!-- -->A spooky error when you have a string bigger than 512MB in Chrome</a></div><div><a class="hover:underline" href="/posts/2021-10-05-jest">2021-10-05<!-- --> - <!-- -->Jest parallelization, globals, mocks, and squawkless tests</a></div><div><a class="hover:underline" href="/posts/2021-09-05-typescript">2021-09-05<!-- --> - <!-- -->Decrease your idle CPU usage when developing typescript apps with this one weird environment variable</a></div><div><a class="hover:underline" href="/posts/2021-08-15-map-limit">2021-08-15<!-- --> - <!-- -->An amazing error message if you put more than 2^24 items in a JS Map object</a></div><div><a class="hover:underline" href="/posts/2021-07-27-npm-dependencies">2021-07-27<!-- --> - <!-- -->Do you understand your NPM dependencies?</a></div><div><a class="hover:underline" href="/posts/2020-12-26-pt2">2020-12-26<!-- --> - <!-- -->Making a HTTPS accessible S3 powered static site with CloudFront+route 53</a></div><div><a class="hover:underline" href="/posts/2020-12-26">2020-12-26<!-- --> - <!-- -->Making a serverless website for photo and video upload pt. 2</a></div><div><a class="hover:underline" href="/posts/2020-12-24">2020-12-24<!-- --> - <!-- -->Making a serverless website for photo upload pt. 1</a></div></div></div></section><a class="hover:underline" href="/archive">More posts...</a></div></main></div><footer class="bg-accent-1 border-t border-accent-2"><div class="container mx-auto px-5"><div class="py-14 flex flex-col lg:flex-row items-center"><div class="m-4"><a class="hover:underline" href="/">Home</a></div><div class="m-4"><a class="hover:underline" href="https://github.com/cmdcolin">Github</a></div><div class="m-4"><a class="hover:underline" href="https://twitter.com/cmdcolin">Twitter</a></div><div class="m-4"><a class="hover:underline" href="/projects">Projects</a></div><div class="m-4"><a class="hover:underline" href="/rss.xml">RSS</a></div></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"allPosts":[{"title":"A spooky error when you have a string bigger than 512MB in Chrome","date":"2021-10-30","slug":"2021-10-30-spooky","content":"\nNow gather round for a spooky story\n\nLate one night... in the haunted office space castle (hindenbugs cackling in\nthe background amongst the dusty technical books) the midnight candles were\nburning bright and we entered data for a user file\n\nA simple 52MB gzipped datafile that we want to process in the browser. We unzip\nit, decode it, and ...an error\n\nERROR: data not found\n\n![](/media/pumpkin-dark.jpg)\n\nBut... our code is so simple (we of course abide by the religion of writing \"simple code\" you know)...what could be happening?\n\nThe code looks like this\n\n```js\nconst buf = unzip(file)\nconst str = new TextDecoder().decode(buf)\n```\n\nWe trace it back and run a console.log(str)\n\nIt looks empty. We try running console.log(str.length) ... it prints out 0\n\nBut if we console.log(buffer.length) we get 546,483,710 bytes...\n\nWhat could be happening?\n\nWe see in the TextDecoder documentation that it has a note called \"fatal\". We try\n\n```js\nconst buf = unzip(file)\nconst str = new TextDecoder('utf8', { fatal: true }).decode(buf)\n```\n\nThis doesn't change the results though\n\nThen it dawns on us while the lightning hits and the thunderclap booms and the\nwind blows through the rattly windows\n\nWe have hit...the maximum string length in Chrome\n\nBWAHAHAHAHA\n\nThe maximum string length!!! Nooooooo\n\nIt is 512MB on the dot... 536,870,888 bytes. We test this to be sure\n\n```\nconst len = 536_870_888;\nconst buf = new Uint8Array(len);\nfor (let i = 0; i \u003c len; i++) {\n  buf[i] = \"a\".charCodeAt(0);\n}\nconst str = new TextDecoder().decode(buf);\nconsole.log(str.length);\n\n```\n\nThis is correct, outputs 536,870,888\n\nWith anything, even one byte more, it fails and outputs 0\n\nhappy halloween!!\n\npumpkin photo source: http://mountainbikerak.blogspot.com/2010/11/google-chrome-pumpkin.html\n\nchrome 95 tested\n\nnodejs 15 - at 512MB+1 bytes it prints an error message `Error: Cannot create a string longer than 0x1fffffe8 characters` for significantly greater than 512MB\ne.g. 600MB it actually prints a different error `TypeError [ERR_ENCODING_INVALID_ENCODED_DATA]: The encoded data was not valid for encoding utf-8`)\n\nfirefox 93 - goes up to ~1GB but then gives Exception { name: \"NS_ERROR_OUT_OF_MEMORY\", message: \"\", result: 2147942414\n\nmidori 6 (safari-alike/webkit) - goes up to ~2GB fine! will have to test more\n"},{"title":"Jest parallelization, globals, mocks, and squawkless tests","date":"2021-10-05","slug":"2021-10-05-jest","content":"\nI found that there is a little bit of confusion and misunderstanding around how\nthings like parallelization work in jest, which sometimes leads to additional\nhacking around problems that may not exist or speculating incorrectly about\ntest failure. This is also of course a point of concern when you have code that\nfor some reason or another uses global variables. Here are a short summary of\nthings that may cause confusion.\n\n## Tests in a single file are NOT run in parallel\n\nSimple example, the global variable r is included in the test condition, but it\nis accurately run in all cases because the tests are not run in parallel.\n\n```js\nlet r = 0\n\nfunction timeout(ms) {\n  return new Promise(resolve =\u003e setTimeout(resolve, ms))\n}\n\ndescribe('tests', () =\u003e {\n  it('t1', async () =\u003e {\n    await timeout(1000)\n    expect(r).toBe(0)\n    r++\n  })\n  it('t2', async () =\u003e {\n    await timeout(1000)\n    expect(r).toBe(1)\n    r++\n  })\n  it('t3', async () =\u003e {\n    await timeout(1000)\n    expect(r).toBe(2)\n    r++\n  })\n})\n```\n\nThis test will take 3 seconds, and will accurately count the global variable.\nIf it was in parallel, it may only take 1 second, and would inaccurately count\nthe global variable due to race conditions\n\n## Tests in different files ARE run in parallel\n\nLet's take another example where we use a global variable, and then two\ndifferent tests use the global variable.\n\nfile_using_some_globals.js\n\n```js\nlet myGlobal = 0\n\nexport function doStuff() {\n  myGlobal++\n  return myGlobal\n}\n\nexport function resetMyGlobal() {\n  myGlobal = 0\n}\n\nexport function timeout(ms) {\n  return new Promise(resolve =\u003e setTimeout(resolve, ms))\n}\n```\n\ntest_global_vars1.test.js\n\n```js\nimport { doStuff, timeout } from './dostuff'\ntest('file1', async () =\u003e {\n  doStuff()\n  await timeout(1000)\n  expect(doStuff()).toEqual(2)\n})\n```\n\ntest_global_vars2.test.js\n\n```js\nimport { doStuff, timeout } from './dostuff'\n\ntest('file1', async () =\u003e {\n  await timeout(1000)\n  expect(doStuff()).toEqual(1)\n})\n```\n\nThis test completes in less than 2 seconds, and these tests are run in\nparallel. They use different instances of the global state, and therefore have\nno worries with colliding their state.\n\n## Does a mock from one test affect another test?\n\nWhile seeking the fabled \"squawk-less\" test, it is often useful to mock console\nso that tests that produce an expected error don't actually print an error\nmessage. However, if not done carefully, you will remove errors across tests\n\nSo, could a mock from one test affect another test? If it's in the same file,\nyes!\n\nmock_console.test.js\n\n```\ntest(\"test1\", () =\u003e {\n  console.error = jest.fn();\n  console.error(\"wow\");\n  expect(console.error).toHaveBeenCalled();\n});\n\ntest(\"test2\", () =\u003e {\n  // this console.error will not appear because test1 mocked away console.error\n  // without restoring it\n  console.error(\"Help I can't see!\");\n});\n\n\n```\n\nTo properly mock these, you should restore the console mock at the end of your\nfunction\n\n```\ntest(\"test1\", () =\u003e {\n  const orig = console.error;\n  console.error = jest.fn();\n  console.error(\"I should not see this!\");\n  expect(console.error).toHaveBeenCalled();\n  console.error = orig;\n});\n\ntest(\"test2\", () =\u003e {\n  const consoleMock = jest.spyOn(console, \"error\").mockImplementation();\n  console.error(\"I should not see this!\");\n  consoleMock.mockRestore();\n});\n\ntest(\"test3\", () =\u003e {\n  console.error(\"I should see this error!\");\n});\n```\n\n## Add-on: Achieve squawkless tests!\n\nYour test output should just be a big list of PASS statements, not interleaved\nwith console.error outputs from when you are testing error conditions of your\ncode\n\n\"Squawkless tests\" is a term I made up, but it means that if you have code\nunder test that prints some errors to the console, then mock the console.error\nfunction, as in the previous section. Don't stand for having a bunch of verbose\nerrors in your CI logs! However, I also suggest only mocking out console.error\nfor tests that are **expected** to have errors, lest you paper over unexpected\nerrors.\n\n![](/media/squawkless_tests.png)\n\nFigure: a nice clean test suite without a bunch of crazy console.error outputs\n\n## Conclusion\n\nGetting better at testing requires exercise, and understanding the basics of\nyour tools can help! Hopefully this helps you achieve a better understanding\nand write cleaner jest tests.\n"},{"title":"Decrease your idle CPU usage when developing typescript apps with this one weird environment variable","date":"2021-09-05","slug":"2021-09-05-typescript","content":"\nTL;DR:\n\nadd this to your bashrc\n\n```\nexport TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling\n```\n\n\u003chr/\u003e\n\nBy default, the typescript watcher configuration e.g. tsc --watch or whatever\nis run internally to a create-react-app typescript app (I see it in the process\nmanager as fork-ts-checker-webpack-plugin cpu usage) can have high idling\n(doing nothing...) CPU usage\n\nThis is because the default configuration polls for file changes (constantly\nasks the computer if there are changes every 250ms or so). There is an\nalternative configuration for this to change it to a file watcher so it\nreceives file system notifications on file change. There is discussion here on\nthis.\n\nThe main summary is that a env variable set to\nTSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling allows this\n\nhttps://github.com/microsoft/TypeScript/issues/31048\n\nThe issue thread shows that it can go from roughly ~7% idle CPU usage to 0.2%.\nThis corresponds with what I see too after applying this! Detailed docs for\ntypescript discuss some of the reasoning behing not making this the default\n\nhttps://github.com/microsoft/TypeScript-Handbook/blob/master/pages/Configuring%20Watch.md#background\n\nIt claims that some OS specific behaviors of file watching could be harmful to\nmaking it the default. For example, that (maybe?) on linux, it may use a large\nnumber of file watchers which can exceed notify handles (this is a setting I\ncommonly have to increase in linux, guide here\nhttps://dev.to/rubiin/ubuntu-increase-inotify-watcher-file-watch-limit-kf4)\n\nPS: if you have a package.json of a `create-react-app --template typescript` or\nsomething like this then you can edit the package.json to apply this\nautomatically\n\n```\n-\"start\": \"react-scripts start\"\n+\"start\": \"cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling react-scripts start\"\n```\n\nPhew. I can already feel my laptop running cooler...or at least I can sleep\nmore soundly knowing that my readers adopt this and save some CPU cycles for\nplanet earth...and hopefully don't run into any of the caveats\n\nEdit: It may be worth it to note, the 'UseFsEvents' part of this uses the\nnode.js fs.watch API and the polling based API is based on fs.watchFile\n\nFun table of how the watchers are implemented on different OSs\n[[1](https://github.com/microsoft/TypeScript/issues/31048#issuecomment-495483957)]\n\n```\nOn Linux systems, this uses inotify(7).\nOn BSD systems, this uses kqueue(2).\nOn macOS, this uses kqueue(2) for files and FSEvents for directories.\nOn SunOS systems (including Solaris and SmartOS), this uses event ports.\nOn Windows systems, this feature depends on ReadDirectoryChangesW.\nOn Aix systems, this feature depends on AHAFS, which must be enabled.\n```\n\nAnd in general, these should all respond more or less the same, but there are\nsmall corner cases that are discussed\nhttps://nodejs.org/docs/latest/api/fs.html#fs_availability\n\nDisclaimer: it may be worth reading the reasons that typescript does not have\nthis enabled by default before pushing this into your dev environment and all\nyour teammates, but as far as I could tell, it seems ok!\n"},{"title":"An amazing error message if you put more than 2^24 items in a JS Map object","date":"2021-08-15","slug":"2021-08-15-map-limit","content":"\nOne of the fun things about working with big data is that you can often hit\nweird limits with a system.\n\nI was personally trying to load every 'common' single nucleotide polymorphism\nfor the human genome into memory (dbSNP), of which there are over 37 million\nentries (there are many more uncommon ones) for the purposes of making a custom\nsearch index for them [1].\n\nTurns out, you may run into some hard limits. Note that these are all V8-isms\nand may not apply to all browsers or engines (I was using node.js for this)\n\n```\nconst myObject = new Map();\nfor (let i = 0; i \u003c= 50_000_000; i++) {\n  myObject.set(i,i);\n  if(i%100000==0) { console.log(i) }\n}\n```\n\nThis will crash after adding approx 16.7M elements and say\n\n```\n0\n100000\n200000\n...\n16400000\n16500000\n16600000\n16700000\n\nUncaught RangeError: Value undefined out of range for undefined options\nproperty undefined\n```\n\nThat is a very weird error message. It says “undefined” three times! Much\nbetter than your usual “TypeError: Can’t find property ‘lol’ of undefined”. See\nhttps://bugs.chromium.org/p/v8/issues/detail?id=11852 for a bug filed to help\nimprove the error message perhaps.\n\nNow, also interestingly enough, if you use an Object instead of a Map\n\n```js\nconst myObject = {};\nfor (let i = 0; i \u003c= 50_000_000; i++) {\n  myObject['myobj_’+i]=i;\n  if(i%100000==0) { console.log(i) }\n}\n```\n\nThen it will print….\n\n```\n0\n100000\n200000\n...\n8000000\n8100000\n8200000\n8300000\n```\n\nAnd it will actually just hang there…frozen…no error message though! And it is\nfailing at ~8.3M elements. Weird right? This is roughly half the amount of\nelements as the 16.7M case\n\nTurns out there is a precise hard limit for the Map case\n\nFor the Map: 2^24=16,777,216\n\nFor the Object it is around 2^23=8,388,608 HOWEVER, I can actually add more\nthan this, e.g. I can add 8,388,609 or 8,388,610 or even more, but the\noperations start taking forever to run, e.g. 8,388,999 was taking many minutes\n\nVery weird stuff! If you expected me to dig into this and explain it in deep\ntechnical detail, well, you’d be wrong. I am lazy. However, this helpful post\non stackoverflow by a V8 js engine developer clarifies the Map case!!\nhttps://stackoverflow.com/questions/54452896/maximum-number-of-entries-in-node-js-map\n\n```\nV8 developer here. I can confirm that 2^24 is the maximum number of entries in\na Map. That’s not a bug, it’s just the implementation-defined limit.\n\nThe limit is determined by:\n\nThe FixedArray backing store of the Map has a maximum size of 1GB (independent\nof the overall heap size limit) On a 64-bit system that means 1GB / 8B = 2^30 /\n2^3 = 2^27 ~= 134M maximum elements per FixedArray A Map needs 3 elements per\nentry (key, value, next bucket link), and has a maximum load factor of 50% (to\navoid the slowdown caused by many bucket collisions), and its capacity must be\na power of 2. 2^27 / (3 * 2) rounded down to the next power of 2 is 2^24, which\nis the limit you observe.  FWIW, there are limits to everything: besides the\nmaximum heap size, there’s a maximum String length, a maximum Array length, a\nmaximum ArrayBuffer length, a maximum BigInt size, a maximum stack size, etc.\nAny one of those limits is potentially debatable, and sometimes it makes sense\nto raise them, but the limits as such will remain. Off the top of my head I\ndon’t know what it would take to bump this particular limit by, say, a factor\nof two – and I also don’t know whether a factor of two would be enough to\nsatisfy your expectations.\n\n```\n\nGreat details there. It would also be good to know what the behavior is for the\nObject, which has those 100% CPU stalls after ~8.3M, but not the same error\nmessage...\n\nAnother fun note: if I modify the Object code to use only “integer IDs” the\ncode actually works fine, does not hit any errors, and is “blazingly fast” as\nthe kids call it\n\n```js\nconst myObject = {}\nfor (let i = 0; i \u003c= 50_000_000; i++) {\n  myObject[i] = i\n  if (i % 100000 == 0) {\n    console.log(i)\n  }\n}\n```\n\nI presume that this code works because it detects that I’m using it like an\narray and it decides to transform how it is working internally and not use a\nhash-map-style data structure, so does not hit a limit. There is a slightly\nhigher limit though, e.g. 1 billion elements gives “Uncaught RangeError:\nInvalid array length”\n\n```js\nconst myObject = {}\nfor (let i = 0; i \u003c= 1_000_000_000; i++) {\n  myObject[i] = i\n  if (i % 100000 == 0) {\n    console.log(i)\n  }\n}\n```\n\nThis has been another episode of ....the twilight zone (other episodes\ncatalogued here) https://github.com/cmdcolin/technical_oddities/\n\n[1] The final product of this adventure was this, to create a search index for\na large number of elements https://github.com/GMOD/ixixx-js\n"},{"title":"Do you understand your NPM dependencies?","date":"2021-07-27","slug":"2021-07-27-npm-dependencies","content":"\nYou are writing a library...or your writing an app and you want to publish some\nof the components of it as a library...\n\nHere are some questions in the form of comments\n\n- Did you realize that your yarn.lock will be ignored for anyone who installs\n  your libraries?\n\n- Did you realize this means that your perfectly running test suite with your\n  yarn.lock could be a failing case for consumers of your app unless you don’t\n  use semver strings like ^1.0.0 and just hardcode it to 1.0.0?\n\n- Did you realize the default of ^1.0.0 automatically gets minor version bumps\n  which are often fairly substantial changes, e.g. even breaking possibly?\n\n- Did you know that larger libraries like @material-ui/core don’t like to bump\n  their major version all the time for example so large changes are often made\n  to the minor version?\n\n- Did you know if you run `yarn upgrade`, it may update what is in your yarn.lock file but will not update what is in your package.json?\n\n- Did you realize that this means that if you depend on the results of running `yarn upgrade` e.g. it gave you a bugfix, you will be shipping buggy code to consumers of your library?\n\nJust something to be aware of! You can always ride the dragon and accept these\nminor breakages from semver bumps, but it can introduce some issues for your\nconsumers\n\nRandom fun thing: Adding a yarn package can even downgrade some other packages.\nFor example if you have ^6.0.0 in your package.json, you yarn upgrade it up to\n^6.1.0 but then later install another library that requires a hard 6.0.1, yarn\nwill decide to downgrade you to 6.0.1\n"},{"title":"Making a HTTPS accessible S3 powered static site with CloudFront+route 53","date":"2020-12-26","slug":"2020-12-26-pt2","content":"\nThis is not a very authoritative post because I stumbled though this but\nI think I got it working now on my website :)\n\n## Setup your S3 bucket\n\nFirst setup your S3 bucket, your bucket must be named yourdomain.com\ne.g. named after your domain\n\nThen if you have a create-react-app setup I add a script in package.json\nthat runs\n\n```\n \"predeploy\": \"npm run build\",\n \"deploy\": \"aws sync --delete build s3://yourdomain.com\"\n```\n\nThen we can run \"yarn deploy\" and it will automatically upload our\ncreate-react-app website to our S3 static site bucket.\n\nThen make sure your bucket has public permissions enabled\n\u003chttps://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2\u003eThen\nmake sure your bucket has \"static site hosting\" enabled too\n\n## Setup route 53, and make your NS entries in domains.google.com\n\nI bought a domain with domains.google.com\n\nGoogle then emailed me to validate my ownership\n\nThen I went to aws.amazon.com route 53 and I created a hosted zone\n\nThis generated 4 name server entries and I added those to the\ndomains.google.com site\n\n![](/media/638618421776515072_0.png)\n\nScreenshot shows copying the NS values from route 53 to the name servers\narea of domains.google.com\n\n## Setup your Amazon certificate for making SSL work on CloudFront\n\nTo properly setup However, this does not work so you need to go to\nAmazon Certificates-\u003eProvision certificates\n\nWe request the certificate for\n\n[www.yourdomain.com](http://www.yourdomain.com)\nyourdomain.com\n\nThen it generates some codes for a CNAME value for each of those two\nentries, and has a button to autoimport those CNAME values to route53\n\nThen it will say \"Pending validation\"...I waited like an hour and then\nit changed to \"Success\".\n\n![](/media/638618421776515072_1.png)\n\nScreenshot shows the now successful Amazon Certificate. After you get\nthis, you can proceed to finishing your cloudfront\n\n## Create a CloudFront distribution and add \"Alternative CNAME\" entries for your domain\n\nThen we can update our CloudFront distribution and add these to\nthe \"Alternative CNAME\" input box\n\nyourdomain.com\n[www.yourdomain.com](http://www.yourdomain.com)\n\nNote also that I first generated my certificate in us-east-2 but the\n\"Import certificate form\" in cloudfront said I had to create it in\nus-east-1\n\n![](/media/638618421776515072_2.png)\n\n## Add a default object index.html to the CloudFront setting\n\nMake your CloudFront \"default object\" is index.html\n\nYou have to manually type this in :)\n\n## Add the CloudFront distribution to your Route 53\n\nAdd a Route 53 \"A\" record that points to the CloudFront domain name e.g.\nd897d897d87d98dd.cloudfront.net\n\n## Summary of steps needed\n\nThe general hindsight 20/20 procedure is\n\n1.  Upload your static content to an S3 bucket called yoursite.com (must\n    be your domain name)\n2.  Make your S3 bucket have the \"static website\" setting on in the\n    properties menu and add a permissions policy that supports getObject\n    e.g. \u003chttps://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-2\u003e\n3.  Create a CloudFront distribution for your website\n4.  Make the CloudFront default object index.html\n5.  Create your domain with domains.google.com or similar\n6.  Point the google domain's name server to Route 53 NS list from AWS\n7.  Add Route 53 A records that point to the CloudFront domain name e.g.\n    d897d897d87d98dd.cloudfront.net\n8.  Create Amazon issued certificate for yourdomain.com, which can\n    auto-import a validation CNAME to your Route 53\n9.  Make your CloudFront domain support your Alternative CNAME's e.g.\n    yourdomain.com which requires importing (e.g. selecting from a list\n    that they auto-populate) your Amazon-issued-certificate\n\n## Troubleshooting and notes\n\nProblem: Your website gives 403 CloudFlare error\nSolution: You have to get the Alternateive CNAME configuration setup\n(pre-step involves the certificate request and validation)\n\nProblem: Your website gives an object not found error\nSolution: Set the CloudFront \"default object\" to index.html\n\n## Random comment\n\nThis is one of those processes (creating the cloudfront/route 53) that\nprobably could have done with the aws-sam CLI and it would have possibly\nbeen easier, it is quite fiddly doing all these steps in the web\ninterface\n"},{"title":"Making a serverless website for photo and video upload pt. 2","date":"2020-12-26","slug":"2020-12-26","content":"\nThis post follows\non \u003chttps://cmdcolin.github.io/2020-12-24.html\u003e\n\nIt is possible I zoomed ahead too fast to make this a continuous\ntutorial, but overall I just wanted to post an update\n\nIn pt. 1 I learned how to use the `aws-sam` CLI tool. This was a great\ninsight for me about automating deployments. I can now simply run `sam deploy` and it will create new dynamodb tables, lambda functions, etc.\n\nAfter writing pt 1. I converted the existing vue-js app that was in the\naws tutorial and converted it to react. Then I extended the app to allow\n\n- Posting comments on photos\n- Uploading multiple files\n- Uploading videos\n  etc.\n\nIt will be hard to summarize all the changes since now the app has taken\noff a little bit but it looks like this:\n\nRepo structure\n\n```\n ./frontend # created using npx create-react-app frontend --template\n typescript\n ./frontend/src/App.tsx # main frontend app code in react\n ./lambdas/\n ./lambdas/postFile # post a file to the lambda, this uploads a row to\n dynamodb and returns a pre-signed URL for uploading (note that if the\n client failed it's upload, that row in the lambda DB might be in a bad\n state...)\n ./lambdas/getFiles # get all files that were ever posted\n ./lambdas/postComment # post a comment on a picture with POST\n request\n ./lambdas/getComments?file=filename.jpg # get comments on a\n picture/video with GET request\n```\n\nHere is a detailed code for uploading the file. We upload one file at a\ntime, but the client code post to the lambda endpoint individually for\neach file\n\nThis generates a pre-signed URL to allow the client-side JS (not the\nlambda itself) to directly upload to S3, and also posts a row in the S3\nto the filename that will. It is very similar code in\nto \u003chttps://cmdcolin.github.io/2020-12-24.html\u003e\n\n./lambdas/postFile/app.js\n\n```js\n'use strict'\n\nconst AWS = require('aws-sdk')\nconst multipart = require('./multipart')\nAWS.config.update({ region: process.env.AWS_REGION })\nconst s3 = new AWS.S3()\n\n// Change this value to adjust the signed URL's expiration\nconst URL_EXPIRATION_SECONDS = 300\n\n// Main Lambda entry point\nexports.handler = async event =\u003e {\n  return await getUploadURL(event)\n}\n\nconst { AWS_REGION: region } = process.env\n\nconst dynamodb = new AWS.DynamoDB({ apiVersion: '2012-08-10', region })\n\nasync function uploadPic({\n  timestamp,\n  filename,\n  message,\n  user,\n  date,\n  contentType,\n}) {\n  const params = {\n    Item: {\n      timestamp: {\n        N: `${timestamp}`,\n      },\n      filename: {\n        S: filename,\n      },\n      message: {\n        S: message,\n      },\n      user: {\n        S: user,\n      },\n      date: {\n        S: date,\n      },\n      contentType: {\n        S: contentType,\n      },\n    },\n    TableName: 'files',\n  }\n  return dynamodb.putItem(params).promise()\n}\n\nconst getUploadURL = async function (event) {\n  try {\n    const data = multipart.parse(event)\n    const { filename, contentType, user, message, date } = data\n    const timestamp = +Date.now()\n    const Key = `${timestamp}-${filename}` // Get signed URL from S3\n\n    const s3Params = {\n      Bucket: process.env.UploadBucket,\n      Key,\n      Expires: URL_EXPIRATION_SECONDS,\n      ContentType: contentType, // This ACL makes the uploaded object publicly readable. You must also uncomment // the extra permission for the Lambda function in the SAM template.\n\n      ACL: 'public-read',\n    }\n\n    const uploadURL = await s3.getSignedUrlPromise('putObject', s3Params)\n\n    await uploadPic({\n      timestamp,\n      filename: Key,\n      message,\n      user,\n      date,\n      contentType,\n    })\n\n    return JSON.stringify({\n      uploadURL,\n      Key,\n    })\n  } catch (e) {\n    const response = {\n      statusCode: 500,\n      body: JSON.stringify({ message: `${e}` }),\n    }\n    return response\n  }\n}\n```\n\n./lambdas/getFiles/app.js\n\n```js\n// eslint-disable-next-line import/no-unresolved\nconst AWS = require('aws-sdk')\n\nconst { AWS_REGION: region } = process.env\n\nconst docClient = new AWS.DynamoDB.DocumentClient()\n\nconst getItems = function () {\n  const params = {\n    TableName: 'files',\n  }\n\n  return docClient.scan(params).promise()\n}\n\nexports.handler = async event =\u003e {\n  try {\n    const result = await getItems()\n    return {\n      statusCode: 200,\n      body: JSON.stringify(result),\n    }\n  } catch (e) {\n    return {\n      statusCode: 400,\n      body: JSON.stringify({ message: `${e}` }),\n    }\n  }\n}\n```\n\n./frontend/src/App.tsx (excerpt)\n\n```tsx\nasync function myfetch(params: string, opts?: any) {\n  const response = await fetch(params, opts)\n  if (!response.ok) {\n    throw new Error(`HTTP ${response.status}\n ${response.statusText}`)\n  }\n  return response.json()\n}\n\nfunction UploadDialog({\n  open,\n  onClose,\n}: {\n  open: boolean\n  onClose: () =\u003e void\n}) {\n  const [images, setImages] = useState\u003cFileList\u003e()\n  const [error, setError] = useState\u003cError\u003e()\n  const [loading, setLoading] = useState(false)\n  const [total, setTotal] = useState(0)\n  const [completed, setCompleted] = useState(0)\n  const [user, setUser] = useState('')\n  const [message, setMessage] = useState('')\n  const classes = useStyles()\n\n  const handleClose = () =\u003e {\n    setError(undefined)\n    setLoading(false)\n    setImages(undefined)\n    setCompleted(0)\n    setTotal(0)\n    setMessage('')\n    onClose()\n  }\n\n  return (\n    \u003cDialog onClose={handleClose} open={open}\u003e\n           \u003cDialogTitle\u003eupload a file (supports picture or video)\u003c/DialogTitle\u003e \n         \u003cDialogContent\u003e\n               \u003clabel htmlFor=\"user\"\u003ename (optional) \u003c/label\u003e\n               \u003cinput\n          type=\"text\"\n          value={user}\n          onChange={event =\u003e setUser(event.target.value)}\n          id=\"user\"\n        /\u003e\n               \u003cbr /\u003e       \u003clabel htmlFor=\"user\"\u003emessage (optional) \u003c/label\u003e\n               \n        \u003cinput\n          type=\"text\"\n          value={message}\n          onChange={event =\u003e setMessage(event.target.value)}\n          id=\"message\"\n        /\u003e\n               \u003cbr /\u003e\n               \n        \u003cinput\n          multiple\n          type=\"file\"\n          onChange={e =\u003e {\n            let files = e.target.files\n            if (files \u0026\u0026 files.length) {\n              setImages(files)\n            }\n          }}\n        /\u003e\n               {error ? (\n          \u003cdiv className={classes.error}\u003e{`${error}`}\u003c/div\u003e\n        ) : loading ? (\n          `Uploading...${completed}/${total}`\n        ) : completed ? (\n          \u003ch2\u003eUploaded \u003c/h2\u003e\n        ) : null}       \n        \u003cDialogActions\u003e\n                   \n          \u003cButton\n            style={{ textTransform: 'none' }}\n            onClick={async () =\u003e {\n              try {\n                if (images) {\n                  setLoading(true)\n                  setError(undefined)\n                  setCompleted(0)\n                  setTotal(images.length)\n                  await Promise.all(\n                    Array.from(images).map(async image =\u003e {\n                      const data = new FormData()\n                      data.append('message', message)\n                      data.append('user', user)\n                      data.append('date', new Date().toLocaleString())\n                      data.append('filename', image.name)\n                      data.append('contentType', image.type)\n                      const res = await myfetch(API_ENDPOINT + '/postFile', {\n                        method: 'POST',\n                        body: data,\n                      })\n\n                      await myfetch(res.uploadURL, {\n                        method: 'PUT',\n                        body: image,\n                      })\n\n                      setCompleted(completed =\u003e completed + 1)\n                    }),\n                  )\n                  setTimeout(() =\u003e {\n                    handleClose()\n                  }, 500)\n                }\n              } catch (e) {\n                setError(e)\n              }\n            }}\n            color=\"primary\"\n          \u003e\n                       upload          \n          \u003c/Button\u003e\n                   \u003cButton\n            onClick={handleClose}\n            color=\"primary\"\n            style={{ textTransform: 'none' }}\n          \u003e\n                       cancel          \n          \u003c/Button\u003e       \n        \u003c/DialogActions\u003e\n             \n      \u003c/DialogContent\u003e   \n    \u003c/Dialog\u003e\n  )\n}\n```\n\ntemplate.yaml for AWS\n\n```\n AWSTemplateFormatVersion: 2010-09-09\n Transform: AWS::Serverless-2016-10-31\n Description: S3 Uploader\n\n Resources:\n  filesDynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      AttributeDefinitions:\n        - AttributeName: \"timestamp\"\n          AttributeType: \"N\"\n      KeySchema:\n        - AttributeName: \"timestamp\"\n          KeyType: \"HASH\"\n      ProvisionedThroughput:\n        ReadCapacityUnits: \"5\"\n        WriteCapacityUnits: \"5\"\n      TableName: \"files\"\n\n  # HTTP API\n  MyApi:\n    Type: AWS::Serverless::HttpApi\n    Properties:\n      # CORS configuration - this is open for development only and\n should be restricted in prod.\n      # See\n \u003chttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-property-httpapi-httpapicorsconfiguration.html\u003e\n      CorsConfiguration:\n        AllowMethods:\n          - GET\n          - POST\n          - DELETE\n          - OPTIONS\n        AllowHeaders:\n          - \"*\"\n        AllowOrigins:\n          - \"*\"\n\n  UploadRequestFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: lambdas/postFile/\n      Handler: app.handler\n      Runtime: nodejs12.x\n      Timeout: 3\n      MemorySize: 128\n      Environment:\n        Variables:\n          UploadBucket: !Ref S3UploadBucket\n      Policies:\n        - AmazonDynamoDBFullAccess\n        - S3WritePolicy:\n            BucketName: !Ref S3UploadBucket\n        - Statement:\n            - Effect: Allow\n              Resource: !Sub \"arn:aws:s3:::${S3UploadBucket}/\"\n              Action:\n                - s3:putObjectAcl\n      Events:\n        UploadAssetAPI:\n          Type: HttpApi\n          Properties:\n            Path: /postFile\n            Method: post\n            ApiId: !Ref MyApi\n\n\n  FileReadFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: lambdas/getFiles/\n      Handler: app.handler\n      Runtime: nodejs12.x\n      Timeout: 3\n      MemorySize: 128\n      Policies:\n        - AmazonDynamoDBFullAccess\n      Events:\n        UploadAssetAPI:\n          Type: HttpApi\n          Properties:\n            Path: /getFiles\n            Method: get\n            ApiId: !Ref MyApi\n\n  ## S3 bucket\n  S3UploadBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      CorsConfiguration:\n        CorsRules:\n          - AllowedHeaders:\n              - \"*\"\n            AllowedMethods:\n              - GET\n              - PUT\n              - HEAD\n            AllowedOrigins:\n              - \"*\"\n\n\n ## Take a note of the outputs for deploying the workflow templates\n in this sample application\n Outputs:\n  APIendpoint:\n    Description: \"HTTP API endpoint URL\"\n    Value: !Sub\n \"https://${MyApi}.execute-api.${AWS::Region}.amazonaws.com\"\n  S3UploadBucketName:\n    Description: \"S3 bucket for application uploads\"\n    Value: !Ref \"S3UploadBucket\"\n\n```\n\nTo display all the pictures I use a switch from video or img tag based\non contentType.startsWith('video'). I also use the \"figcaption\" HTML tag\nto have a little caption on the pics/videos\n\n./frontend/src/App.tsx\n\n```\n function Media({\n  file,\n  style,\n  onClick,\n  children,\n }: {\n  file: File;\n  onClick?: Function;\n  style?: React.CSSProperties;\n  children?: React.ReactNode;\n }) {\n  const { filename, contentType } = file;\n  const src = `${BUCKET}/${filename}`;\n  return (\n    \u003cfigure style={{ display: \"inline-block\" }}\u003e\n      \u003cpicture\u003e\n        {contentType.startsWith(\"video\") ? (\n          \u003cvideo style={style} src={src} controls onClick={onClick as\n any} /\u003e\n        ) : (\n          \u003cimg style={style} src={src} onClick={onClick as any} /\u003e\n        )}\n      \u003c/picture\u003e\n      \u003cfigcaption\u003e{children}\u003c/figcaption\u003e\n    \u003c/figure\u003e\n  );\n }\n```\n\nNow the really fun part: if you get an image of a picture frame\nlike \u003chttps://www.amazon.com/Paintings-Frames-Antique-Shatterproof-Osafs2-Gld-A3/dp/B06XNQ8W9T\u003e\n\nYou can make it a border for any image or video using border-image CSS\n\n```\n     style = {\n         border: \"30px solid\",\n         borderImage: `url(borders/${border}) 30 round`\n     }\n```\n\n![](/media/638602799897329664_0.png)\n\nSummary\n\nThe template.yaml automatically deploys the lambdas for postFile/getFile\nand the files table in dynamoDB\n\nThe React app uses postFile for each file in an `\u003cinput type=\"file\"/\u003e`,\nthe code uses React hooks and functional components but is hopefully not\ntoo complex\n\nI also added commenting on photos. The code is not shown here but you\ncan look in the source code for details\n\n![](/media/638602799897329664_1.png)\n\nOverall this has been a good experience learning to develop this app and\nlearning to automate the cloud deployment is really good for ensuring\nreliability and fast iteration.\n\nAlso quick note on serverless CLI vs aws-sam. I had tried a serverless\nCLI tutorial from another user but it didn't click with me, while the\naws-sam tutorial from\n\u003chttps://searchvoidstar.tumblr.com/post/638408397901987840/making-a-serverless-website-for-photo-upload-pt-1\u003e was\na great kick start for me. I am sure the serverless CLI is great too and\nit ensures a bit less vendor lock in, but then is also a little bit\nremoved from the native aws config schemas. Probably fine though\n\nSource code \u003chttps://github.com/cmdcolin/aws_photo_gallery/\u003e\n"},{"title":"Making a serverless website for photo upload pt. 1","date":"2020-12-24","slug":"2020-12-24","content":"\nI set out to make a serverless website for photo uploads. Our dearly\ndeparted dixie dog needed a place to have photo uploads.\n\nI didn't want to get charged dollars per month for a running ec2\ninstance, so I wanted something that was lightweight e.g. serverless,\nand easy\n\nI decided to follow this tutorial\n\n\u003chttps://aws.amazon.com/blogs/compute/uploading-to-amazon-s3-directly-from-a-web-or-mobile-application/\u003e\n\nI really liked the command line deployment (aws-sam) because fiddling\naround with the AWS web based control panel is ridiculously complicated\n\nFor example I also tried following this tutorial which uses the web\nbased UI (\u003chttps://www.youtube.com/watch?v=mw_-0iCVpUc\u003e) and it just did\nnot work for me....I couldn't stay focused (blame ADHD or just my CLI\nobsession?) and certain things like \"Execution role\" that they say to\nmodify are not there in the web UI anymore, so I just gave up (I did try\nthough!)\n\nTo install aws-sam I used homebrew\n\n```\n brew tap aws/tap\n brew install aws-sam-cli\n brew install aws-sam-cli # I had to run the install command twice ref https://github.com/aws/aws-sam-cli/issues/2320#issuecomment-721414971\n\n git clone https://github.com/aws-samples/amazon-s3-presigned-urls-aws-sam\n cd amazon-s3-presigned-urls-aws-sam\n sam deploy --guided\n\n # proceeeds with a guided installation, I used all defaults except I\n made \"UploadRequestFunction may not have authorization defined, Is\n this okay? [y/N]: y\"\n```\n\n![](/media/638408397901987840_0.png)\n\nThey then in the tutorial describe trying to use postman to test\n\nI test with `curl` instead\n\n```\ncurl 'https://fjgbqj5436.execute-api.us-east-2.amazonaws.com/uploads' {\"uploadURL\":\"https://sam-app-s3uploadbucket-1653634.s3.us-east-2.amazonaws.com/112162.jpg?Content-Type=image%2Fjpeg\u0026X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=ASIAU6CQBER6YBNCDDMJ%2F20201224%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20201224T174804Z\u0026X-Amz-Expires=300\u0026X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDIaCXVzLWVhc3QtMiJGMEQCIH65IvgJsofUpIX46lTaG3Pi5WC85ti1lukM3iICh%2BB%2BAiAJEyynPNPhZN8%2Bg1ylO7wthqud9cBcNIChIp2H%2F%2BR7mCryAQjb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDMzOTQ3MDI2MzQyMSIMLqPo1IYyH7udCGZuKsYBSEF3c50YXkmPeSWcLsEzq%2BFBTpeOIrwZTyCUjbJ7fgJUakhM1YRX40jExstN8eJcMXqw00Xd5lYHvZDbU9ajwWPLRAxcEN5BQ0utqn0NGTLyJhibzJUj8cjgm5RguIEKe9GUtMVWa9mi7C5%2FlFpS0i9jK5BSVf74JyPSLETV5mzMMzy5kHBQMGjw1dR66E3MG8PjIqfgKjhVtZmlaicf5OmeqNI2%2F8T5ye%2FICRsH4d7KNEmj4FELa8buW8U%2Fn97ThfH3P7XmMNOok%2F8FOuEBDj1EHluCT4DfZ1jIXjvrJsVv1WtV4POQDn2Dah%2BWosBn%2BFNTtQtw841ACDarYR1ZVbuwcpTjfBPlGuSOncPsbzOhzDy7wYyumsPKsXoPdxTncMWbx4BQkbU5SeF9hjpfIKRMSOqkJBN7%2BtgHXwuW1rfYMDN2OAlQZpTj7uWMPWojUMbvMzyHvI2pfgcRAlrBdGGYDigyjWl9QXP%2Bdi6WiR7XCSXbWcIAJDZh%2Beb%2BIH1asmMJtpAK6nMP8gWczaYh7PMeYyVOIs2B20xQBy%2Bz7oe%2BYQ2GfdEr2hgqPH3jd%2B7c\u0026X-Amz-Signature=11b8cd524c25ef51193e3b3fc4816760ebcde8bfc74bd52f3f91d8bf409620f5\u0026X-Amz-SignedHeaders=host\",\"Key\":\"112162.jpg\"}% \n\n```\n\nThe premise of this is you make a request, and then the response from\nthe API is a pre-signed URL that then allows you to upload directly to\nS3. You can use `curl \u003curl\u003e --upload-file yourfile.jpg`. This\nautomatically does a PUT request to the s3 bucket (yes, this is talking\ndirectly to s3 now, not the lambda! the lambda is just for generating\nthe \"pre-signed URL\" to let you upload). Careful to copy it exactly as\nis\n\n```\n curl \"https://sam-app-s3uploadbucket-1653634.s3.us-east-2.amazonaws.com/112162.jpg?Content-Type=image%2Fjpeg\u0026X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=ASIAU6CQBER6YBNCDDMJ%2F20201224%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20201224T174804Z\u0026X-Amz-Expires=300\u0026X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDIaCXVzLWVhc3QtMiJGMEQCIH65IvgJsofUpIX46lTaG3Pi5WC85ti1lukM3iICh%2BB%2BAiAJEyynPNPhZN8%2Bg1ylO7wthqud9cBcNIChIp2H%2F%2BR7mCryAQjb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDMzOTQ3MDI2MzQyMSIMLqPo1IYyH7udCGZuKsYBSEF3c50YXkmPeSWcLsEzq%2BFBTpeOIrwZTyCUjbJ7fgJUakhM1YRX40jExstN8eJcMXqw00Xd5lYHvZDbU9ajwWPLRAxcEN5BQ0utqn0NGTLyJhibzJUj8cjgm5RguIEKe9GUtMVWa9mi7C5%2FlFpS0i9jK5BSVf74JyPSLETV5mzMMzy5kHBQMGjw1dR66E3MG8PjIqfgKjhVtZmlaicf5OmeqNI2%2F8T5ye%2FICRsH4d7KNEmj4FELa8buW8U%2Fn97ThfH3P7XmMNOok%2F8FOuEBDj1EHluCT4DfZ1jIXjvrJsVv1WtV4POQDn2Dah%2BWosBn%2BFNTtQtw841ACDarYR1ZVbuwcpTjfBPlGuSOncPsbzOhzDy7wYyumsPKsXoPdxTncMWbx4BQkbU5SeF9hjpfIKRMSOqkJBN7%2BtgHXwuW1rfYMDN2OAlQZpTj7uWMPWojUMbvMzyHvI2pfgcRAlrBdGGYDigyjWl9QXP%2Bdi6WiR7XCSXbWcIAJDZh%2Beb%2BIH1asmMJtpAK6nMP8gWczaYh7PMeYyVOIs2B20xQBy%2Bz7oe%2BYQ2GfdEr2hgqPH3jd%2B7c\u0026X-Amz-Signature=11b8cd524c25ef51193e3b3fc4816760ebcde8bfc74bd52f3f91d8bf409620f5\u0026X-Amz-SignedHeaders=host\" --upload-file test.jpg\n```\n\nThere is no response, but I can then check the s3 console and see the\nfile upload is successful (all files are renamed)\n\n![](/media/638408397901987840_1.png)\n\nFigure shows that the file upload is successful :)\n\nThen we can edit the file frontend/index.html from the repo we cloned to\ncontain the lambda with the /uploads/ suffix\n\n![](/media/638408397901987840_2.png)\n\nFigure shows editing the index.html with the lambda endpoint\n\nThen we manually upload this file to another s3 bucket or test it\nlocally\n\n```\n aws s3 cp index.html s3://mybucket/\n\n\n# then ...visit that in the browser\n```\n\nAt this point the files are getting uploaded but not publically\naccessible. To make them publically accessible we uncomment the\nACL: 'public-read' in the getSignedURL/app.js folder in the github repo\n\n![](/media/638408397901987840_3.png)\n\nFigure showing the public-read uncommented\n\n![](/media/638408397901987840_4.png)\n\nFigure showing the lines that need uncommenting in template.yaml in the\nroot of the github repo that allows putObject in s3 with the public-read\nACL\n\nRe-run `sam deploy --guided`, same thing as at the start\n\nNow the objects are publicly accessible!\n"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"OTCGaf-RlOFSHOyCKH0Gd","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>