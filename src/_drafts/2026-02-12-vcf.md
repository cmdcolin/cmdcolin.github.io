---
title: VCF - from variant calls to population genetics
date: 2026-02-07
---

The
[VCF (Variant Call Format)](https://samtools.github.io/hts-specs/VCFv4.3.pdf) is
the standard file format for representing genomic variants. It encodes SNPs,
indels, structural variants, and more. Like the SAM format, it is deceptively
simple on the surface but has significant complexity underneath.

This post tries to cover both the low-level mechanics of VCF and how researchers
use it as the raw material for population genetics, GWAS, and related analyses
[^tools].

[^tools] This post intentionally focuses on the data format rather than specific
tools. Tools come and go — the popular pipeline of today may be obsolete in five
years — but the fundamental representation issues in VCF (how genotypes are
encoded, what normalization means, why breakends are hard) persist across tool
generations. Understanding the raw format gives you the ability to evaluate new
tools, debug unexpected results, and write your own analyses when existing tools
don't fit.

## Basics

### What is a VCF file?

A VCF file is a tab-delimited text file with a header (lines starting with `##`)
and data lines. Each data line represents a variant site (or a non-variant
reference block in gVCF). The file has 8 fixed columns (`CHROM`, `POS`, `ID`,
`REF`, `ALT`, `QUAL`, `FILTER`, `INFO`) and then optionally a `FORMAT` column
followed by one column per sample.

### The VCF header

The lines starting with `##` at the top of a VCF file are the header. The header
makes VCF self-describing — it defines the meaning and type of every `INFO`
field, `FORMAT` field, `FILTER` value, and contig that appears in the data
lines.

```
##fileformat=VCFv4.3
##INFO=<ID=DP,Number=1,Type=Integer,Description="Total depth">
##INFO=<ID=AF,Number=A,Type=Float,Description="Allele frequency">
##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">
##FORMAT=<ID=DP,Number=1,Type=Integer,Description="Read depth">
##FORMAT=<ID=GQ,Number=1,Type=Integer,Description="Genotype quality">
##FORMAT=<ID=PL,Number=G,Type=Integer,Description="Phred-scaled genotype likelihoods">
##FILTER=<ID=LowQual,Description="Low quality">
##contig=<ID=chr1,length=248956422>
##contig=<ID=chr2,length=242193529>
#CHROM  POS  ID  REF  ALT  QUAL  FILTER  INFO  FORMAT  SAMPLE1  SAMPLE2
```

Key header elements:

- `##fileformat`: The VCF version. Affects what features are supported (e.g.
  breakend notation changed between versions).
- `##INFO` and `##FORMAT`: Declare each field's ID, how many values it has
  (`Number`), its data type (`Type`), and a human-readable description. The
  `Number` field uses special codes: `A` means one value per ALT allele, `R`
  means one per allele including REF, `G` means one per possible genotype, and
  `.` means variable.
- `##contig`: Declares each chromosome/contig with its length. This is useful
  for verifying that two VCFs use the same reference genome — if the contig
  lengths differ, the files were likely generated against different references.
  Some `##contig` lines also include an MD5 checksum of the reference sequence.
- `##FILTER`: Declares each filter name that might appear in the `FILTER`
  column.
- The last header line (starting with `#CHROM`) is the column header row and
  includes the sample names.

Tools that manipulate VCFs should propagate and update header lines
appropriately. Adding a new INFO field without a corresponding `##INFO` header
line produces a technically invalid VCF (though many tools will still read it).

### What is a BCF file?

BCF is the binary, compressed equivalent of VCF, analogous to how BAM relates to
SAM. BCF files are smaller and significantly faster to parse because the fields
are pre-encoded as binary types rather than requiring string parsing. For large
cohort VCFs with thousands of samples, BCF can be dramatically faster. Tools
like `bcftools` can convert between VCF and BCF.

```sh
bcftools view input.vcf -Ob -o output.bcf
bcftools index output.bcf
```

### The 8 fixed columns

- `CHROM`: Chromosome or contig name
- `POS`: 1-based position of the variant on the reference
- `ID`: Variant identifier (e.g. an rsID from dbSNP), or `.` if none
- `REF`: The reference allele at this position
- `ALT`: The alternate allele(s), comma-separated if there are multiple
- `QUAL`: Phred-scaled quality score for the assertion that the `ALT` allele
  exists
- `FILTER`: `PASS` if the site passed all filters, or a semicolon-separated list
  of filter names that it failed
- `INFO`: A semicolon-separated list of key-value pairs with site-level
  annotations

### Multi-allelic sites

A single VCF line can have multiple `ALT` alleles, separated by commas. For
example:

```
chr1  100  .  A  T,C  .  PASS  .
```

This site has two alternate alleles: `T` and `C`. The alleles are indexed: `REF`
is allele 0, the first `ALT` is allele 1, the second `ALT` is allele 2, and so
on. This indexing is used throughout the genotype fields.

Multi-allelic sites add complexity to almost every downstream operation. Many
tools "decompose" multi-allelic sites into multiple biallelic records (one per
`ALT` allele) before analysis, using tools like `bcftools norm -m-` or
`vt decompose`. This simplifies genotype interpretation at the cost of
duplicating some information.

### The FORMAT and sample columns

After the 8 fixed columns, a VCF with genotype data has a `FORMAT` column that
describes the per-sample fields, followed by one column per sample. The `FORMAT`
column is a colon-separated list of keys, and each sample column has
colon-separated values in the same order.

```
FORMAT    SAMPLE1       SAMPLE2
GT:DP:GQ  0/1:30:99    1/1:25:85
```

Here, `GT` is the genotype, `DP` is read depth, and `GQ` is genotype quality.

### The GT (genotype) field

The `GT` field encodes which alleles a sample carries. It uses the allele
indices (0 for `REF`, 1 for first `ALT`, etc.) separated by `/` (unphased) or
`|` (phased).

- `0/0` - homozygous reference (both copies match the reference)
- `0/1` - heterozygous (one copy is reference, one is alternate), unphased
- `1/1` - homozygous alternate (both copies carry the alternate allele)
- `0|1` - heterozygous, phased (first haplotype carries ref, second carries alt)
- `1|0` - heterozygous, phased (first haplotype carries alt, second carries ref)
- `./.` - missing genotype
- `0/2` - heterozygous for ref and the second ALT allele (at a multi-allelic
  site)

The `GT` field also supports higher ploidy:

- `0/0/1` - triploid, two copies of ref and one alt
- `0/0/0/1` - tetraploid

And haploid:

- `0` - haploid reference
- `1` - haploid alternate

This flexibility means parsers cannot assume diploid genotypes.

### Missing data and no-calls

VCF uses `.` to represent missing data. A missing genotype is `./.` (or just `.`
for haploid). This is called a "no-call" — the variant caller could not
determine the genotype at this site for this sample.

No-calls arise for several reasons:

- Insufficient read depth at the site
- Ambiguous or conflicting read evidence
- The site fell in a region that was filtered out during calling
- For array-based genotyping: the probe failed for this sample

Individual fields within a sample column can also be missing: `0/1:.:99` means
the genotype is `0/1`, depth is missing, and genotype quality is 99.

Trailing missing fields can be dropped entirely, so `0/1` is valid even when the
`FORMAT` column specifies `GT:DP:GQ`. This means parsers must handle
variable-length sample columns.

The distinction between "missing" and "homozygous reference" matters for
population statistics. If a sample has `./.`, it should be excluded from allele
frequency calculations (it contributes to neither the numerator nor
denominator). If a sample has `0/0`, it actively contributes reference alleles.
This distinction is why `AN` (allele number) in the INFO field counts only
non-missing alleles, not `2 * total_samples`.

### Minor allele frequency (MAF)

The minor allele frequency is the frequency of the less common allele at a
biallelic site. If the `ALT` allele frequency is 0.3, the `REF` frequency is
0.7, and the MAF is 0.3. If the `ALT` frequency is 0.8, the MAF is 0.2 (the
`REF` is the minor allele).

MAF is distinct from `AF` (alternate allele frequency): `AF` is always relative
to the alternate allele, while MAF is always the smaller of the two frequencies.
MAF is always <= 0.5.

MAF is used as a filter in many analyses:

- **Common variants** (MAF >= 0.01 or 0.05): Used for GWAS, PCA, LD
  calculations. Common variants are well-powered for association testing.
- **Low-frequency variants** (0.001 <= MAF < 0.01): May require larger sample
  sizes for association testing, but can harbor moderate-effect variants.
- **Rare variants** (MAF < 0.001): Often analyzed with burden tests or
  collapsing methods that aggregate multiple rare variants per gene, because
  individual rare variants have insufficient statistical power.
- **Singletons** (observed in only one sample, i.e. AC=1): The rarest possible
  observed variant. Singleton count is a QC metric — an excess of singletons in
  one sample may indicate contamination or sequencing error.

For multi-allelic sites, MAF is less well-defined. Some tools compute it per
`ALT` allele; others skip multi-allelic sites entirely when computing MAF-based
statistics.

### Genotype uncertainty: the PL, GL, and GP FORMAT fields

The `GT` field gives a hard call — the caller's best guess at the genotype. But
callers also quantify their uncertainty, and this uncertainty is encoded in
additional FORMAT fields.

**PL (phred-scaled genotype likelihoods)**: For each possible genotype at a
site, the PL field gives a phred-scaled likelihood — how well the observed reads
support that genotype. Lower values mean better support. The called genotype
always has PL=0, and other genotypes have positive values indicating how much
worse they fit.

For a biallelic diploid site, there are three possible genotypes (`0/0`, `0/1`,
`1/1`), so PL has three values:

```
GT:PL  0/1:120,0,90
```

This says: the called genotype is `0/1` (PL=0), the likelihood for `0/0` is
phred 120 (very unlikely), and for `1/1` is phred 90 (also unlikely but less
so). A PL difference of 30 corresponds to a 1000-fold likelihood difference.

A low PL difference between the called genotype and the next-best genotype
indicates an uncertain call. For example, `PL=0,3,50` means `0/0` was called but
`0/1` is almost as likely (only a 2-fold likelihood difference).

**GL (genotype log-likelihoods)**: The same information as PL but stored as
log10 likelihoods (negative floats) rather than phred-scaled integers.
`GL=-12.0,0.0,-9.0` is equivalent to `PL=120,0,90`. GL is less common in modern
VCFs but appears in older files and in some population genetics tools.

**GP (genotype posterior probabilities)**: While PL/GL are likelihoods (they
only consider the read data), GP incorporates prior information (e.g. allele
frequencies from a population) to produce posterior probabilities. GP values are
phred-scaled probabilities that sum to 1 (before scaling). GP is commonly output
by imputation tools, where the prior is the haplotype reference panel.

**GQ (genotype quality)**: A single number summarizing confidence in the called
genotype. It is typically the second-lowest PL value (i.e. the PL of the
second-best genotype). GQ=99 means the second-best genotype is at least 1
billion times less likely. GQ is capped at 99 in many callers.

These uncertainty fields matter for:

- **Low-coverage sequencing**: With 4x coverage, many sites have genuinely
  uncertain genotypes. Using hard calls loses information; using PLs or dosages
  preserves it.
- **Imputation**: Imputation tools output GP/DS (dosage) to express the
  uncertainty of imputed genotypes.
- **Rare variant association**: Tests that account for genotype uncertainty
  (using PLs or dosages) have more statistical power than tests on hard calls,
  especially at low-coverage or low-quality sites.

## Variant representation in depth

### SNPs

A SNP (single nucleotide polymorphism) is the simplest variant: a single-base
`REF` and a single-base `ALT`.

```
chr1  100  .  A  T  .  PASS  .
```

There are 12 possible single-nucleotide changes (A>T, A>C, A>G, T>A, T>C, T>G,
etc.), which reduce to 6 mutation classes when you account for complementary
strands (e.g. A>T on the forward strand is T>A on the reverse). These are
commonly grouped as transitions (purine-to-purine or pyrimidine-to-pyrimidine:
A<>G, C<>T) and transversions (all others). The transition/transversion ratio
(Ti/Tv) is a useful QC metric: for whole-genome human data it is typically
around 2.0-2.1, and for exome data around 2.8-3.3. A Ti/Tv ratio significantly
lower than expected suggests an excess of false positive calls.

Multi-allelic SNPs (e.g. `A` to `T,C`) are possible but rare in practice for
biallelic organisms. They are more common at hypermutable positions or when
merging calls across large populations.

### Indels

An insertion adds bases relative to the reference. In VCF, the `REF` includes an
anchor base (also called a padding base):

```
chr1  100  .  A  ATCG  .  PASS  .
```

This represents a 3bp insertion of `TCG` after position 100. The anchor base `A`
must match the reference at that position. The anchor base is necessary because
VCF requires `REF` and `ALT` to share at least one base — a pure insertion with
an empty `REF` is not valid.

A deletion removes bases:

```
chr1  100  .  ATCG  A  .  PASS  .
```

This represents a 3bp deletion of `TCG` starting after position 100.

The anchor base convention means that the `POS` field for an indel does not
point directly at the inserted or deleted sequence — it points at the anchor
base before it. This is a source of confusion when converting between formats.
BED format, for example, uses 0-based half-open coordinates, so a 3bp deletion
at VCF position 100 (1-based, anchor base) would be `chr1 100 103` in BED
(0-based, spanning the deleted bases, not the anchor).

Indels are harder to call than SNPs because:

- They are more prone to representation ambiguity (see normalization below)
- Sequencing errors involving homopolymer runs can mimic small indels
- Short-read aligners may misalign reads around indels, especially at tandem
  repeats
- The local realignment step in callers like GATK HaplotypeCaller exists
  specifically to address this

Complex indels where both `REF` and `ALT` have multiple bases and differ in
length are sometimes called "complex substitutions":

```
chr1  100  .  ACG  TTAA  .  PASS  .
```

This is a simultaneous deletion of `CG` and insertion of `TAA` (relative to the
anchor `A`). Some tools decompose these into separate indels and SNPs; others
leave them as-is.

### Variant normalization

The same variant can be represented in multiple ways in VCF. Consider a deletion
of `A` in a poly-A run `AAAA` at positions 100-103:

```
chr1  100  .  AA  A   .  PASS  .
chr1  101  .  AA  A   .  PASS  .
chr1  102  .  AA  A   .  PASS  .
```

All three represent the same biological event. This ambiguity causes problems
when comparing variant calls from different tools.

Normalization addresses this by applying two rules:

1. **Left-align**: Shift the variant as far left as possible in the reference
2. **Trim**: Remove redundant bases from `REF` and `ALT` (while keeping at least
   one anchor base for indels)

Left-alignment requires access to the reference genome, because you need to know
the surrounding sequence to determine how far left the variant can shift. This
is why `bcftools norm` and `vt normalize` require a `-f reference.fa` argument.

When comparing VCFs from different callers (e.g. for benchmarking),
normalization is essential.

Note that normalization against the same reference genome is necessary but may
not be sufficient for variant comparison. Consider two VCFs that call the same
haplotype but decompose it differently:

```
# Caller A: one MNP
chr1  100  .  ACG  TTA  .  PASS  .  GT  1/1

# Caller B: three SNPs
chr1  100  .  A  T  .  PASS  .  GT  1/1
chr1  101  .  C  T  .  PASS  .  GT  1/1
chr1  102  .  G  A  .  PASS  .  GT  1/1
```

Both represent the same derived sequence. Normalization alone cannot reconcile
these. Tools like [`vcfeval`](https://github.com/RealTimeGenomics/rtg-tools)
from RTG Tools and [`hap.py`](https://github.com/Illumina/hap.py) perform
haplotype-aware comparison, which reconstructs the actual haplotype sequences
and compares those instead of individual variant records.

### MNPs (multi-nucleotide polymorphisms)

An MNP is a substitution of multiple adjacent bases:

```
chr1  100  .  ACG  TTA  .  PASS  .
```

Some callers emit these as separate SNPs, others as a single MNP. This matters
for protein-level interpretation: two adjacent SNPs in the same codon might
change the amino acid differently depending on whether they are on the same
haplotype (cis) or different haplotypes (trans). An MNP representation
implicitly asserts they are in cis.

Whether a caller produces MNPs or separate SNPs depends on the caller's local
assembly or haplotype construction. MNPs can be split into individual SNPs or
merged from adjacent SNPs during normalization.

### Spanning deletions and the `*` allele

When a site overlaps a deletion called at an upstream position, the `*` allele
is used to represent the "this base was already deleted" state. This arises in
joint-called VCFs:

```
chr1  100  .  ACGT  A     .  PASS  .  0/1
chr1  101  .  C     T,*   .  PASS  .  1/2
```

At position 101, the sample has genotype `1/2`: one haplotype carries the `T`
SNP, and the other haplotype carries `*`, meaning it was deleted by the upstream
4bp-to-1bp deletion. The `*` allele signals "this base is absent due to an
overlapping deletion".

Without the `*` allele, there would be no way to represent a sample that is
heterozygous for an upstream deletion and a SNP at an overlapping position. The
`*` preserves the diploid genotype model: the sample still has two alleles at
position 101, but one of them is "deleted".

This representation is difficult to work with and many tools simply ignore `*`
alleles or decompose the records to avoid them. When decomposing multi-allelic
sites into biallelic records, the `*` allele record is typically dropped.

### The `<*>` allele in gVCF

The `<*>` (or `<NON_REF>`) symbolic allele appears in gVCF files. It represents
"any possible non-reference allele" and is used at variant sites in gVCFs to
capture evidence for alleles not explicitly listed:

```
chr1  100  .  A  T,<*>  .  .  .  GT:PL  0/1:0,30,300,30,300,300
```

Here the site has an explicit `ALT` allele `T`, plus `<*>` representing all
other possible non-reference alleles. The `PL` field contains likelihoods for
all genotype combinations including `<*>`: `0/0`, `0/1`, `1/1`, `0/<*>`,
`1/<*>`, `<*>/<*>`.

The `<*>` allele exists because gVCFs are produced per-sample before joint
calling. At the time of single-sample calling, the caller does not know what
alternate alleles other samples might have at this position. The `<*>` captures
the likelihood that the sample could carry some unknown non-reference allele,
which the joint caller uses when combining samples.

After joint calling, `<*>` alleles are resolved into concrete alleles or
dropped. They should not appear in a final multi-sample VCF.

The difference between `*` and `<*>`:

- `*` means "this base is deleted by a known upstream deletion" (a concrete
  structural state)
- `<*>` means "any non-reference allele not otherwise listed" (a placeholder for
  unknown variation)

## gVCF and joint calling

### What is a gVCF?

A gVCF (genomic VCF) contains information about every position in the genome,
not just variant sites. Non-variant regions are represented as reference blocks
with a symbolic `<NON_REF>` allele (GATK convention) or as explicit reference
records.

```
chr1  100  .  A  <NON_REF>  .  .  END=200  GT:DP:GQ  0/0:30:99
```

This says positions 100-200 are homozygous reference with depth 30 and genotype
quality 99. The `END` INFO field defines the span.

gVCFs are an intermediate format. They are produced per-sample by variant
callers, and are then combined across samples in "joint calling" to produce a
multi-sample VCF with genotypes for all samples at every variant site.

### Why joint calling?

Single-sample calling has a fundamental limitation: if a sample is homozygous
reference at a site, you cannot distinguish "no variant was called" from "the
site was not examined" (e.g. due to low coverage). gVCF + joint calling solves
this because the gVCF explicitly records reference-confident regions, so the
joint caller knows which samples had data at each site.

Joint calling also enables better genotyping at multi-allelic sites by
considering all samples simultaneously.

### What gVCF reference blocks look like

A gVCF alternates between variant records (which look like normal VCF) and
reference blocks (which cover non-variant stretches):

```
chr1  100  .  A  T       .  .  .              GT:DP:GQ  0/1:30:99
chr1  101  .  C  <NON_REF>  .  .  END=500    GT:DP:GQ  0/0:25:40
chr1  501  .  G  A       .  .  .              GT:DP:GQ  1/1:28:85
chr1  502  .  T  <NON_REF>  .  .  END=1000   GT:DP:GQ  0/0:22:60
```

The `GQ` (genotype quality) on reference blocks represents the minimum
confidence across the spanned region. Callers typically break reference blocks
when `GQ` or `DP` drops below certain thresholds, so a region of low coverage
might be split into multiple blocks with different qualities. This means the
size and boundaries of reference blocks are not biologically meaningful — they
are artifacts of the confidence thresholds used during calling.

gVCF files are much larger than standard VCF files because they contain records
for the entire genome, not just variant sites. A single whole-genome gVCF is
typically 1-5 GB compressed, compared to a few hundred MB for the variant-only
VCF from the same sample.

## Large-scale VCF: parsing and optimization

### Why VCF parsing is slow

A VCF line for a cohort of 100,000 samples at a biallelic site contains at
minimum 100,000 genotype values. If the `FORMAT` field is `GT:DP:GQ:PL`, each
sample has 4 colon-separated fields, and the entire line might be several
megabytes of text.

Parsing this naively (split by tab, then for each sample split by colon, then
parse each field) is expensive. The string splitting and number parsing
dominates runtime.

### BCF binary encoding

BCF avoids the text parsing bottleneck. Genotypes are stored as arrays of
integers, depths as arrays of integers, etc. Reading 100,000 genotypes from BCF
is a single read of a packed integer array, rather than parsing 100,000 strings.

For large cohorts, always use BCF.

### Genotype encoding in BCF

In BCF, the `GT` field is encoded as an array of small integers. Each allele
value is stored as `(allele_index + 1) << 1 | phased_bit`. So allele 0
(reference) unphased is `(0+1) << 1 | 0 = 2`, allele 1 unphased is
`(1+1) << 1 | 0 = 4`, and allele 1 phased is `(1+1) << 1 | 1 = 5`. Missing
allele is encoded as 0. This encoding packs both the allele index and phase into
a single integer per allele.

This means that a diploid genotype `0/1` is stored as two bytes: `[2, 4]`, and
`0|1` is stored as `[2, 5]`. Libraries like `htslib` expose functions that
decode this, but knowing the encoding helps when writing high-performance
parsers or debugging malformed files.

### Lazy field parsing

Most operations on a VCF file only need a subset of the fields. A parser that
reads genotypes does not need to parse the `INFO` column, and vice versa.
Efficient VCF/BCF libraries (like `htslib`, `cyvcf2`, `noodles`) support lazy
parsing: they read the raw bytes/text of each field but defer parsing until the
field is accessed. For BCF, `htslib` can skip over fields entirely because each
field has a known byte length.

This matters when you only need, say, the `GT` field from a file with
`GT:AD:DP:GQ:PL`. Parsing the `PL` (phred-scaled genotype likelihoods) field for
100,000 samples is expensive and wasteful if you only need genotypes.

### Region queries with tabix and CSI indices

Like BAM files, VCF/BCF files can be indexed for random access by genomic
region.

- **tabix** (`.tbi`): Uses a binning scheme similar to BAM's `.bai` index. Works
  for both VCF and BCF.
- **CSI** (`.csi`): A more general coordinate-sorted index that supports longer
  chromosomes (>2^29 bp). Created with `bcftools index`.

```sh
bcftools view -r chr1:1000000-2000000 input.bcf
```

Without an index, this query requires scanning the entire file.

## VCF for population genetics

### Allele frequency and the AC/AN/AF INFO fields

VCF files from population studies typically include:

- `AC` (allele count): Number of alternate alleles observed across all samples
- `AN` (allele number): Total number of alleles called (2 \* number of diploid
  samples with non-missing genotypes)
- `AF` (allele frequency): `AC / AN`

These are site-level summaries in the `INFO` column. They can be recalculated
from the genotypes with `bcftools +fill-tags -- -t AC,AN,AF`, which is necessary
if you have subsetted samples from a larger callset.

### Computing basic population statistics from VCF

Given a VCF with genotypes, you can compute:

- **Allele frequencies**: Count alleles from `GT` fields, or use the `AF` INFO
  field
- **Hardy-Weinberg equilibrium**: Compare observed genotype frequencies
  (hom-ref, het, hom-alt) to expected frequencies under HWE. Deviations can
  indicate genotyping error, population structure, or selection.
- **Nucleotide diversity (pi)**: Average number of pairwise differences per
  site.
- **Fixation index (Fst)**: Measures genetic differentiation between
  populations. Requires population labels for each sample.

### Linkage disequilibrium

LD (the non-random association of alleles at different sites) is computed from
phased or unphased genotypes in VCF. LD is foundational for GWAS (identifying
tag SNPs), imputation (filling in missing genotypes from reference panels), and
understanding recombination.

### Principal component analysis (PCA)

PCA on genotype data reveals population structure. The typical workflow is to
filter the VCF to biallelic SNPs, remove rare variants (e.g. MAF < 0.01), and
prune for LD (to avoid correlated SNPs dominating the PCs), then compute
principal components from the genotype matrix.

The first few principal components typically separate continental-level ancestry
groups. PCA is routinely used as both a quality control step (detecting
mislabeled samples or outliers) and as covariates in GWAS to correct for
population stratification.

### Admixture analysis

Admixture methods estimate ancestry proportions from VCF genotypes. The input is
typically a pruned set of biallelic SNPs (similar to PCA input). These methods
model each individual as a mixture of K ancestral populations, producing the
familiar stacked bar plots of ancestry proportions.

### Detecting natural selection from VCF

Genotype data from VCFs can reveal signatures of natural selection — where
certain variants or regions were favored or disfavored by evolutionary
pressures. Different types of selection leave different patterns:

**Tajima's D**: Compares two estimates of genetic diversity that should be equal
under neutral evolution (no selection). One estimate uses the number of
segregating sites (positions where at least one sample differs from the
reference), the other uses the average pairwise differences (pi). When a
beneficial variant recently swept through a population (a "selective sweep"), it
drags nearby variants to high frequency, leaving a characteristic deficit of
rare variants. This produces a negative Tajima's D. Conversely, balancing
selection (where multiple alleles are maintained, as at immune system genes)
produces an excess of intermediate-frequency variants and a positive Tajima's D.

Tajima's D is computed in windows across the genome from allele frequency data
in the VCF. Extreme values (in either direction) in a genomic window are
candidates for selection.

**iHS (integrated haplotype score)**: Detects ongoing positive selection (where
a beneficial allele is increasing in frequency but has not yet fixed). It uses
phased haplotypes from VCF and measures how far a haplotype extends without
being broken up by recombination. A variant under selection will sit on a long,
intact haplotype (because selection drove it up in frequency faster than
recombination could break it apart). iHS compares the haplotype lengths of the
ancestral and derived alleles — if the derived allele has an unusually long
intact haplotype, it suggests recent positive selection.

**Fst outliers**: As described in the Fst section, genomic regions with
unusually high Fst between populations are candidates for local adaptation. The
skin pigmentation gene SLC24A5, for example, shows extreme Fst between European
and African populations because it was under strong selection in European
populations.

### Relatedness and identity by descent (IBD)

Given a VCF with genotypes for multiple samples, you can estimate how related
any two individuals are. This is useful both as a QC step (detecting unexpected
relatives or duplicate samples in a study) and as a research tool.

**Pairwise kinship**: The kinship coefficient between two individuals is
estimated by comparing their genotypes across all SNPs. At each site, you ask:
do these individuals share 0, 1, or 2 alleles identical by state (IBS, meaning
the alleles are the same, regardless of whether they were inherited from the
same ancestor)? The pattern of sharing across many sites estimates the kinship:

- Unrelated: kinship ~0
- Parent-child or full siblings: kinship ~0.25
- Half-siblings, uncle-nephew: kinship ~0.125
- First cousins: kinship ~0.0625
- Identical twins or duplicate samples: kinship ~0.5

**IBD segments**: Identity by descent means two individuals inherited the same
chromosome segment from the same recent common ancestor. IBD segments are
detected from phased VCFs by looking for long stretches where two haplotypes are
identical. Short shared segments are common (everyone shares distant ancestors),
but long IBD segments (>5 cM) indicate recent common ancestry.

IBD detection has practical applications:

- **QC**: Finding duplicate samples, mislabeled samples, or unexpected family
  relationships in a study cohort
- **IBD mapping**: In genetic disease studies, affected individuals from the
  same population may share an IBD segment around the causal variant, inherited
  from a common founder. Identifying the shared segment narrows the search.
- **Population genetics**: The distribution of IBD segment lengths across a
  population reveals demographic history — recent bottlenecks produce many long
  IBD segments.

### Local ancestry and chromosome painting

PCA and admixture give you a genome-wide summary: "this individual is 70%
European and 30% African ancestry." But ancestry is not uniform across the
genome. Due to recombination (the shuffling of chromosomes that happens each
generation), different segments of a single chromosome may trace back to
different ancestral populations. Local ancestry inference determines, for each
segment of each chromosome, which population it came from.

The input is a phased VCF (the `|` separator matters here — you need to know
which alleles are on the same physical chromosome). The method slides along each
haplotype, comparing the pattern of alleles in each window to reference panels
from known populations, and assigns the most likely ancestral origin to each
segment.

The output is sometimes called a "chromosome painting": a visualization where
each chromosome is colored by ancestry, segment by segment. For an admixed
individual (someone with mixed ancestry), the painting shows the mosaic of
ancestral blocks — long blocks indicate recent admixture (few generations of
recombination to break them up), short blocks indicate ancient admixture (many
generations of recombination).

```
Chromosome 1, individual X:
|---European---|---African---|---European---|---African---|---European---|
```

Local ancestry has practical applications:

- **Admixture mapping**: Testing whether a disease is more common on chromosome
  segments from a particular ancestry, which can localize disease genes
- **Understanding population history**: The length distribution of ancestry
  tracts (unbroken segments from one population) tells you when admixture
  occurred — longer tracts mean more recent mixing
- **Correcting for ancestry in association studies**: Instead of correcting with
  genome-wide PCA (a single ancestry estimate per individual), you can correct
  for local ancestry at each tested variant

### The ancestral recombination graph (ARG)

The ancestral recombination graph is a data structure that represents the full
genealogical history of a set of sampled genomes, including both coalescence
(where two lineages merge going back in time, meaning they share a common
ancestor) and recombination (where a single ancestral chromosome is split into
two segments that have different histories).

A standard phylogenetic tree assumes one tree for the whole genome. But
recombination means that different positions along the genome have different
genealogical trees. The ARG encodes this: it is a sequence of correlated trees
along the chromosome, where adjacent trees differ by one recombination event (a
branch detaches from one place in the tree and reattaches to another).

The ARG is sometimes described as "the ultimate data structure for population
genetics" because in principle, if you knew the true ARG, you could derive
almost any population genetic statistic from it: relatedness, selection,
demography, migration, local ancestry — these are all projections of the ARG.

The connection to VCF: an ARG is inferred from phased genotype data (a phased
VCF). Methods that infer ARGs take the haplotypes from VCF and reconstruct the
genealogical history that is most consistent with the observed pattern of
variants. The ARG is not stored in VCF — it is typically stored in tree sequence
format (`.trees` files) — but VCF is the starting point from which it is
inferred.

ARG inference is computationally intensive and has only recently become
practical for large datasets. The resulting tree sequences are compact and
enable fast computation of population genetic statistics that would be slow to
compute directly from VCF genotypes, because traversing a tree is faster than
recomputing statistics from a genotype matrix.

### Detecting introgression from VCF

Introgression is gene flow between species (or diverged populations) through
hybridization — for example, Neanderthal DNA that entered the modern human gene
pool through ancient interbreeding, or wild species DNA that crossed into
domesticated crop varieties.

Introgression leaves a distinctive signature in genotype data: segments of the
genome in one species that look like they belong to another species. These
introgressed segments have unusually high genetic similarity to the donor
species and unusually low similarity to the rest of the recipient species.

**The ABBA-BABA test (D-statistic)**: The most widely used test for
introgression operates directly on VCF genotype data. It requires four
populations: P1 and P2 (two closely related populations, one of which may have
received introgression), P3 (the candidate donor), and an outgroup O. At each
biallelic SNP, you classify the site by its allele pattern:

```
         P1  P2  P3  O
ABBA:     A   B   B   A    (P2 shares derived allele with P3)
BABA:     B   A   B   A    (P1 shares derived allele with P3)
```

Where `A` = ancestral allele (matching the outgroup) and `B` = derived allele.
Under no introgression, ABBA and BABA sites should be equally frequent (by
incomplete lineage sorting — random sorting of ancestral variation). An excess
of ABBA sites suggests gene flow from P3 into P2; an excess of BABA suggests
gene flow from P3 into P1.

The D-statistic is:

```
D = (count_ABBA - count_BABA) / (count_ABBA + count_BABA)
```

D = 0 means no evidence of introgression. D significantly different from 0
(tested by block jackknife) suggests gene flow.

Computing this from VCF is straightforward: for each biallelic SNP, extract the
allele frequencies (or a single representative genotype) for each population,
classify the site as ABBA or BABA based on which populations carry the derived
allele, and count. The entire calculation operates on the 0/1/2 genotype matrix.

**Windowed divergence and f-statistics**: To find which specific genomic regions
are introgressed, you can compute the D-statistic (or the related f_d statistic)
in sliding windows along the genome. Windows with extreme values indicate
candidate introgressed regions. These regions can then be examined for their
gene content — for example, Neanderthal-introgressed regions in humans are
enriched for genes related to immune function and skin/hair keratins.

**Local ancestry for introgression**: The chromosome painting methods described
above can also detect introgression by "painting" chromosomes of one species
using reference panels from another. Segments that paint as the donor species
are candidate introgressed tracts. The length distribution of these tracts
estimates when the introgression occurred, just as with within-species
admixture.

### How these methods relate to each other

These population genetics methods form a hierarchy of increasing resolution:

- **PCA / Admixture**: One ancestry estimate per individual (genome-wide
  average). Input: unphased or phased VCF genotype matrix.
- **Chromosome painting / Local ancestry**: One ancestry estimate per chromosome
  segment per individual. Input: phased VCF + reference panels.
- **ARG inference**: A full genealogical tree per genomic position for all
  individuals. Input: phased VCF.

Each level requires more computational effort and more assumptions, but reveals
more about the history encoded in the genotype data. All start from the same
VCF.

## Mapping genotypes to traits

### The genotype matrix: the starting point for everything

Regardless of whether you are doing GWAS, QTL mapping, or genomic prediction,
the fundamental input is the same: a matrix of genotypes extracted from VCF.
Each row is a variant (site), each column is a sample, and each cell is coded as
0 (homozygous reference), 1 (heterozygous), or 2 (homozygous alternate).

```
         Sample1  Sample2  Sample3  Sample4
chr1:100    0        1        0        2
chr1:200    1        1        2        0
chr1:300    0        0        1        1
chr2:500    2        1        0        0
```

This matrix is derived directly from the `GT` field across all samples in the
VCF. Every downstream analysis — association testing, relationship estimation,
breeding value prediction — is a computation on this matrix paired with
phenotype data (measurements of traits like height, disease status, yield,
etc.).

### GWAS (genome-wide association studies)

GWAS tests for association between each variant in a VCF and a phenotype
(disease status, height, blood pressure, etc.). For a biallelic SNP with
genotypes coded as 0, 1, or 2, the simplest test is a linear or logistic
regression:

```
phenotype ~ genotype + covariates
```

This is repeated independently for every variant in the VCF (often millions of
sites), producing a p-value per site. The result is typically visualized as a
Manhattan plot: genomic position on the x-axis, -log10(p-value) on the y-axis,
with significant associations appearing as peaks.

GWAS is performed on populations of unrelated (or distantly related)
individuals. It leverages the historical recombination that has accumulated over
many generations in a natural population, which gives fine mapping resolution —
a significant signal can sometimes be narrowed to a single gene or even a single
causal variant.

### QTL mapping (quantitative trait loci)

QTL mapping uses a controlled cross instead of a natural population. You cross
two parents that differ in a trait (e.g. a tall plant x a short plant, or a
disease-resistant line x a susceptible line), then genotype and phenotype the
offspring.

The genotype matrix for a QTL study looks different from a GWAS matrix. In a
biparental cross, every site is biallelic by definition (parent A allele vs
parent B allele), and the genotypes track which parent contributed which allele:

```
         Offspring1  Offspring2  Offspring3  Offspring4
marker1     A           B           A           H
marker2     H           A           B           B
marker3     A           H           H           A
```

Where `A` = homozygous for parent A allele, `B` = homozygous for parent B
allele, `H` = heterozygous. In VCF terms, if parent A is the reference, these
map to `0/0`, `1/1`, and `0/1`.

QTL mapping tests whether offspring that inherited a particular parental allele
at a marker tend to have higher or lower trait values. Because the cross has
limited recombination events (only a few generations), QTL mapping identifies
broad genomic regions (often several megabases) rather than individual variants.

### GWAS vs QTL mapping: the tradeoff

|                      | GWAS                              | QTL mapping                                  |
| -------------------- | --------------------------------- | -------------------------------------------- |
| Population           | Unrelated individuals             | Offspring from a controlled cross            |
| Resolution           | Fine (kb-scale)                   | Coarse (Mb-scale)                            |
| Alleles tested       | All segregating in the population | Only those differing between the two parents |
| Sample size needed   | Large (thousands+)                | Moderate (hundreds)                          |
| Population structure | Must be corrected for             | Not a concern (cross is the structure)       |
| Common in            | Human genetics                    | Plant/animal breeding, model organisms       |

Both produce the same type of result: a set of genomic regions associated with a
trait. The difference is resolution, study design, and which alleles you can
detect. Some breeding programs use both: QTL mapping to find a broad region,
then GWAS or fine-mapping in a diverse population to narrow it down.

### Quality control before GWAS

Before running association tests, extensive QC is performed on the VCF:

- **Sample-level QC**: Remove samples with high missingness (too many no-calls),
  extreme heterozygosity (abnormal ratio of het to hom calls, which can indicate
  contamination), sex mismatches, or relatedness (detected from genotypes)
- **Variant-level QC**: Remove variants with high missingness, low MAF,
  Hardy-Weinberg disequilibrium (in controls), or poor imputation quality
- **Population stratification** (systematic ancestry differences between cases
  and controls): PCA to detect and correct for this

These steps are critical. Failure to perform adequate QC leads to false
associations.

### Imputation and the DS FORMAT field

Most GWAS arrays genotype only a subset of variants (~500K-2M sites). Imputation
fills in the gaps using a reference panel (e.g. 1000 Genomes, TOPMed, or the
Haplotype Reference Consortium). The process involves phasing the genotyped
variants, then imputing ungenotyped sites by leveraging LD structure in the
reference panel.

The output of imputation is a VCF with the `DS` (dosage) FORMAT field and an
imputation quality metric (often `R2` in the INFO field). Imputed dosages are
continuous values between 0 and 2, representing the expected alternate allele
count. A dosage of 1.8 means the imputation is fairly confident the sample is
homozygous alt, but not certain. GWAS association tests can use dosages directly
rather than hard-called genotypes, which properly accounts for imputation
uncertainty.

### Imputation beyond human GWAS

Imputation is not limited to human genetics. In plant and animal breeding,
genotyping is often done with low-density SNP arrays (thousands of markers
instead of millions) because it is cheaper per individual, and large numbers of
individuals need to be genotyped. These sparse genotypes are then imputed up to
a higher-density marker set using a reference panel of individuals genotyped at
higher density.

The accuracy of imputation depends on the LD structure of the population.
Populations with long LD blocks (e.g. inbred crop lines, livestock breeds with
small effective population sizes) can be imputed accurately from fewer markers
than outbred populations with short LD blocks.

## Genomic prediction and breeding values

### The genomic relationship matrix (GRM)

A central use of VCF genotype data outside of GWAS is constructing a genomic
relationship matrix (GRM). The GRM captures the genetic similarity between every
pair of individuals in a dataset, computed from genome-wide marker data.
Conceptually, for each pair of individuals, you ask: across all genotyped SNPs,
how similar are their genotypes?

The GRM replaces pedigree-based (family tree-based) estimates of relatedness
with empirically measured genomic similarity. Two full siblings share on average
50% of their genome by pedigree, but the actual sharing varies — the GRM
captures this realized sharing rather than the expected average.

### GBLUP (genomic best linear unbiased prediction)

GBLUP uses the GRM to predict breeding values (an individual's genetic merit for
a trait) from genotype data. The model is:

```
phenotype = mean + genomic_breeding_value + residual
```

where the genomic breeding values are drawn from a distribution whose covariance
structure is defined by the GRM. Individuals that are more genetically similar
(according to the GRM) are expected to have more similar breeding values.

GBLUP is the workhorse of modern genomic selection in plant and animal breeding.
It enables prediction of an individual's genetic merit before the trait is
measured — for example, predicting the milk yield of a bull's daughters before
any daughters are born, or predicting grain yield of a crop line before field
trials. This accelerates breeding cycles significantly.

The input to GBLUP is the same VCF genotype data used for GWAS: a matrix of
individuals x SNPs, coded as 0/1/2 (counts of alternate alleles). The difference
is in what you do with it: GWAS tests each SNP individually for association with
a trait, while GBLUP fits all SNPs simultaneously to predict overall genetic
merit.

### Genomic selection in practice

In livestock and crop breeding programs, the typical workflow is:

1. Genotype a training population (individuals with both genotype and phenotype
   data) using SNP arrays, producing VCF files
2. Build a GRM from the genotypes
3. Fit GBLUP (or a related model) to estimate SNP effects and predict breeding
   values
4. Genotype selection candidates (young animals or untested plant lines) using
   the same or a lower-density array
5. Impute up to the training panel density if needed
6. Predict breeding values for the selection candidates using the trained model
7. Select the best candidates for breeding

This cycle relies on VCF as the data interchange format at multiple steps. The
genotype data flows from arrays or sequencing, through VCF, into the statistical
models.

### Beyond GBLUP

More complex models extend GBLUP in various ways:

- **BayesA, BayesB, BayesC**: Allow different SNPs to have different effect size
  distributions, unlike GBLUP which assumes all SNPs have equal variance. These
  can perform better when a few large-effect loci exist.
- **Single-step GBLUP (ssGBLUP)**: Combines pedigree information (for
  non-genotyped individuals) with genomic information (for genotyped
  individuals) into a single analysis. This is important in livestock where
  historical phenotype data exists for non-genotyped ancestors.
- **Multi-trait models**: Fit multiple correlated traits simultaneously,
  leveraging genetic correlations to improve prediction accuracy.

These methods all consume the same genotype data from VCF, differing only in
their statistical assumptions about how genetic effects are distributed across
the genome.

## Structural variants in VCF

### The gray zone between indels and SVs

There is no hard boundary between a large indel and a structural variant. By
convention, variants >= 50bp are often called "structural variants", but this is
arbitrary. VCF supports both representations:

- A 45bp deletion can be written with literal `REF`/`ALT` strings (45 reference
  bases + anchor vs 1 anchor base)
- A 50kb deletion is impractical to write literally, so symbolic notation is
  used

Some callers produce literal `REF`/`ALT` for variants up to several hundred base
pairs, while others switch to symbolic notation at 50bp. This inconsistency
between callers means that SV comparison tools must handle both representations.

### Symbolic ALT alleles

Large structural variants use symbolic `ALT` alleles enclosed in angle brackets:

```
chr1  1000  .  N  <DEL>  .  PASS  SVTYPE=DEL;END=51000;SVLEN=-50000
```

The standard symbolic alleles are:

- `<DEL>` — deletion. The region from `POS` to `END` is deleted. `SVLEN` is
  negative.
- `<INS>` — insertion. Sequence is inserted at `POS`. The inserted sequence
  itself may be in the `INFO/SEQ` field, or may not be specified at all. `SVLEN`
  is positive.
- `<DUP>` — duplication. The region from `POS` to `END` is duplicated.
  `<DUP:TANDEM>` specifies tandem duplication.
- `<INV>` — inversion. The region from `POS` to `END` is inverted.
- `<CNV>` — copy number variant. The `CN` FORMAT field specifies the copy number
  per sample.

Key INFO fields for SVs:

- `END` — end position of the variant on the reference
- `SVTYPE` — type of structural variant
- `SVLEN` — length difference between REF and ALT alleles (negative for
  deletions)
- `CIPOS` — confidence interval around `POS` (e.g. `CIPOS=-10,10` means the true
  breakpoint is within 10bp of the reported position)
- `CIEND` — confidence interval around `END`

The confidence intervals (`CIPOS`, `CIEND`) are important for SV comparison. Two
callers might call the same deletion with breakpoints differing by 5-20bp. SV
comparison tools like `truvari` use a combination of position tolerance, size
similarity, and optionally sequence similarity to match calls.

The genotype field for SVs follows the same conventions as for small variants
(`0/0`, `0/1`, `1/1`, etc.), though some callers add SV-specific FORMAT fields
like `CN` (copy number) or `DV` (number of variant-supporting reads).

### The insertion problem

Insertions are the hardest SV type to represent in VCF because VCF is
fundamentally reference-coordinate-based. A deletion removes reference sequence,
so it has clear `POS` and `END` coordinates on the reference. An insertion adds
novel sequence that has no reference coordinates.

For a `<INS>`, the `END` field equals `POS` (the insertion is a point event on
the reference), and the inserted sequence may or may not be provided. Some
callers put the full inserted sequence in `ALT` (as a literal string), some put
it in an `INFO/SEQ` field, and some omit it entirely, providing only the
insertion length. This makes insertion comparison across callers particularly
difficult.

Large insertions also cannot be found by tools that only look at reference
coordinates: an insertion at `chr1:5000` occupies a single point and may not
appear in a region query for `chr1:4999-5001` depending on how the query tool
handles symbolic alleles.

### Breakends (BND)

Breakends are the most general and most complex SV representation in VCF. A
breakend describes a novel adjacency: the reference sequence is broken at a
point and joined to another location. The `ALT` field uses a special syntax:

```
chr1  1000  bnd1  A  A]chr4:2000]  .  PASS  SVTYPE=BND;MATEID=bnd2
chr4  2000  bnd2  C  C]chr1:1000]  .  PASS  SVTYPE=BND;MATEID=bnd1
```

The bracket notation encodes both the partner location and the orientation of
the join. There are four bracket patterns:

- `t[p[` — piece extending to the right of `t` is joined to piece extending to
  the right of `p`. The partner is in forward orientation.
- `t]p]` — piece extending to the right of `t` is joined to piece extending to
  the left of `p`. The partner is in reverse orientation.
- `]p]t` — piece extending to the left of `t` is joined to piece extending to
  the left of `p`. The partner is in reverse orientation.
- `[p[t` — piece extending to the left of `t` is joined to piece extending to
  the right of `p`. The partner is in forward orientation.

Where `t` is the sequence adjacent to the breakpoint (from the `REF` column) and
`p` is the partner coordinate.

### Interpreting breakends: examples

**Deletion as breakends**:

A simple deletion from `chr1:1000` to `chr1:5000` can be written as:

```
chr1  1000  del1  A  A[chr1:5000[  .  PASS  SVTYPE=BND;MATEID=del2
chr1  5000  del2  C  ]chr1:1000]C  .  PASS  SVTYPE=BND;MATEID=del1
```

This says: the sequence to the right of position 1000 joins to the sequence to
the right of position 5000. Everything between 1000 and 5000 is deleted.

**Inversion as breakends**:

An inversion of the region `chr1:1000-5000` requires four breakend records (two
pairs), because an inversion breaks the reference at two points and reconnects
them in reverse:

```
chr1  1000  inv1  A  A]chr1:5000]  .  PASS  SVTYPE=BND;MATEID=inv2
chr1  5000  inv2  C  C]chr1:1000]  .  PASS  SVTYPE=BND;MATEID=inv1
chr1  1000  inv3  A  [chr1:5000[A  .  PASS  SVTYPE=BND;MATEID=inv4
chr1  5000  inv4  C  [chr1:1000[C  .  PASS  SVTYPE=BND;MATEID=inv3
```

The first pair connects the right side of position 1000 to the left side of
position 5000 (in reverse), and the second pair connects the left side of
position 1000 to the right side of position 5000 (in reverse). Together, they
describe the inversion.

This is why breakends are complex: a single biological event (inversion)
requires 4 VCF records and careful interpretation of bracket orientations to
reconstruct. Compare this to `<INV>` which represents the same event in a single
record.

**Translocation as breakends**:

An inter-chromosomal translocation is where breakends are essential, because
symbolic alleles like `<DEL>` cannot represent inter-chromosomal events:

```
chr1  1000  tra1  A  A[chr4:2000[  .  PASS  SVTYPE=BND;MATEID=tra2
chr4  2000  tra2  G  ]chr1:1000]G  .  PASS  SVTYPE=BND;MATEID=tra1
```

### Inserted sequence at breakpoints

Breakends can include inserted sequence (bases not from either reference
location) between the brackets:

```
chr1  1000  .  A  ATTTTT]chr4:2000]  .  PASS  SVTYPE=BND
```

The `TTTTT` is novel inserted sequence at the junction. This is common in real
rearrangements due to non-templated insertion during DNA repair.

### The breakend graph

Each breakend is an edge in a graph where the nodes are reference positions with
orientations (left side or right side of a position). Reconstructing the derived
genome from a set of breakends requires:

1. Building the graph from all breakend records
2. Matching mates (using `MATEID` or `EVENT` INFO fields)
3. Traversing paths through the graph to reconstruct derived chromosomes

This is a nontrivial graph problem. The VCF spec defines the notation but does
not specify how to traverse the graph. In practice, few tools implement full
breakend graph reconstruction. Most SV analysis tools convert breakends to
simpler representations (deletion, inversion, etc.) where possible.

The `EVENT` INFO field groups breakend records that belong to the same event,
which helps when an event produces more than two breakend records (e.g. an
inversion produces four).

### The breakend interpretation gap

In practice, the breakend graph is one of the least well-supported parts of the
VCF ecosystem. Most SV callers output breakend records but immediately attempt
to simplify them: if two breakends face each other on the same chromosome with
the right orientations, label them as a deletion or inversion and move on. The
breakend notation is treated as an intermediate representation to be escaped
from, rather than a useful abstraction to reason with.

This creates a cascade of limitations. SV merging tools (which compare calls
across samples or callers) often skip or poorly handle BND records — they
operate on interval-based logic (does this deletion overlap that deletion?) and
have no framework for comparing breakend graphs. Annotation tools like VEP only
added BND support relatively recently, and even then they annotate individual
breakpoint positions without reasoning about breakend pairs or the graph they
form. The `MATEID` and `EVENT` INFO tags that link breakends into coherent
events are widely ignored by downstream tools.

The result is that complex rearrangements — the events where breakend notation
is most needed — are also the events least served by existing tools.
Chromothripsis (catastrophic shattering and reassembly of a chromosome) can
involve dozens to hundreds of breakpoints. Chromoplexy (chained rearrangements
spanning multiple chromosomes) produces breakend chains that must be traversed
as a graph to understand the derivative chromosome structure. These events are
biologically important (chromothripsis is found in over 50% of several cancer
types) but the path from "a VCF with 200 BND records" to "this is what the
derivative chromosome looks like" requires solving a nontrivial graph traversal
problem that most tools do not attempt.

One notable exception is the GRIDSS-PURPLE-LINX pipeline (Cameron et al.,
2021/2023), which takes breakend-level calls and copy number data, clusters
breakend pairs into higher-order events, and chains them together to reconstruct
derivative chromosomes. LINX classifies events as simple (DEL, DUP, INV) or
complex (chromothripsis, chromoplexy, breakage-fusion-bridge cycles), and
predicts functional consequences including gene fusions formed through
multi-breakpoint chains. GRIDSS also phases nearby breakpoints as cis or trans
(on the same or different derivative chromosomes), which constrains the graph
traversal. But this pipeline is cancer-focused and tightly coupled — using a
different SV caller means losing the interpretation layer.

The core idea behind LINX is that breakend records define an adjacency graph on
the genome, and derivative chromosome reconstruction is a path-finding problem
in that graph. LINX identifies "facing breakends" — pairs from different SVs
where the reference segment between them is carried into the derivative
chromosome as a templated insertion — and chains them together using a greedy
priority system (assembly-confirmed links first, then forced choices, adjacent
pairs, junction-copy-number-matched pairs, and nearest pairs as fallback). Copy
number data constrains which links are valid by pruning segments where the
cluster allele copy number drops to zero. The output is a set of chains, each
representing a derivative chromosome as an ordered sequence of reference
segments. For a standalone, caller-agnostic implementation of this approach, see
[derivative-chromosome-utils](https://github.com/cmdcolin/derivative-chromosome-utils).

For visualization, most genome browsers display breakends as arcs connecting two
positions, which helps for individual events but breaks down when dozens of
breakends interact. Circular genome plots (Circos-style views) can show
inter-chromosomal connections but flatten the graph into a 2D layout that
obscures the path structure. SVTopo (PacBio, 2025) is a newer tool that
specifically targets multi-breakpoint event visualization from long-read data,
reflecting growing recognition that this is an unsolved problem.

The fundamental issue is architectural: almost the entire SV tooling ecosystem
is built around interval-based thinking (this region is deleted, this region is
duplicated) rather than graph-based thinking (these two positions are now
adjacent in the sample genome). The VCF spec provides the notation for
graph-based representation through breakends, but the tools have not caught up.
For researchers working with complex rearrangements, this means a significant
amount of manual interpretation, custom scripting, and staring at breakend
records — exactly the kind of work that a mature tooling ecosystem should
automate.

### Single breakends

Sometimes only one side of a breakend is known (e.g. the partner maps to an
unplaced contig or cannot be determined). VCF represents these as single
breakends:

```
chr1  1000  .  A  .TTTTT  .  PASS  SVTYPE=BND
```

The `.` before the inserted/ALT sequence indicates that the partner location is
unknown. These are common in tumor genomes where rearrangements may involve
sequences not in the reference, such as viral insertions or highly rearranged
segments that cannot be mapped.

### SV genotyping challenges

Genotyping SVs is harder than genotyping SNPs. For a SNP, you count reads
supporting each allele at a single position. For an SV, the evidence is
distributed across the event: split reads at breakpoints, read depth changes
across the deletion, discordant read pairs spanning the event. Different
genotypers weight these signals differently.

Furthermore, the genotype model for SVs is less clean than for SNPs. Copy number
variants may not fit a diploid model (a sample might have 0, 1, 2, 3, or more
copies). Some SV callers output a `CN` FORMAT field alongside or instead of
`GT`. The relationship between `GT` and `CN` can be ambiguous: does `GT=0/1` for
a `<DUP>` mean one copy is duplicated (CN=3) or does `GT=1/1` mean both copies
are duplicated (CN=4)?

### `<TRA>` — the non-standard translocation

Some SV callers (notably Delly) use `<TRA>` as a symbolic allele for
translocations. This is not part of the VCF specification — the spec uses
breakend notation for translocations. `<TRA>` records typically have INFO fields
like `CHR2` and `POS2` to indicate the partner locus. Tools that consume SV VCFs
need to handle both `<TRA>` and breakend notation, even though only the latter
is spec-compliant.

## Representation challenges for structural variants

### Why SV representation is fundamentally harder than SNP representation

A SNP is a point event: one base at one position changes to another. The VCF
representation is unambiguous and compact. SVs break this simplicity in several
ways:

- **Size**: A 50kb deletion cannot be written as a literal `REF`/`ALT` string
  without a 50kb `REF` field. Hence symbolic alleles.
- **Breakpoint uncertainty**: SV callers often cannot determine exact
  breakpoints, so `CIPOS` and `CIEND` confidence intervals are used. Two
  representations of the "same" deletion might differ by 20bp at each end.
- **Multiple valid decompositions**: A complex rearrangement can be decomposed
  into different sets of simpler events. One representation might call an
  inversion + flanking deletions; another might call it as a single complex
  event with breakends.
- **Inserted sequence may be unknown**: For insertions, the actual inserted
  sequence may not be provided (only the length), making comparison between
  callsets difficult.

### The same event, three representations

Consider a 5kb deletion. It can appear in VCF as:

**1. Literal REF/ALT** (if the caller includes the full reference sequence):

```
chr1  1000  .  ACGT...5000bp...  A  .  PASS  .
```

**2. Symbolic allele**:

```
chr1  1000  .  A  <DEL>  .  PASS  SVTYPE=DEL;END=6000;SVLEN=-5000
```

**3. Breakend pair**:

```
chr1  1000  del1  A  A[chr1:6000[  .  PASS  SVTYPE=BND;MATEID=del2
chr1  6000  del2  C  ]chr1:1000]C  .  PASS  SVTYPE=BND;MATEID=del1
```

All three describe the same event. Any tool that compares SV callsets must
recognize all three as equivalent. This is one reason SV benchmarking requires
specialized comparison logic rather than simple position matching.

### Representation of overlapping events

When multiple SVs overlap on different haplotypes, VCF's line-oriented format
struggles. Consider a sample that is heterozygous for two different deletions
that partially overlap:

- Haplotype 1: 3kb deletion at chr1:1000-4000
- Haplotype 2: 5kb deletion at chr1:2000-7000

These cannot be cleanly represented as a single multi-allelic VCF record because
they have different `POS` and `END` values. They must be two separate records,
and the phasing information (`|` separator in `GT`) is needed to assign each
deletion to the correct haplotype. If the calls are unphased, it is ambiguous
which haplotype carries which deletion.

This gets worse with duplications and copy number variants, where a region might
have different copy numbers on each haplotype, and where copy number does not
map cleanly to the diploid `GT` model.

### The END field and its overloading

The `END` INFO field has different meanings depending on context:

- For a `<DEL>`: `END` is the last deleted position on the reference
- For a `<INS>`: `END` typically equals `POS` (the insertion is a point on the
  reference)
- For a `<DUP>`: `END` is the end of the duplicated region
- For a `<INV>`: `END` is the end of the inverted region
- In gVCF reference blocks: `END` is the last position of the reference-
  confident block

The `END` field is parsed by tabix/CSI indexing to determine the span of the
record for region queries. If `END` is missing or incorrect, region queries will
not return the record when they should. Some SV callers have historically
produced records with incorrect `END` values, causing downstream tools to
silently miss variants.

### Detecting CNVs: depth and B-allele frequency

Copy number variants can be detected from VCF data using two complementary
signals: read depth and B-allele frequency (BAF). These two signals provide
independent evidence and are most powerful when used together.

**Read depth (log R ratio)**: If a region is duplicated, sequencing reads pile
up there, producing higher-than-expected coverage. If a region is deleted, fewer
reads map there. The read depth at each position (often the `DP` FORMAT field in
VCF, or computed separately from the BAM file) is compared to the expected depth
for that region. The ratio is typically log-transformed, producing the "log R
ratio" (LRR):

- LRR near 0: normal copy number (2 copies for diploid)
- LRR shifted down (negative): deletion (fewer copies)
- LRR shifted up (positive): duplication (more copies)

Depth alone cannot distinguish the zygosity (whether a deletion or duplication
affects one or both copies).

**B-allele frequency (BAF)**: At heterozygous SNP sites within a region, the
ratio of reads supporting each allele tells you about copy number state. This is
the "B-allele frequency": the fraction of reads (or allele intensity, for
arrays) supporting the non-reference allele. In a VCF, this can be derived from
the `AD` (allelic depth) FORMAT field, which gives read counts per allele:

- **Normal diploid (2 copies)**: Heterozygous sites have BAF near 0.5 (roughly
  equal reads from both alleles). Homozygous sites are at 0 or 1.
- **Heterozygous deletion (1 copy)**: No heterozygous sites exist in the deleted
  region — the remaining single copy produces BAF of only 0 or 1. The
  "disappearance" of heterozygosity is the signal.
- **Homozygous deletion (0 copies)**: No reads at all. BAF is undefined; depth
  is zero.
- **Duplication to 3 copies**: Heterozygous sites shift from BAF of 0.5 to
  either ~0.33 or ~0.67 (one allele is on two copies, the other on one). This
  produces a characteristic "split" of the BAF track away from 0.5.
- **Duplication to 4 copies**: Possible BAF values at het sites are 0.25, 0.5,
  or 0.75, depending on the allele configuration.

The key insight is that BAF reveals copy number states that depth alone cannot.
A duplication from 2 to 3 copies increases depth by 50%, which can be hard to
distinguish from noise. But the shift of BAF from 0.5 to 0.33/0.67 is a distinct
pattern. Similarly, a copy-neutral loss of heterozygosity (LOH, where both
copies are identical — common in cancer) shows normal depth but BAF is entirely
0 or 1, with no heterozygous sites.

In array-based genotyping (SNP arrays), LRR and BAF are the primary signals used
by CNV callers. In sequencing-based analysis, the equivalent data comes from the
VCF `DP` and `AD` fields, or from the BAM file directly.

```
Normal region:        LRR ~0,    BAF has points at 0, 0.5, 1
Het deletion:         LRR < 0,   BAF has points at 0 and 1 only
Hom deletion:         LRR << 0,  no data
Single-copy gain:     LRR > 0,   BAF has points at 0, 0.33, 0.67, 1
Copy-neutral LOH:     LRR ~0,    BAF has points at 0 and 1 only
```

Visualizing LRR and BAF across the genome (one plot above the other, both
sharing a genomic position x-axis) is standard practice for interpreting CNV
calls. The combination of the two tracks makes the copy number state visually
obvious in a way that either track alone does not.

### Loss of heterozygosity (LOH)

Loss of heterozygosity is when a region of the genome that was originally
heterozygous (two different alleles) becomes homozygous (two identical alleles).
In a VCF, this manifests as a stretch of consecutive sites where a sample that
"should" be heterozygous (based on population frequency) is instead homozygous.

There are two main mechanisms:

**Copy-loss LOH (deletion LOH)**: One copy of the region is deleted, leaving
only one copy. This is a true deletion — depth drops and all heterozygosity is
lost in the region. In VCF terms: the `DP` drops, and every `GT` in the region
is `0/0` or `1/1`, with no `0/1` calls. LRR is negative, BAF shows only 0 and

1.

**Copy-neutral LOH**: One copy is lost and the remaining copy is duplicated, or
one copy is replaced by a duplicate of the other (through mitotic
recombination). The result is two identical copies — normal depth but no
heterozygosity. In VCF terms: `DP` is normal, but again every `GT` is
homozygous. LRR is near zero, BAF shows only 0 and 1. This is invisible to
depth-based methods alone and can only be detected through BAF.

LOH is particularly important in cancer genomics. Tumor suppressor genes often
follow a "two-hit" model: one copy is inactivated by a mutation, then the other
copy is lost through LOH. The LOH removes the remaining functional copy,
contributing to tumor progression. Detecting LOH from a tumor VCF (often by
comparing to a matched normal sample from the same patient) is a standard part
of somatic analysis.

### Runs of homozygosity (ROH)

A related concept in germline (non-cancer) genetics is runs of homozygosity. ROH
are long continuous stretches where an individual is homozygous at every site.
Unlike LOH (which is a somatic event in cancer), ROH in germline samples result
from inheriting two copies of the same ancestral chromosome segment — the
individual's parents shared a recent common ancestor for that region.

Short ROH (< 1 Mb) are common and reflect ancient population history (small
ancestral population sizes). Long ROH (> 5 Mb) suggest recent inbreeding (the
parents are closely related, so large chromosome segments are identical by
descent).

In VCF, ROH detection works by scanning for stretches of consecutive homozygous
genotypes, allowing for occasional heterozygous calls (which may be genotyping
errors). The total fraction of the genome in ROH (the "F_ROH" coefficient) is an
estimate of the inbreeding coefficient, and is more accurate than pedigree-based
estimates because it reflects realized rather than expected inbreeding.

ROH have clinical relevance: recessive disease variants are more likely to be
homozygous in individuals with high ROH, because both copies of the variant were
inherited from the same ancestor.

### Allelic imbalance

Allelic imbalance is a broader term for any deviation of BAF from the expected
0.5 at heterozygous sites. Beyond CNVs and LOH, allelic imbalance can indicate:

- **Somatic mosaicism** (a mixture of cells with different genotypes): if only a
  fraction of cells have a deletion, the BAF shifts partially — say from 0.5 to
  0.6 — rather than all the way to 0 or 1. The degree of shift reflects the
  fraction of affected cells (tumor purity in cancer, or clone fraction in
  mosaic conditions).
- **Allele-specific expression** (one allele is transcribed more than the
  other): detectable in RNA-seq VCFs where the `AD` field reflects RNA read
  counts rather than DNA read counts. A heterozygous site showing 80% reads from
  one allele and 20% from the other suggests regulatory variation or imprinting
  (where only the copy from one parent is active).

### SV merging is a representation problem

When combining SV calls from multiple samples into a population callset, the
fundamental challenge is deciding when two calls represent the "same" event. For
SNPs, two calls at the same position with the same `ALT` are identical. For SVs,
the question is: how close in position, how similar in size, and how similar in
sequence must two calls be to be considered the same?

There is no single correct answer. Different merging strategies (requiring 50%
reciprocal overlap vs. 500bp breakpoint distance vs. 70% size similarity)
produce different merged callsets. The choice of merge parameters directly
affects downstream allele frequency estimates. This is a fundamental limitation
of representing inherently fuzzy events in a format designed for precise genomic
coordinates.

### Nested and complex SVs

Some structural variants are nested: a deletion may contain an inversion within
it, or a duplication may have an insertion at the junction. VCF represents these
as separate records, but the relationship between them is not formally encoded.

The `EVENT` INFO field can group related breakend records, but there is no
standard way to express "this inversion is nested inside this deletion" or "this
insertion occurred at the boundary of this duplication". Interpreting the
combined effect of nearby or overlapping SV records on the derived sequence
requires reconstructing the local haplotype, which is nontrivial.

### Read-based vs assembly-based SV calling and what it means for VCF

SVs in VCF generally come from one of two workflows, and the workflow affects
the VCF representation:

**Read-based calling** maps sequencing reads to a reference genome and infers
SVs from alignment signals: large CIGAR deletions/insertions, split reads
(supplementary alignments), discordant read pairs, and depth of coverage
changes. The resulting VCF often has:

- `CIPOS`/`CIEND` confidence intervals (breakpoints are approximate)
- Missing inserted sequence for `<INS>` calls (reads may not fully span the
  insertion)
- Symbolic alleles (`<DEL>`, `<INV>`, etc.) rather than literal `REF`/`ALT`
- Breakend notation for inter-chromosomal events

**Assembly-based calling** first assembles reads into contigs (ideally
haplotype-resolved), then aligns the assembled contigs to the reference. SVs are
called from the contig-to-reference alignment. The resulting VCF tends to have:

- Exact breakpoints (no `CIPOS`/`CIEND` needed)
- Full inserted sequence in the `ALT` field or `INFO/SEQ`
- Literal `REF`/`ALT` strings for many SVs, even large ones, because the
  assembled contig provides the complete sequence
- Better representation of complex events where multiple SVs are adjacent

This means comparing an assembly-based callset to a read-based callset requires
handling both literal and symbolic representations of the same event, different
breakpoint precisions, and presence vs absence of inserted sequence. The
comparison problem is as much a representation problem as a biological one.

### BEDPE as an alternative for pairwise events

Some tools output SVs in BEDPE format instead of or in addition to VCF. BEDPE
(BED Paired-End) represents each SV as a pair of genomic intervals:

```
chr1  1000  1050  chr4  2000  2050  event1  .  +  -
```

This format naturally represents inter-chromosomal events and carries
orientation information in the strand columns. It avoids the complexity of
breakend bracket notation but cannot represent genotypes, quality scores, or
per-sample information. BEDPE is useful as an intermediate format for SV
analysis but lacks the richness of VCF for population-level studies.

## Mixed ploidy and sex chromosomes

### The problem

Most human autosomes (non-sex chromosomes) are diploid (two copies), but the X
chromosome in males is haploid (one copy) outside the pseudoautosomal regions
(small regions at the tips of X and Y that behave like autosomes). The Y
chromosome is haploid, and mitochondrial DNA is haploid. Some organisms have
more complex ploidy (number of chromosome copies): wheat is hexaploid (six
copies), some fish are tetraploid (four copies), and ploidy can vary across the
genome in organisms with B chromosomes (extra non-essential chromosomes).

VCF handles this by allowing different numbers of alleles in the `GT` field per
sample:

```
FORMAT   MALE       FEMALE
GT       0          0/1
```

Here the male has a haploid genotype `0` and the female has a diploid genotype
`0/1`. This works, but many tools assume diploid genotypes everywhere and will
break on haploid calls.

### Practical approaches

For human genetics, the common approach is to either:

- Call males as diploid on X (treating hemizygous (one copy) as homozygous (two
  identical copies)), which is technically incorrect but simplifies downstream
  analysis
- Call males as haploid on X, which is correct but requires all downstream tools
  to handle mixed ploidy
- Split the analysis: process the pseudoautosomal regions (PAR) as diploid and
  non-PAR X as haploid for males

Most variant callers have specific ploidy arguments for sex chromosomes,
sometimes accepting a ploidy map file that specifies per-contig, per-sample
ploidy.

### Polyploid organisms

For polyploid organisms, the genotype field naturally extends:

```
0/0/0/1    (tetraploid, one alt allele)
0/0/1/1    (tetraploid, two alt alleles)
0/1/2/3    (tetraploid, four different alleles)
```

The number of possible genotypes grows combinatorially with ploidy. For a
biallelic site with ploidy `p`, there are `p + 1` possible genotypes. For a site
with `n` alleles and ploidy `p`, the count is `binomial(n + p - 1, p)`. The `PL`
(phred-scaled genotype likelihood) field must contain a value for each possible
genotype, so this field grows rapidly for polyploid multi-allelic sites.

This combinatorial explosion means that joint calling in polyploids with
multi-allelic sites can produce very large `PL` arrays.

## Phasing

### What is phasing?

Diploid organisms have two copies of each chromosome — one from each parent.
Phasing is the process of determining which alleles are on the same physical
chromosome (same haplotype). An unphased genotype `0/1` tells you the individual
is heterozygous but not which parent contributed which allele. A phased genotype
`0|1` says the first haplotype (by convention, often paternal) carries the
reference allele, and the second haplotype carries the alternate.

The `/` separator means unphased; the `|` separator means phased. This is one of
the most important notational distinctions in VCF.

### A concrete example: why phase matters

Consider two heterozygous variants in the same gene:

```
#CHROM  POS  ID  REF  ALT  ...  FORMAT  SAMPLE
chr1    100  .   A    T    ...  GT      0/1
chr1    200  .   C    G    ...  GT      0/1
```

Without phasing, we don't know if the two alternate alleles (`T` and `G`) are on
the same chromosome or different chromosomes. This matters:

- **Same chromosome (cis)**: One copy of the gene has both mutations, the other
  copy is completely normal. The organism has one functional copy.
- **Different chromosomes (trans)**: Each copy of the gene has one mutation.
  Neither copy is normal.

Phased, this would be either:

```
GT  0|1    (T is on haplotype 2)
GT  0|1    (G is on haplotype 2)  → both on same copy (cis)
```

or:

```
GT  0|1    (T is on haplotype 2)
GT  1|0    (G is on haplotype 1)  → on different copies (trans)
```

### Trio phasing example

A trio (mother, father, child) is one of the simplest ways to determine phase.
Consider a site where the child is heterozygous `0/1`:

```
#CHROM  POS  ID  REF  ALT  ...  FORMAT  MOTHER  FATHER  CHILD
chr1    100  .   A    T    ...  GT      0/0     0/1     0/1
chr1    200  .   C    G    ...  GT      0/1     0/0     0/1
```

At position 100: the mother is `0/0` (homozygous ref), so the child's `T` allele
must have come from the father. At position 200: the father is `0/0`, so the
child's `G` allele must have come from the mother. Therefore:

```
chr1    100  ...  GT  0|0     0|1     0|1     (T from father = haplotype 2)
chr1    200  ...  GT  0|1     0|0     1|0     (G from mother = haplotype 1)
```

The child's phased genotypes tell us: haplotype 1 (maternal) carries `A` at 100
and `G` at 200; haplotype 2 (paternal) carries `T` at 100 and `C` at 200. The
variants are in trans.

When both parents are heterozygous at a site, trio phasing is ambiguous — you
cannot determine which parent transmitted which allele without additional
information (e.g. phasing of a nearby variant that breaks the tie).

### Why phasing matters

- **Compound heterozygosity** (described above): Two heterozygous variants in
  the same gene may both affect the same copy (cis, meaning on the same
  chromosome — potentially pathogenic) or different copies (trans, meaning on
  opposite chromosomes — potentially benign). Only phasing can distinguish
  these.
- **Haplotype-based association**: Some disease associations are with specific
  haplotypes (combinations of alleles), not individual variants.
- **Imputation accuracy**: Imputation methods work on haplotypes, so phased data
  improves imputation quality.
- **Ancestry inference**: Phased haplotypes enable detailed ancestry analysis
  that unphased genotypes cannot support.

### Phasing methods

- **Statistical phasing**: Uses LD (linkage disequilibrium — the tendency for
  nearby variants to be inherited together) patterns in a population to infer
  phase. Requires a population of samples.
- **Read-backed phasing**: Uses sequencing reads that span multiple variant
  sites to determine phase. Works on a single sample but is limited by read
  length.
- **Trio phasing**: Uses parent-child relationships as described above.
- **Long-read phasing**: Long reads (PacBio HiFi, Oxford Nanopore) can span many
  variants, enabling chromosome-scale phasing when combined with statistical
  methods.

### Phase sets

The `PS` (phase set) FORMAT field indicates which variants are phased relative
to each other. Variants with the same `PS` value are on a known haplotype
relative to each other, but the phase relationship between different phase sets
is unknown.

```
chr1  1000  .  A  T  .  PASS  .  GT:PS  0|1:1000
chr1  2000  .  C  G  .  PASS  .  GT:PS  1|0:1000
chr1  5000  .  T  A  .  PASS  .  GT:PS  0|1:5000
```

The first two variants are in the same phase set (PS=1000), so we know the `T`
at position 1000 and the `C` (ref) at position 2000 are on the same haplotype.
The variant at position 5000 is in a different phase set, so its phase
relationship to the first two is unknown.

Phase sets arise because no phasing method provides perfect chromosome-length
phase. Read-backed phasing produces phase sets bounded by the distance between
heterozygous variants that reads can span. Statistical phasing can produce
longer phase sets but may still have switch errors (points where the inferred
phase flips incorrectly between haplotypes).

### Phasing in population-level VCFs

Large population VCFs (like those from the 1000 Genomes Project or gnomAD) are
often distributed in phased form. Every genotype uses the `|` separator:

```
GT  0|0  0|1  1|0  0|1  1|1  0|0  ...
```

This phasing was typically performed computationally using statistical phasing,
which leverages LD across the entire cohort. With thousands of samples, shared
haplotype structure provides enough information to phase most common variants
with high accuracy.

In a phased population VCF, each sample's data can be decomposed into two
haplotype rows. A VCF with 2,504 diploid samples (as in 1000 Genomes Phase 3)
implicitly contains 5,008 haplotypes. Population genetics methods that operate
on haplotypes (e.g. computing extended haplotype homozygosity for selection
scans, or building haplotype reference panels for imputation) rely on this
phased representation.

Phasing accuracy is higher for common variants (many samples share the same
haplotypes, making the pattern clear) and lower for rare variants (few samples
carry them, so there is less LD information to resolve phase). Switch error
rates for common variants in large panels are typically below 1%, but can be
much higher for singletons and doubletons (variants seen in only 1-2 samples).

## Variant calling in mixed cell populations

Variant calling is often described in terms of "germline" (inherited) and
"somatic" (acquired) variants, but in practice the distinction is less clean
than it sounds. The underlying issue is cell population heterogeneity: every
sample is a mixture of cells, and the caller's job is to detect variants that
may be present in only a fraction of them.

In a typical germline setting, you expect variants at roughly 0%, 50%, or 100%
allele frequency (corresponding to 0/0, 0/1, and 1/1 genotypes). The caller can
lean on this prior. In a tumor sample, a variant might be present at 5%, 30%, or
73% — there is no fixed expectation, and the variant allele fraction (VAF) is
itself informative. The same is true for mosaic variants in developmental
biology, heteroplasmy in mitochondrial DNA, or mixed infections in microbiology.

This means the FORMAT fields carry different information depending on context. A
germline VCF typically has `GT:GQ:DP:PL` — a hard genotype call and its
confidence. A VCF from a mixed-population caller often includes `GT:AD:AF:DP`,
where `AD` (allelic depth) gives raw read counts per allele and `AF` gives the
observed variant allele fraction. Some callers add fields like `F1R2`/`F2R1`
(forward/reverse strand read counts per allele, useful for detecting strand bias
artifacts) or `SB` (strand bias).

The FILTER column also tends to be richer. Rather than just `PASS` or `.`,
mixed-population callers apply multiple soft filters: minimum allele count,
strand bias, read position bias, mapping quality, contamination estimates, and
panel-of-normals filters (where variants seen in many unrelated samples are
flagged as likely artifacts rather than true low-frequency variants).

A practical consequence: if you see a VCF with `AD` and `AF` FORMAT fields and
extensive FILTER annotations, you can generally infer it came from a
mixed-population calling workflow. If you see `PL` and `GQ` with hard GT calls,
it is more likely from a standard diploid caller. But these are patterns, not
rules — some callers produce both, and the VCF spec does not enforce any
particular combination.

One important representation issue: when a caller reports a variant at, say, 3%
VAF, it may still encode this as `GT=0/1` because VCF genotypes are
fundamentally discrete. The continuous information lives in the `AD` and `AF`
fields. For downstream analysis of mixed populations, the `GT` field is often
less informative than the raw allelic depths.

## Variant annotation

### What variant annotation does

A raw VCF tells you what variants exist and in which samples. It does not tell
you what those variants mean biologically. Variant annotation adds this context:
does this variant fall in a gene? Does it change a protein? Is it in a
regulatory region? Has it been seen before in other populations? Is it predicted
to be damaging?

Annotation works by taking each variant in the VCF and comparing its position
and alleles against databases of gene models, known variants, functional
predictions, and conservation scores. The results are typically added to the
`INFO` column as new key-value pairs, or written as a separate table.

### How annotation works for SNPs and indels

For a SNP or small indel, annotation proceeds roughly as follows:

1. **Determine the genomic context**: Is the variant in a coding exon, an
   intron, a UTR (untranslated region — the parts of mRNA that are not
   translated into protein), a promoter, or an intergenic region (between
   genes)?

2. **Determine the coding consequence** (if in a coding exon): The annotator
   takes the reference codon, applies the variant to get the alternate codon,
   and translates both to amino acids. This produces consequence terms like:
   - **Synonymous** (silent): The amino acid does not change. E.g. `GCT` → `GCC`
     both encode alanine.
   - **Missense**: The amino acid changes. E.g. `GCT` (alanine) → `GAT`
     (aspartate). The protein has a different amino acid at this position.
   - **Nonsense** (stop-gained): The variant creates a premature stop codon,
     truncating the protein.
   - **Frameshift**: An indel whose length is not a multiple of 3 shifts the
     reading frame, changing all downstream amino acids and usually hitting a
     premature stop codon.
   - **In-frame indel**: An indel whose length is a multiple of 3 inserts or
     removes amino acids without disrupting the reading frame.
   - **Splice site**: The variant falls at a splice donor or acceptor site (the
     GT-AG boundaries of introns), potentially disrupting RNA splicing.

3. **Look up population frequency**: Is this variant in databases of known
   variation like gnomAD or dbSNP? A variant seen at 5% frequency in gnomAD is
   unlikely to cause a severe disease. A variant never seen before (novel) is
   more suspicious.

4. **Add pathogenicity predictions**: Computational tools predict whether a
   missense variant is damaging based on protein structure, evolutionary
   conservation, and biochemical properties of the amino acid change. Scores
   like SIFT, PolyPhen-2, CADD, and REVEL are commonly added during annotation.

### How annotation works for structural variants

SV annotation operates on a larger scale. Instead of asking "what codon does
this SNP change?", the question is "what genes does this SV overlap, and how?":

- **Deletions**: Which genes (or which exons) are partially or fully deleted? A
  deletion that removes exons 3-5 of a gene is likely a loss-of-function
  variant. A deletion in an intergenic region may have no consequence.
- **Duplications**: Is a gene fully duplicated (potentially increasing dosage)?
  Is it partially duplicated (potentially disrupting the gene by breaking it
  into an incomplete copy)?
- **Inversions**: Does the inversion breakpoint fall within a gene, disrupting
  it? Or does it invert an entire gene without breaking it (potentially
  affecting regulation but not the gene itself)?
- **Translocations**: Do the breakpoints disrupt genes? Gene fusions (where a
  breakpoint joins parts of two different genes) are a hallmark of certain
  cancers — e.g. the BCR-ABL fusion in chronic myeloid leukemia.

SV annotation requires knowing not just whether the SV overlaps a gene, but how
it overlaps: does it delete the entire gene, just one exon, or just an intron?
Does a duplication encompass the whole gene (which might be tolerated) or just
part of it (which likely disrupts function)?

### The annotation is added to the VCF

Annotation results are typically written back into the VCF as INFO fields. A
common convention is the `ANN` or `CSQ` INFO field (used by SnpEff and VEP
respectively), which packs multiple annotation fields into a single
pipe-delimited string:

```
INFO  ...;CSQ=T|missense_variant|MODERATE|BRCA1|ENSG00000012048|...
```

This is admittedly ugly to parse — it is a nested data structure crammed into a
flat INFO field — but it is the standard. Each `|`-separated element corresponds
to a field defined in the VCF header (gene name, consequence, impact, transcript
ID, etc.), and there can be multiple comma-separated entries if the variant
affects multiple transcripts.

### Multiple transcripts, multiple consequences

A single variant can have different consequences depending on which transcript
(version of the gene's mRNA) you consider. A gene may have multiple transcript
isoforms (produced by alternative splicing), and a variant in one exon might be:

- Missense in the canonical (main) transcript
- Intronic in an alternative transcript that skips that exon
- In the UTR of a third transcript with a different start site

Annotation tools report consequences for all overlapping transcripts, which can
produce a long list per variant. Filtering to the canonical or MANE Select
transcript (the community-agreed primary transcript for clinical reporting)
simplifies interpretation.

### Annotation is coordinate-dependent

All annotation depends on the variant's coordinates matching the gene model's
coordinates. If the VCF is on GRCh38 but the gene models are on GRCh37, the
annotation will be wrong (it will look up the wrong genomic position and find
the wrong gene or no gene at all). This is another manifestation of the
reference genome consistency problem discussed in the pain points section.

Similarly, normalization matters: a non-left-aligned indel might be annotated as
affecting a different codon than the same indel after normalization, because the
position shifted.

## Common VCF pain points

### "chr1" vs "1" — a symptom of a deeper problem

VCF files from different sources use different chromosome naming conventions.
UCSC-style names use `chr1`, `chr2`, etc. Ensembl/GRC-style uses `1`, `2`, etc.
When you encounter a naming mismatch between two files, treat it as an alarm
bell: it likely means the files were produced against different reference
genomes. At best, the underlying reference sequence is the same version with
different chromosome names (e.g. GRCh38 vs hg38 — these are the same assembly
with different naming). At worst, the files were generated against genuinely
different reference versions (e.g. GRCh37 vs GRCh38), which have different
coordinates, different sequences in some regions, and different sets of contigs.
Renaming chromosomes in that case would silently produce incorrect results.

Even "the same version with different names" can have subtle differences. The
UCSC hg38 includes some patches and alternate sequences that the Ensembl GRCh38
may not, and the mitochondrial sequence differs between some distributions.

Before renaming chromosomes, verify that the underlying reference sequences
actually match (e.g. compare sequence lengths and checksums). The VCF header's
`##contig` lines, when present, include sequence length and sometimes MD5
checksums that help with this verification.

### Merging VCFs from different projects

A common practical scenario: you want to combine your own VCF with a public
dataset (e.g. 1000 Genomes, gnomAD) or combine VCFs from different sequencing
batches within a project. This routinely fails or produces incorrect results
because:

- The VCFs were called against different reference genome versions (see above)
- One VCF has multi-allelic sites, the other has decomposed biallelic records
- The variant representations are not normalized the same way (left-aligned vs
  not, different padding bases)
- One VCF includes sites where all samples are homozygous reference (common in
  gVCF-derived files), the other only includes variant sites
- The `INFO` and `FORMAT` fields have different definitions or different sets of
  annotations

Naive merging (e.g. just concatenating files or using a simple merge command)
will silently produce wrong allele frequencies or duplicated sites. The
practical solution is to normalize both files against the same reference, ensure
matching variant representations, and then merge — but even then, batch effects
(systematic differences in variant calls between sequencing runs) can confound
downstream analysis.

### Subsetting samples and forgetting to update annotations

When you extract a subset of samples from a large VCF, the site-level INFO
fields (`AC`, `AN`, `AF`) still reflect the original full cohort. If you filter
to 50 samples from a 10,000-sample VCF, the `AF=0.01` that was correct for the
original cohort is now meaningless. Forgetting to recalculate these fields after
subsetting is a common source of incorrect allele frequencies in downstream
analyses.

Similarly, after subsetting, some sites may become monomorphic (every remaining
sample is `0/0`). These sites are often unintentionally retained, inflating
variant counts and wasting computation in GWAS or population genetics.

### Multi-allelic decomposition breaks genotypes

Decomposing a multi-allelic site (`A -> T,C`) into biallelic records (`A -> T`
and `A -> C`) requires rewriting the `GT` field. A genotype of `1/2` (het for
the two ALT alleles) cannot be represented in either biallelic record alone.
Decomposition tools handle this differently: some set it to `0/1` in both
records, some set it to missing in one, some use the `*` allele. There is no
universally correct approach, and downstream tools may interpret the decomposed
genotypes differently.

This causes real problems when computing allele frequencies or running
association tests on decomposed files: a sample that is `1/2` at the original
multi-allelic site will appear to be heterozygous in both biallelic records,
inflating the apparent frequency of both alleles.

### Liftover between genome builds

When a reference genome is updated (e.g. GRCh37/hg19 to GRCh38/hg38), VCF files
need to be "lifted over" (coordinates remapped) to the new coordinate system.
This requires remapping each variant's position, updating the `REF` allele to
match the new reference, and handling cases where the reference allele changed
between builds (a variant in one build may be the reference allele in another).

Liftover can fail for variants in regions that were structurally rearranged
between builds, or where the reference allele swap creates ambiguity. Indels in
regions that changed between builds are especially problematic. A fraction of
variants are always lost during liftover, and the lost variants are not random —
they are enriched in the regions that changed most between builds, which tend to
be biologically interesting (e.g. segmental duplications, centromeric regions).

A practical consequence: if you are starting a new project, choosing GRCh38 (or
T2T-CHM13) from the start avoids liftover entirely.

### Filtering: FILTER column vs INFO-based filtering

The `FILTER` column is supposed to be the definitive record of whether a site
passed quality control. But in practice, many VCFs are distributed with
`FILTER=.` (not evaluated) or `FILTER=PASS` for all sites, with the actual
quality information buried in INFO fields like `QUAL`, `QD` (quality by depth),
`FS` (Fisher strand bias), `MQ` (mapping quality), etc.

This means that users who naively trust `FILTER=PASS` may be working with
unfiltered data, while users who want to apply their own filters must understand
which INFO fields to use and what thresholds are appropriate for their caller
and data type. The "right" filter thresholds depend on the caller, the
sequencing technology, the coverage, and the application (clinical vs. research,
rare variant analysis vs. common variant GWAS), and there is no universal set of
thresholds.

GATK's VQSR (Variant Quality Score Recalibration) addresses this by training a
model on known true positives and false positives, producing a single
recalibrated quality score, but this requires sufficient data for training and
is not always applicable to non-human organisms or small callsets.

### VCF lines are not independent

Unlike many tabular formats, VCF lines are not fully self-contained. The `*`
allele depends on an upstream deletion. Phased genotypes (`|`) are only
meaningful relative to other phased variants in the same phase set. The `END`
INFO field in gVCF blocks implicitly claims information about positions not on
that line. This means naive line-by-line processing (e.g. `grep`, random
sampling of lines) can produce misleading results.

### Large cohort VCFs are unwieldy

A VCF with 100,000+ samples has lines that are megabytes long. A single variant
line in a 500,000-sample VCF can exceed 10 MB. Simple operations like extracting
a single sample or computing allele frequency become I/O-bound because you must
read and parse the entire line even if you only need one sample's genotype.

BCF (binary VCF) helps with parsing speed, but does not solve the fundamental
layout problem: VCF and BCF are both row-oriented, meaning all samples are
stored together on each variant line. This layout is efficient for accessing all
samples at one variant (e.g. computing allele frequency) but terrible for
accessing all variants for one sample (e.g. extracting a single individual).

Several approaches have emerged for working at this scale:

**Columnar and sparse storage.** Formats that store data by column (one column
per sample, or one column per field) allow extracting a single sample without
reading all the others. Sparse representations can also help: in a cohort of
500,000 samples, most genotypes at any given site are homozygous reference.
Storing only the non-reference genotypes (with an implicit default of 0/0)
dramatically reduces storage and I/O.

**Sample sharding.** Instead of one massive file, the cohort is split into
blocks of samples. Each shard contains all variants but only a subset of
samples. Queries that touch a few samples only need to read a few shards. This
is how some cloud-native genomic databases organize their storage.

**Site-level summaries.** For many population-level analyses (allele frequency,
Hardy-Weinberg, association testing), you do not need individual genotypes at
all — just allele counts per site. Precomputing and storing these summaries
avoids the need to parse genotype fields entirely.

**Genotype matrices.** Tools like plink's binary format (BED/BIM/FAM) store
genotypes as a packed binary matrix (2 bits per diploid genotype), which is
orders of magnitude more compact and faster to access than VCF for
sample-by-variant operations. Hail's MatrixTable provides a similar
matrix-oriented abstraction backed by columnar storage.

**Zarr-based VCF (VCF-Zarr).** Zarr is a chunked, compressed, N-dimensional
array format designed for cloud-native access. VCF-Zarr stores VCF fields as
separate Zarr arrays — one array for GT, one for DP, one for POS, and so on —
each chunked along both the variant and sample axes. This means you can read a
rectangular slice (a region of variants for a subset of samples) without
touching the rest of the dataset, and each chunk can be fetched independently
over HTTP with range requests. This makes it well-suited for cloud storage where
random access is cheap but full scans are expensive.

**What gets lost.** Each of these alternative formats makes trade-offs against
VCF's generality. Plink's 2-bit encoding can represent only biallelic, unphased,
diploid genotypes — multi-allelic sites, phasing, and non-diploid samples are
dropped or collapsed during conversion. Genotype quality (GQ), allelic depth
(AD), and likelihood fields (PL/GL) are discarded entirely. Hail's MatrixTable
preserves more fields but introduces its own type system and requires the Hail
runtime to access. Zarr-based approaches can in principle store all VCF fields
losslessly, but in practice many implementations store only a subset of FORMAT
fields (typically GT, DP, GQ, and AD) and may not round-trip the full VCF
header, INFO field semantics, or unusual genotype representations like single
breakends. Site-level summaries obviously lose all individual-level data.

The broader pattern is that VCF is a "write once, read in whatever subset you
need" format, but its row-oriented layout fights against this aspiration at
scale. Alternative formats solve the access pattern problem but fragment the
ecosystem: a dataset stored as plink BED files cannot easily be queried for
phasing information that was present in the original VCF, and a Zarr store may
not preserve the exact FILTER annotations a reviewer wants to inspect.

The practical reality is that VCF/BCF remains the interchange format: variant
callers produce it, and archives distribute it. But most large-scale analyses
convert to a more efficient internal representation as a first step. The
overhead of this conversion is usually small compared to the downstream
computation, but it means that researchers working with large cohorts need
familiarity with at least two or three format ecosystems — and need to be aware
of what was silently dropped during conversion.

## Appendix A: Parsing genotypes from a VCF line

The `GT` field is the most commonly accessed field in a VCF. Parsing it
correctly requires handling `/` vs `|` separators, missing alleles (`.`), and
variable ploidy.

```ts
// Parse a GT string like "0/1", "0|1|2", ".", "./.", "0"
// Returns an array of allele indices (or -1 for missing)
// and whether the genotype is phased
function parseGT(gt: string) {
  const phased = gt.includes('|')
  const sep = phased ? '|' : '/'
  const alleles = gt.split(sep).map(a => (a === '.' ? -1 : parseInt(a, 10)))
  return { alleles, phased }
}
```

Note that this handles arbitrary ploidy: a diploid `0/1` returns two alleles, a
tetraploid `0/0/1/2` returns four, and a haploid `1` returns one. Code that
assumes `alleles.length === 2` will break on non-diploid data.

A subtle edge case: the spec allows mixed separators like `0/1|2`, though this
is rare in practice and arguably malformed. Most parsers pick one separator.

## Appendix B: Computing allele frequency

Allele frequency is the most basic population statistic derivable from a VCF.
The `AF` INFO field often provides this, but it may be stale if you have
subsetted samples. Computing it yourself is straightforward:

```ts
// Given an array of GT strings for all samples at a biallelic site,
// compute the alternate allele frequency
function computeAF(genotypes: string[]) {
  let altCount = 0
  let totalAlleles = 0

  for (const gt of genotypes) {
    const { alleles } = parseGT(gt)
    for (const a of alleles) {
      if (a >= 0) {
        // not missing
        totalAlleles++
        if (a > 0) {
          altCount++
        }
      }
    }
  }

  if (totalAlleles === 0) {
    return NaN // all missing
  }
  return altCount / totalAlleles
}
```

This handles missing data (skips `.` alleles) and arbitrary ploidy (counts all
non-missing alleles regardless of how many there are per sample). For a
multi-allelic site, you would compute a separate frequency for each `ALT`
allele.

The standard tool for recomputing allele frequencies after sample subsetting is
`bcftools +fill-tags -- -t AC,AN,AF`.

## Appendix C: Computing Ti/Tv ratio

The transition/transversion ratio is a useful QC metric for SNP callsets. As
mentioned earlier, it should be ~2.0-2.1 for whole-genome and ~2.8-3.3 for
exome. Computing it requires classifying each biallelic SNP:

```ts
function isTransition(ref: string, alt: string) {
  const pair = ref + alt
  // A<->G and C<->T are transitions
  return pair === 'AG' || pair === 'GA' || pair === 'CT' || pair === 'TC'
}

// Given arrays of REF and ALT for biallelic SNPs only
function computeTiTv(refs: string[], alts: string[]) {
  let ti = 0
  let tv = 0

  for (let i = 0; i < refs.length; i++) {
    // skip if not a SNP (both must be single base)
    if (refs[i].length !== 1 || alts[i].length !== 1) {
      continue
    }
    if (isTransition(refs[i], alts[i])) {
      ti++
    } else {
      tv++
    }
  }

  return tv === 0 ? Infinity : ti / tv
}
```

Multi-allelic sites should be decomposed first (e.g. `A -> T,C` becomes two
separate SNPs for this calculation). `bcftools stats` computes Ti/Tv as part of
its summary output.

## Appendix D: Hardy-Weinberg equilibrium test

HWE testing checks whether observed genotype frequencies match expectations
under random mating. For a biallelic site with allele frequency `p` (ref) and
`q = 1 - p` (alt), the expected frequencies are `p^2` (hom-ref), `2pq` (het),
and `q^2` (hom-alt). Significant deviation from HWE in control samples can
indicate genotyping error.

```ts
function hweChiSquared(genotypes: string[]) {
  let homRef = 0
  let het = 0
  let homAlt = 0

  for (const gt of genotypes) {
    const { alleles } = parseGT(gt)
    // skip non-diploid or missing
    if (alleles.length !== 2 || alleles[0] < 0 || alleles[1] < 0) {
      continue
    }
    const sum = alleles[0] + alleles[1]
    if (sum === 0) {
      homRef++
    } else if (sum === 2) {
      homAlt++
    } else {
      het++
    }
  }

  const n = homRef + het + homAlt
  if (n === 0) {
    return NaN
  }

  const p = (2 * homRef + het) / (2 * n)
  const q = 1 - p

  const expHomRef = p * p * n
  const expHet = 2 * p * q * n
  const expHomAlt = q * q * n

  // chi-squared with 1 degree of freedom
  const chi2 =
    (homRef - expHomRef) ** 2 / expHomRef +
    (het - expHet) ** 2 / expHet +
    (homAlt - expHomAlt) ** 2 / expHomAlt

  return chi2
}
```

A chi-squared value above ~3.84 (p < 0.05, 1 df) suggests deviation from HWE. In
practice, GWAS QC uses a stricter threshold like `1e-6` to avoid removing sites
with genuine biological signals (selection, population structure).

Note: this only works for diploid, biallelic sites. Multi-allelic or non-diploid
sites require a generalized test. `bcftools +fill-tags -- -t HWE` annotates each
site with an HWE p-value.

## Appendix E: Parsing breakend ALT fields

Breakend `ALT` fields are one of the trickiest things to parse in VCF. The
bracket notation encodes the partner chromosome, position, orientation, and any
inserted sequence all in one string.

```ts
// Parse a breakend ALT string
// Returns the partner location, orientation, and any inserted sequence
function parseBreakend(alt: string) {
  // Four patterns:
  //   t[p[    t]p]    ]p]t    [p[t
  // where t is sequence and p is chr:pos

  let match

  // Right-extending, forward join: t[chr:pos[
  match = alt.match(/^([A-Za-z.]+)\[(.+):(\d+)\[$/)
  if (match) {
    return {
      insertedSeq: match[1].slice(1), // remove the ref base
      partner: { chr: match[2], pos: parseInt(match[3], 10) },
      localForward: true,
      partnerForward: true,
    }
  }

  // Right-extending, reverse join: t]chr:pos]
  match = alt.match(/^([A-Za-z.]+)\](.+):(\d+)\]$/)
  if (match) {
    return {
      insertedSeq: match[1].slice(1),
      partner: { chr: match[2], pos: parseInt(match[3], 10) },
      localForward: true,
      partnerForward: false,
    }
  }

  // Left-extending, reverse join: ]chr:pos]t
  match = alt.match(/^\](.+):(\d+)\]([A-Za-z.]+)$/)
  if (match) {
    return {
      insertedSeq: match[3].slice(0, -1), // remove the ref base
      partner: { chr: match[1], pos: parseInt(match[2], 10) },
      localForward: false,
      partnerForward: false,
    }
  }

  // Left-extending, forward join: [chr:pos[t
  match = alt.match(/^\[(.+):(\d+)\[([A-Za-z.]+)$/)
  if (match) {
    return {
      insertedSeq: match[3].slice(0, -1),
      partner: { chr: match[1], pos: parseInt(match[2], 10) },
      localForward: false,
      partnerForward: true,
    }
  }

  // Single breakend: .ACGT or ACGT.
  if (alt.startsWith('.')) {
    return {
      insertedSeq: alt.slice(1),
      partner: null,
      localForward: false,
      partnerForward: false,
    }
  }
  if (alt.endsWith('.')) {
    return {
      insertedSeq: alt.slice(0, -1),
      partner: null,
      localForward: true,
      partnerForward: false,
    }
  }

  return null
}
```

The `insertedSeq` extraction above is simplified — in practice the "ref base"
and "inserted sequence" boundary depends on the `REF` column, and the inserted
sequence may be empty (just the ref base with no novel insertion at the
junction). Chromosome names containing `:` also complicate parsing, though this
is rare.

There is no widely-used standalone library just for breakend parsing; most
people handle it in application code or rely on their VCF library's built-in
support.

## Appendix F: Sample missingness

A common QC step is computing per-sample missingness (the fraction of sites
where a sample has a missing genotype). Samples with high missingness are
typically excluded.

```ts
// Given a sample index and an array of VCF lines (split by tab),
// compute the fraction of sites with missing genotype
function sampleMissingness(lines: string[][], sampleIdx: number) {
  let missing = 0
  let total = 0

  for (const fields of lines) {
    const format = fields[8]
    const sample = fields[9 + sampleIdx]
    if (!sample) {
      continue
    }

    // GT is always the first FORMAT field (when present)
    const gtIdx = format.split(':').indexOf('GT')
    if (gtIdx < 0) {
      continue
    }

    const gt = sample.split(':')[gtIdx]
    total++

    // missing if all alleles are "."
    const { alleles } = parseGT(gt)
    if (alleles.every(a => a < 0)) {
      missing++
    }
  }

  return total === 0 ? NaN : missing / total
}
```

`vcftools --missing-indv` and `bcftools stats -s-` both compute per-sample
missingness. Per-site missingness (fraction of samples missing at a given site)
is the complementary metric, computed similarly but iterating over samples
within a single site.

## Appendix G: Computing nucleotide diversity (pi)

Nucleotide diversity (pi) measures how genetically different two randomly chosen
individuals from a population are, averaged across sites. In simple terms: if
you picked two chromosomes at random from the population and compared them
base-by-base, pi is the fraction of sites where they would differ.

For a single biallelic site with allele frequency `p` (frequency of the ALT
allele), the probability that two randomly chosen alleles are different is
`2 * p * (1 - p)`. This is because one must be ALT (probability `p`) and the
other must be REF (probability `1 - p`), and this can happen in two ways.

Pi is the average of `2 * p * (1 - p)` across all sites in a region.

```ts
// Compute nucleotide diversity (pi) for a set of biallelic sites
// freqs: array of ALT allele frequencies (one per site), from computeAF()
// Each frequency was computed from the GT fields in the VCF
function computePi(freqs: number[]) {
  let sum = 0
  let sites = 0
  for (const p of freqs) {
    if (!isNaN(p)) {
      sum += 2 * p * (1 - p)
      sites++
    }
  }
  return sites === 0 ? NaN : sum / sites
}
```

Pi is typically computed in windows (e.g. 10kb or 100kb) across the genome.
Regions with low pi have low genetic diversity (possibly due to a recent
selective sweep — where a beneficial mutation rose to high frequency, dragging
nearby variants with it). Regions with high pi have high diversity. Comparing pi
between populations or between genomic regions is a basic tool for detecting
selection and understanding demographic history.

Note: the formula above uses allele frequencies, which can be computed from any
number of samples. With a small sample size correction (multiplying by
`n / (n - 1)` where `n` is the number of alleles sampled), this becomes an
unbiased estimator. `vcftools --site-pi` and `pixy` compute pi from VCF.

## Appendix H: Computing Fst

Fst (fixation index) measures how genetically different two populations are. It
ranges from 0 (the populations are genetically identical — same allele
frequencies) to 1 (the populations are completely differentiated — fixed for
different alleles).

The intuition: Fst compares genetic diversity within populations to genetic
diversity between populations. If two populations have very different allele
frequencies at a site, that site has high Fst. If the allele frequencies are
similar, Fst is low.

For a single biallelic site, the Weir-Cockerham estimator (the standard) is
complex, but the simplest approximation (Hudson's Fst) is:

```
Fst = (between-population diversity - within-population diversity)
      / between-population diversity
```

Where "within-population diversity" is the average pi computed separately within
each population, and "between-population diversity" is the pi you would get if
you randomly picked one allele from each population.

```ts
// Simplified Fst for a single biallelic site (Hudson's estimator)
// p1: ALT frequency in population 1
// p2: ALT frequency in population 2
// n1: number of alleles sampled in population 1
// n2: number of alleles sampled in population 2
function hudsonFst(p1: number, p2: number, n1: number, n2: number) {
  // numerator: between-population variance in allele frequency
  const numerator =
    (p1 - p2) ** 2 - (p1 * (1 - p1)) / (n1 - 1) - (p2 * (1 - p2)) / (n2 - 1)

  // denominator: total expected heterozygosity between populations
  const denominator = p1 * (1 - p2) + p2 * (1 - p1)

  if (denominator === 0) {
    return NaN // both populations monomorphic
  }
  return numerator / denominator
}
```

Per-site Fst values are noisy and can even be negative (which just means "less
differentiated than expected by chance" — effectively zero). In practice, Fst is
averaged across many sites, either genome-wide or in windows.

Genome-wide Fst between human continental populations is around 0.10-0.15,
meaning about 85-90% of human genetic variation exists within populations rather
than between them. Specific genomic regions with unusually high Fst are
candidates for local adaptation (natural selection driving different allele
frequencies in different environments — for example, skin pigmentation genes
show high Fst between populations from different latitudes).

`vcftools --weir-fst-pop` computes the full Weir-Cockerham estimator. The
calculation requires knowing which samples belong to which population, which is
metadata external to the VCF itself.

## Appendix I: Computing the D-statistic (ABBA-BABA) from VCF

The D-statistic tests for introgression using allele frequencies from four
populations. Given a VCF with samples labeled by population, you compute the
frequency of the derived (ALT) allele in each population at each biallelic SNP,
then accumulate ABBA and BABA counts.

```ts
// Compute allele frequency for a set of sample indices at a biallelic site
// Returns the frequency of the ALT allele (allele 1), or NaN if all missing
function altFreq(genotypes: string[], sampleIndices: number[]) {
  let altCount = 0
  let totalAlleles = 0
  for (const idx of sampleIndices) {
    const { alleles } = parseGT(genotypes[idx])
    for (const a of alleles) {
      if (a >= 0) {
        totalAlleles++
        if (a > 0) {
          altCount++
        }
      }
    }
  }
  return totalAlleles === 0 ? NaN : altCount / totalAlleles
}

// Compute D-statistic from a VCF
// p1Samples, p2Samples, p3Samples, outSamples are arrays of sample indices
// lines is an array of VCF data lines, each split by tab
//
// The four populations:
//   P1, P2: two closely related populations (test if P3 introgressed into one)
//   P3: candidate donor of introgression
//   O:  outgroup (used to polarize ancestral vs derived alleles)
function dStatistic(
  lines: string[][],
  p1Samples: number[],
  p2Samples: number[],
  p3Samples: number[],
  outSamples: number[],
) {
  let abba = 0
  let baba = 0

  for (const fields of lines) {
    const ref = fields[3]
    const alt = fields[4]

    // skip non-biallelic sites
    if (ref.length !== 1 || alt.length !== 1 || alt.includes(',')) {
      continue
    }

    const format = fields[8]
    const gtIdx = format.split(':').indexOf('GT')
    if (gtIdx < 0) {
      continue
    }

    // extract GT for all samples
    const genotypes = fields.slice(9).map(s => s.split(':')[gtIdx])

    const p1 = altFreq(genotypes, p1Samples)
    const p2 = altFreq(genotypes, p2Samples)
    const p3 = altFreq(genotypes, p3Samples)
    const pO = altFreq(genotypes, outSamples)

    if (isNaN(p1) || isNaN(p2) || isNaN(p3) || isNaN(pO)) {
      continue
    }

    // the outgroup defines the ancestral allele
    // if ALT is common in the outgroup, it is likely ancestral, so we
    // flip: use (1-freq) as the "derived" frequency
    // this is the frequency-based version of the D-statistic
    const d1 = pO > 0.5 ? 1 - p1 : p1
    const d2 = pO > 0.5 ? 1 - p2 : p2
    const d3 = pO > 0.5 ? 1 - p3 : p3

    // ABBA: P1 ancestral, P2 derived, P3 derived
    // BABA: P1 derived, P2 ancestral, P3 derived
    abba += (1 - d1) * d2 * d3
    baba += d1 * (1 - d2) * d3
  }

  const denom = abba + baba
  if (denom === 0) {
    return { D: NaN, abba, baba }
  }
  return { D: (abba - baba) / denom, abba, baba }
}
```

This computes the frequency-based D-statistic (sometimes called D_f), which uses
allele frequencies rather than single representative individuals. At each site,
the outgroup frequency determines which allele is ancestral: if the outgroup
mostly carries the ALT allele, the ALT is treated as ancestral and the REF as
derived.

The ABBA and BABA counts are accumulated as products of frequencies rather than
hard 0/1 classifications, which makes better use of the data when populations
have more than one sample.

To assess significance, the standard approach is a block jackknife: divide the
genome into blocks (e.g. 1Mb windows), recompute D while leaving out each block
in turn, and estimate the standard error from the jackknife replicates. A |D| /
standard_error > 3 (roughly corresponding to a Z-score > 3) is typically
considered significant evidence of introgression.

The equivalent standard tool is the `Dtrios` or `Dquartets` command in `Dsuite`,
or the `-D` option in `ANGSD`.

## Beyond VCF: graph genomes and pangenomes

### The reference bias problem

VCF is built on a fundamental assumption: there is one linear reference genome,
and all variation is described as deviations from it. This creates reference
bias — the tendency for sequences that match the reference to be handled better
than sequences that do not.

Reference bias manifests concretely in read mapping. When a sample has a large
insertion relative to the reference, reads from that insertion have nowhere to
map. They either go unmapped, get soft-clipped, or map with poor quality. The
variant may then be missed or called with low confidence. The same is true for
divergent haplotypes in regions like the MHC (major histocompatibility complex,
the highly variable immune gene region) where some individuals carry sequences
that differ substantially from the reference.

In VCF terms, reference bias means that some variants are systematically harder
to call than others — not because the sequencing data is bad, but because the
reference genome does not represent the sample's genome well enough for reads to
map correctly.

### Graph genomes as an alternative reference

A graph genome (or pangenome graph) represents multiple genomes simultaneously
as a graph. Instead of one linear sequence, the reference is a graph where:

- Nodes are sequences (segments of DNA)
- Edges connect nodes that are adjacent in at least one genome
- Known variants are encoded as alternative paths through the graph

A simple SNP creates a "bubble" in the graph: the path splits into two nodes
(one for each allele) and rejoins. A deletion removes a node from one path. An
insertion adds a node on one path that is absent from the other.

The key advantage: when you align reads to a graph genome, reads from non-
reference alleles now have a path to map to. The insertion that was invisible to
a linear aligner has explicit nodes in the graph. Reads from divergent MHC
haplotypes map correctly because those haplotypes are represented in the graph.

### Graph-to-linear projection: getting back to VCF

Graph alignment produces alignments in graph coordinates (e.g. GAF format, which
describes paths through graph nodes). But most downstream tools — GWAS, variant
annotation, population genetics — expect linear coordinates and VCF.

The solution is projection: after aligning to the graph, the variant calls are
projected back onto the linear reference to produce a standard VCF. The graph
alignment improved the mapping and variant calling, but the output is still
expressed in VCF coordinates relative to a linear reference.

This "align to graph, project to linear" workflow gives the best of both worlds:

- Reads from non-reference alleles map correctly (because the graph contains
  those alleles)
- Variant calls are less biased toward the reference allele
- The output is a standard VCF that works with all existing tools

In practice, this approach improves genotyping accuracy at known variant sites
(because the graph contains the variants, reads supporting them map better) and
reduces reference bias in allele balance at heterozygous sites (more even
coverage of both alleles).

### What the pangenome graph contains

The Human Pangenome Reference Consortium (HPRC) has constructed a pangenome
graph from dozens of haplotype-resolved assemblies. This graph contains:

- SNPs and small indels from all input haplotypes
- Structural variants (insertions, deletions, inversions) embedded as
  alternative paths
- Sequences absent from the GRCh38 reference (novel insertions found in other
  populations)

The graph is stored in GFA format (Graphical Fragment Assembly), and reads can
be aligned to it to produce variant calls. The resulting VCF calls are more
complete and more accurate than calls from linear alignment alone, particularly
for SVs and in regions of high diversity.

### Limitations of graph genomes

Graph genomes do not solve all problems:

- **Novel variation**: If a sample has a variant that is not in the graph, the
  situation is the same as with a linear reference — reads from the novel allele
  have no path to map to. The graph only helps for variation that was included
  when the graph was built.
- **Complexity**: Graph data structures are harder to work with than linear
  sequences. Indexing, coordinate systems, and visualization are all more
  complex. The "position on chromosome 1" concept that makes VCF and genome
  browsers intuitive does not have a clean analog in a graph.
- **Tool ecosystem**: Most bioinformatics tools assume linear coordinates.
  Transitioning the entire ecosystem to graph-native tools is a slow process. In
  the interim, the "project back to linear" approach bridges the gap.
- **Representation of rare variants**: Building a graph from a reference panel
  means common variants are well-represented but rare variants (found in only
  one or two individuals in the panel) may not be included. The graph reflects
  the diversity of its input genomes.

### VCF as an interchange format for graph-derived calls

Even as graph genomes become more common, VCF is likely to remain the standard
interchange format for variant calls for a long time. The reason is pragmatic:
the entire ecosystem of downstream tools (GWAS, clinical interpretation,
population genetics, genome browsers) works with VCF. Replacing VCF would
require rewriting or adapting hundreds of tools.

The more likely trajectory is that VCF continues to be used for the final output
of variant calling pipelines, while the internal steps (alignment, genotyping)
increasingly use graph-based methods. The graph improves the quality of the VCF,
even though the VCF itself remains a linear representation.

## Conclusion

VCF is a format that is simple enough to read by eye for a single sample, but
becomes complex when dealing with multi-allelic sites, structural variants,
mixed ploidy, and large cohorts. The format serves as the common interface
between variant calling and downstream analysis: the same VCF feeds into
population genetics, GWAS pipelines, clinical variant interpretation, and
evolutionary analyses.

Understanding the low-level details of VCF (normalization, genotype encoding,
breakend notation, the `*` allele) helps in debugging the inevitable edge cases
that arise when working with real data.
